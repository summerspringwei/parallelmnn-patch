diff --git a/3rd_party/OpenCLHeaders/CL/cl2.hpp b/3rd_party/OpenCLHeaders/CL/cl2.hpp
index 6023234..64a78ed 100644
--- a/3rd_party/OpenCLHeaders/CL/cl2.hpp
+++ b/3rd_party/OpenCLHeaders/CL/cl2.hpp
@@ -3675,6 +3675,18 @@ public:
         }
     }
 
+    Buffer(
+        const Context& context,
+        cl_mem_flags flags,
+        size_type size,
+        void* host_ptr,
+        float, float, float, float)
+    {
+        cl_int error;
+        object_ = ::clCreateBuffer(context(), flags, size, host_ptr, &error);
+
+        detail::errHandler(error, __CREATE_BUFFER_ERR);
+    }
     /*! \brief Constructs a Buffer in the default context.
      *
      *  Wraps clCreateBuffer().
diff --git a/CMakeLists.txt b/CMakeLists.txt
index 9c97b4a..8f6c143 100644
--- a/CMakeLists.txt
+++ b/CMakeLists.txt
@@ -58,7 +58,7 @@ option(MNN_BUILD_HARD "Build -mfloat-abi=hard or not" OFF)
 option(MNN_BUILD_SHARED_LIBS "MNN build shared or static lib" ON)
 option(MNN_FORBID_MULTI_THREAD "Disable Multi Thread" OFF)
 option(MNN_OPENMP "Enable Multiple Thread Linux|Android" ON)
-option(MNN_USE_THREAD_POOL "Use Multiple Thread by Self ThreadPool" ON)
+option(MNN_USE_THREAD_POOL "Use Multiple Thread by Self ThreadPool" OFF)
 option(MNN_BUILD_TRAIN "Build Train Tools" OFF)
 option(MNN_BUILD_DEMO "Build demo/exec or not" OFF)
 option(MNN_BUILD_QUANTOOLS "Build Quantized Tools or not" OFF)
@@ -75,7 +75,7 @@ option(MNN_USE_SSE "Enable SSE Optimizations" ON)
 option(MNN_USE_AVX "Enable AVX Optimizations" ON)
 option(NATIVE_LIBRARY_OUTPUT "Native Library Path" OFF)
 option(NATIVE_INCLUDE_OUTPUT "Native Include Path" OFF)
-
+message(STATUS, ${MNN_USE_THREAD_POOL})
 
 set(MNN_SCHEMA_SUFFIX "default" CACHE STRING "MNN Schema Source Path Suffix")
 IF(APPLE AND MNN_AAPL_FMWK AND MNN_SEP_BUILD)
@@ -89,7 +89,6 @@ if (MNN_USE_THREAD_POOL)
     set(MNN_OPENMP OFF)
     add_definitions(-DMNN_USE_THREAD_POOL)
 endif()
-
 if(MNN_FORBID_MULTI_THREAD)
     add_definitions(-DMNN_FORBIT_MULTI_THREADS)
 endif()
@@ -128,13 +127,16 @@ if (MNN_METAL)
 endif()
 
 # target options
-option(MNN_BUILD_BENCHMARK "Build benchmark or not" OFF)
-option(MNN_BUILD_TEST "Build tests or not" OFF)
+option(MNN_BUILD_BENCHMARK "Build benchmark or not" ON)
+IF(MNN_BUILD_BENCHMARK)
+  add_definitions(-DMNN_BENCHMARK)
+ENDIF()
+option(MNN_BUILD_TEST "Build tests or not" ON)
 option(MNN_BUILD_FOR_ANDROID_COMMAND "Build from command" OFF)
 set (MNN_HIDDEN FALSE)
 IF(CMAKE_BUILD_TYPE MATCHES Debug)
 ELSE()
-    set(MNN_HIDDEN TRUE)
+    set(MNN_HIDDEN FALSE)
 ENDIF(CMAKE_BUILD_TYPE MATCHES Debug)
 
 
@@ -406,7 +408,7 @@ if(APPLE)
       ELSEIF(${CMAKE_SYSTEM_NAME} MATCHES "Android")
           add_definitions(-DMNN_USE_OPENCL_WRAPPER)
           #add_definitions(-DENABLE_OPENCL_TURNING_PROFILER)
-          #add_definitions(-DLOG_VERBOSE)
+          add_definitions(-DLOG_VERBOSE)
       ENDIF()
     ENDIF()
 endif()
@@ -486,6 +488,7 @@ if (NOT APPLE)
           set(OpenMP_CXX_FLAGS "/openmp ${OpenMP_CXX_FLAGS}")
       endif()
       FOREACH(TARGET ${MNN_TARGETS})
+          message("target_link_libraries ${TARGET} ${OpenMP_CXX_LIBRARIES}")
        target_link_libraries(${TARGET} PUBLIC ${OpenMP_CXX_LIBRARIES})
        IF(WIN32)
             target_compile_options(${TARGET} PUBLIC /openmp ${OpenMP_CXX_FLAGS} ${OpenMP_C_FLAGS})
@@ -500,6 +503,7 @@ list(APPEND MNN_TARGETS MNN)
     add_dependencies(${TARGET} MNN_SCHEMA_GEN)
     IF((NOT MSVC) AND (NOT WIN32))
       target_compile_options(${TARGET} PUBLIC -fomit-frame-pointer -fstrict-aliasing -ffunction-sections -fdata-sections -ffast-math -fno-rtti)
+      # target_compile_options(${TARGET} PUBLIC -fomit-frame-pointer -fstrict-aliasing -ffunction-sections -fdata-sections -ffast-math)
       target_compile_options(${TARGET} PRIVATE -fno-exceptions)
       if(MNN_HIDDEN)
           target_compile_options(${TARGET} PRIVATE -fvisibility-inlines-hidden -fvisibility=hidden)
diff --git a/benchmark/CMakeLists.txt b/benchmark/CMakeLists.txt
index 25c2e91..835d237 100644
--- a/benchmark/CMakeLists.txt
+++ b/benchmark/CMakeLists.txt
@@ -1,8 +1,12 @@
 IF(MNN_BUILD_BENCHMARK)
+
   add_executable(benchmark.out ${CMAKE_CURRENT_LIST_DIR}/benchmark.cpp ${CMAKE_CURRENT_SOURCE_DIR}/tools/cpp/revertMNNModel.cpp)
   target_include_directories(benchmark.out PRIVATE ${CMAKE_CURRENT_SOURCE_DIR}/tools/cpp/ ${CMAKE_CURRENT_SOURCE_DIR}/tools/)
   target_link_libraries(benchmark.out ${MNN_DEPS})
 
+  add_executable(benchmarkTensorTransform ${CMAKE_CURRENT_LIST_DIR}/benchmarkTensorTransform.cpp)
+  target_link_libraries(benchmarkTensorTransform ${MNN_DEPS})
+
   file(GLOB_RECURSE SRC_FILES ${CMAKE_CURRENT_LIST_DIR}/exprModels/*.cpp)
   add_executable(benchmarkExprModels.out ${CMAKE_CURRENT_LIST_DIR}/benchmarkExprModels.cpp ${SRC_FILES})
   target_include_directories(benchmarkExprModels.out PRIVATE "${CMAKE_CURRENT_LIST_DIR}/exprModels" ${CMAKE_CURRENT_SOURCE_DIR}/)
diff --git a/benchmark/bench_android.sh b/benchmark/bench_android.sh
index fef57d5..462708a 100755
--- a/benchmark/bench_android.sh
+++ b/benchmark/bench_android.sh
@@ -1,7 +1,7 @@
 set -e
 ABI="armeabi-v7a"
 OPENMP="ON"
-VULKAN="ON"
+VULKAN="OFF"
 OPENCL="ON"
 OPENGL="OFF"
 RUN_LOOP=10
@@ -32,16 +32,17 @@ function clean_build() {
     rm -rf $1
     mkdir $1
 }
-
+# -DCMAKE_BUILD_TYPE=Debug \
 function build_android_bench() {
     if [ "-c" == "$CLEAN" ]; then
         clean_build $BUILD_DIR
     fi
     mkdir -p build
     cd $BUILD_DIR
+    rm ../../source/backend/opencl/execution/cl/opencl_program.cc
+    echo "Removed opencl_program.cc"
     cmake ../../ \
           -DCMAKE_TOOLCHAIN_FILE=$ANDROID_NDK/build/cmake/android.toolchain.cmake \
-          -DCMAKE_BUILD_TYPE=Release \
           -DANDROID_ABI="${ABI}" \
           -DANDROID_STL=c++_static \
           -DCMAKE_BUILD_TYPE=Release \
@@ -51,22 +52,30 @@ function build_android_bench() {
           -DMNN_OPENCL:BOOL=$OPENCL \
           -DMNN_OPENMP:BOOL=$OPENMP \
           -DMNN_OPENGL:BOOL=$OPENGL \
+          -DMNN_BUILD_TEST:BOOL=ON \
           -DMNN_DEBUG:BOOL=OFF \
+          -DMNN_USE_THREAD_POOL:BOOL=OFF \
           -DMNN_BUILD_BENCHMARK:BOOL=ON \
           -DMNN_BUILD_FOR_ANDROID_COMMAND=true \
           -DNATIVE_LIBRARY_OUTPUT=.
-    make -j8 benchmark.out timeProfile.out
+    #make -j8 benchmark.out timeProfile.out benchmarkTensorTransform run_test.out
+    make -j8 benchmark.out timeProfile.out benchmarkTensorTransform
 }
 
 function bench_android() {
     build_android_bench
+    cd arm64-v8a
     find . -name "*.so" | while read solib; do
+        echo $solib
         adb push $solib  $ANDROID_DIR
     done
+    cd -
     adb push benchmark.out $ANDROID_DIR
     adb push timeProfile.out $ANDROID_DIR
     adb shell chmod 0777 $ANDROID_DIR/benchmark.out
-
+    adb push benchmarkTensorTransform $ANDROID_DIR
+    # adb push run_test.out $ANDROID_DIR
+    exit 0
     if [ "" != "$PUSH_MODEL" ]; then
         adb shell "rm -rf $ANDROID_DIR/benchmark_models"
         adb push $BENCHMARK_MODEL_DIR $ANDROID_DIR/benchmark_models
@@ -75,9 +84,9 @@ function bench_android() {
     adb shell "echo >> $ANDROID_DIR/benchmark.txt"
     adb shell "echo Build Flags: ABI=$ABI  OpenMP=$OPENMP Vulkan=$VULKAN OpenCL=$OPENCL >> $ANDROID_DIR/benchmark.txt"
     #benchmark  CPU
-    adb shell "LD_LIBRARY_PATH=$ANDROID_DIR $ANDROID_DIR/benchmark.out $ANDROID_DIR/benchmark_models $RUN_LOOP $FORWARD_TYPE 2>$ANDROID_DIR/benchmark.err >> $ANDROID_DIR/benchmark.txt"
+    # adb shell "LD_LIBRARY_PATH=$ANDROID_DIR $ANDROID_DIR/benchmark.out $ANDROID_DIR/benchmark_models $RUN_LOOP $FORWARD_TYPE 2>$ANDROID_DIR/benchmark.err >> $ANDROID_DIR/benchmark.txt"
     #benchmark  Vulkan
-    adb shell "LD_LIBRARY_PATH=$ANDROID_DIR $ANDROID_DIR/benchmark.out $ANDROID_DIR/benchmark_models $RUN_LOOP 7 2>$ANDROID_DIR/benchmark.err >> $ANDROID_DIR/benchmark.txt"
+    # adb shell "LD_LIBRARY_PATH=$ANDROID_DIR $ANDROID_DIR/benchmark.out $ANDROID_DIR/benchmark_models $RUN_LOOP 7 2>$ANDROID_DIR/benchmark.err >> $ANDROID_DIR/benchmark.txt"
     #benchmark OpenGL
     #adb shell "LD_LIBRARY_PATH=$ANDROID_DIR $ANDROID_DIR/benchmark.out $ANDROID_DIR/benchmark_models 10 6 2>$ANDROID_DIR/benchmark.err >> $ANDROID_DIR/benchmark.txt"
     #benchmark OpenCL
diff --git a/benchmark/benchmark.cpp b/benchmark/benchmark.cpp
index 5646df5..ee09d24 100644
--- a/benchmark/benchmark.cpp
+++ b/benchmark/benchmark.cpp
@@ -16,6 +16,7 @@
 #include <fstream>
 #include <iostream>
 #include <vector>
+#include <unistd.h>
 #if defined(_MSC_VER)
 #include <Windows.h>
 #undef min
@@ -32,6 +33,7 @@
 #include <MNN/MNNDefine.h>
 #include <MNN/Tensor.hpp>
 #include "revertMNNModel.hpp"
+#include "core/MUtils.hpp"
 /**
  TODOs:
  1. dynamically get CPU related info.
@@ -99,25 +101,11 @@ void setInputData(MNN::Tensor* tensor) {
     }
 }
 
-static inline uint64_t getTimeInUs() {
-    uint64_t time;
-#if defined(_MSC_VER)
-    LARGE_INTEGER now, freq;
-    QueryPerformanceCounter(&now);
-    QueryPerformanceFrequency(&freq);
-    uint64_t sec = now.QuadPart / freq.QuadPart;
-    uint64_t usec = (now.QuadPart % freq.QuadPart) * 1000000 / freq.QuadPart;
-    time = sec * 1000000 + usec;
-#else
-    struct timeval tv;
-    gettimeofday(&tv, nullptr);
-    time = static_cast<uint64_t>(tv.tv_sec) * 1000000 + tv.tv_usec;
-#endif
-    return time;
-}
 
 std::vector<float> doBench(Model& model, int loop, int forward = MNN_FORWARD_CPU, bool only_inference = true,
-                           int numberThread = 4, int precision = 2) {
+                           int numberThread = 4, int precision = 2,
+                           MNNExecutionType executionType = MNNExecutionType::MNN_EXECUTION_DEFAULT, 
+                           char * filePath = nullptr, int CPULittleNumThread = 1) {
     auto revertor = std::unique_ptr<Revert>(new Revert(model.model_file.c_str()));
     revertor->initialize();
     auto modelBuffer      = revertor->getBuffer();
@@ -127,6 +115,9 @@ std::vector<float> doBench(Model& model, int loop, int forward = MNN_FORWARD_CPU
     MNN::ScheduleConfig config;
     config.numThread = numberThread;
     config.type      = static_cast<MNNForwardType>(forward);
+    config.executionType = executionType;
+    config.deviceMapFilePath = filePath;
+    config.CPULittleNumThread = CPULittleNumThread;
     MNN::BackendConfig backendConfig;
     backendConfig.precision = (MNN::BackendConfig::PrecisionMode)precision;
     backendConfig.power = MNN::BackendConfig::Power_High;
@@ -149,21 +140,25 @@ std::vector<float> doBench(Model& model, int loop, int forward = MNN_FORWARD_CPU
     auto outputTensor = net->getSessionOutput(session, NULL);
     std::shared_ptr<MNN::Tensor> expectTensor(MNN::Tensor::createHostTensorFromDevice(outputTensor, false));
     // Warming up...
+    MNN_PRINT("Run 3 times to warm up\n");
     for (int i = 0; i < 3; ++i) {
         input->copyFromHostTensor(givenTensor.get());
         net->runSession(session);
         outputTensor->copyToHostTensor(expectTensor.get());
     }
+    MNN_PRINT("Warm up done\n");
 
     for (int round = 0; round < loop; round++) {
-        auto timeBegin = getTimeInUs();
+        auto timeBegin = MNN::getTimeInUs();
 
         input->copyFromHostTensor(givenTensor.get());
         net->runSession(session);
         outputTensor->copyToHostTensor(expectTensor.get());
 
-        auto timeEnd = getTimeInUs();
+        auto timeEnd = MNN::getTimeInUs();
         costs.push_back((timeEnd - timeBegin) / 1000.0);
+        MNN_PRINT("\n<-- Run %d iterations -->\n\n", round + 1);
+        // sleep(1);
     }
     return costs;
 }
@@ -176,7 +171,7 @@ void displayStats(const std::string& name, const std::vector<float>& costs) {
         sum += v;
     }
     avg = costs.size() > 0 ? sum / costs.size() : 0;
-    printf("[ - ] %-24s    max = %8.3fms  min = %8.3fms  avg = %8.3fms\n", name.c_str(), max, avg == 0 ? 0 : min, avg);
+    printf("[ - ] %-24s    max = %8.3f ms  min = %8.3f ms  avg = %8.3f ms\n", name.c_str(), max, avg == 0 ? 0 : min, avg);
 }
 static inline std::string forwardType(MNNForwardType type) {
     switch (type) {
@@ -193,11 +188,14 @@ static inline std::string forwardType(MNNForwardType type) {
     }
     return "N/A";
 }
+
 int main(int argc, const char* argv[]) {
     std::cout << "MNN benchmark" << std::endl;
     int loop               = 10;
     MNNForwardType forward = MNN_FORWARD_CPU;
     int numberThread       = 4;
+    MNNExecutionType executionType = MNNExecutionType::MNN_EXECUTION_DEFAULT;
+    char* deviceMapFilePath = (char*)"/data/local/tmp/redmi_cpu-4-device-placement.txt";
     if (argc <= 2) {
         std::cout << "Usage: " << argv[0] << " models_folder [loop_count] [forwardtype]" << std::endl;
         return 1;
@@ -215,12 +213,41 @@ int main(int argc, const char* argv[]) {
     if (argc >= 6) {
         precision = atoi(argv[5]);
     }
-    std::cout << "Forward type: **" << forwardType(forward) << "** thread=" << numberThread << "** precision=" <<precision << std::endl;
+    if(argc >= 7){
+        auto execution = atoi(argv[6]);
+        switch(execution){
+            case MNNExecutionType::MNN_EXECUTION_DEFAULT:
+                executionType = MNNExecutionType::MNN_EXECUTION_DEFAULT;
+                break;
+            case MNNExecutionType::MNN_EXECUTION_SERIAL_HYBRID:
+                executionType = MNNExecutionType::MNN_EXECUTION_SERIAL_HYBRID;
+                break;
+            case MNNExecutionType::MNN_EXECUTION_PARALLEL:
+                executionType = MNNExecutionType::MNN_EXECUTION_PARALLEL;
+                break;
+            case MNNExecutionType::MNN_EXECUTION_BIG_LITTLE:
+                executionType = MNNExecutionType::MNN_EXECUTION_BIG_LITTLE;
+                break;
+            default:
+            MNN_PRINT("Invalid execution type %d\n", execution);
+        }
+
+    }
+    if(argc >= 8){
+        deviceMapFilePath = (char*)argv[7];
+    }
+    int CPULittleThreadNum = 1;
+    if(argc >=9){
+        CPULittleThreadNum = atoi(argv[8]);
+    }
+    std::cout << "Forward type: **" << forwardType(forward) << "** thread=" << numberThread 
+      << "** precision=" <<precision << "** execution type=" << executionType <<std::endl;
     std::vector<Model> models = findModelFiles(argv[1]);
 
     std::cout << "--------> Benchmarking... loop = " << argv[2] << std::endl;
     for (auto& m : models) {
-        std::vector<float> costs = doBench(m, loop, forward, false, numberThread, precision);
+        std::vector<float> costs = doBench(m, loop, forward, false, numberThread, 
+          precision, executionType, deviceMapFilePath, CPULittleThreadNum);
         displayStats(m.name, costs);
     }
 }
diff --git a/benchmark/benchmarkExecution.cpp b/benchmark/benchmarkExecution.cpp
new file mode 100644
index 0000000..0c6ff10
--- /dev/null
+++ b/benchmark/benchmarkExecution.cpp
@@ -0,0 +1,9 @@
+
+#include "backend/cpu/CPUMatMul.hpp"
+
+int main(int argc, char* argv[]){
+    
+    MNN::CPUBackend* cpuBackend = new MNN::CPUBackend();
+    
+    return 0;
+}
\ No newline at end of file
diff --git a/benchmark/benchmarkTensorTransform.cpp b/benchmark/benchmarkTensorTransform.cpp
new file mode 100644
index 0000000..5e02480
--- /dev/null
+++ b/benchmark/benchmarkTensorTransform.cpp
@@ -0,0 +1,94 @@
+
+#include <memory.h>
+#include <stdio.h>
+#include <stdlib.h>
+#include <sys/time.h>
+#include <unistd.h>
+#include <assert.h>
+
+#include <vector>
+
+#include <MNN/Tensor.hpp>
+#include "backend/opencl/core/ImageBufferConvertor.hpp"
+#include "backend/opencl/core/OpenCLBackend.hpp"
+#include "core/MUtils.hpp"
+
+
+int main(int argc, const char *argv[]) {
+  int n = 1, c = 8, h = 16, w = 16;
+  int loop_count = 100;
+  
+  if (argc > 4) {
+    n = atoi(argv[1]);
+    c = atoi(argv[2]);
+    h = atoi(argv[3]);
+    w = atoi(argv[4]);
+  }
+  if (argc > 5) {
+    loop_count = atoi(argv[5]);
+  }
+  
+  const std::vector<int> input_shape = {n, c, h, w};
+  std::vector<float *> input_data_vector(loop_count);
+  std::vector<MNN::Tensor *> input_tensor_vector(loop_count);
+  printf("Start initialize\n");
+  // Create and initialize input tensors
+  for (int i = 0; i < loop_count; ++i) {
+    input_data_vector[i] = new float[n * c * h * w];
+    input_tensor_vector[i] = MNN::Tensor::create(
+        input_shape, halide_type_of<float>(), (void *)input_data_vector[i],
+        MNN::Tensor::DimensionType::TENSORFLOW);
+    auto data = input_tensor_vector[i]->host<float>();
+    for (int j = 0; j < input_tensor_vector[i]->elementSize(); ++j) {
+      data[j] = (float)j;
+    }
+    // MNN_PRINT("loop %d\n", i);
+    // printf("%d set value\n", i);
+    // if(n*c*h*w <= 1*16*32*32){
+    //     input_tensor_vector[i]->print();
+    // }
+  }
+  
+  auto backend = new MNN::OpenCL::OpenCLBackend(
+      MNN::BackendConfig::BackendConfig::Precision_Low,
+      MNN::BackendConfig::PowerMode::Power_High);
+  
+  std::vector<MNN::Tensor *> output_tensor_vector(loop_count);
+  for (int i = 0; i < loop_count; ++i) {
+    // MNN_PRINT("loop %d\n", i);
+    output_tensor_vector[i] = MNN::Tensor::createDevice<float>(
+        input_shape, MNN::Tensor::DimensionType::TENSORFLOW);
+    backend->onAcquireBuffer(output_tensor_vector[i],
+                             MNN::Backend::StorageType::DYNAMIC_SEPERATE);
+   // MNN_PRINT("loop %d\n", i);
+  }
+
+  // Warm up
+  auto start = MNN::getTimeInUs();
+  for (int i = 0; i < loop_count; ++i) {
+      //  MNN_PRINT("loop %d\n", i);
+    assert(output_tensor_vector[i]->buffer().host == NULL);
+    backend->onCopyBuffer(input_tensor_vector[i], output_tensor_vector[i]);
+    auto host_tensor = MNN::Tensor::create<float>(input_shape, NULL, MNN::Tensor::DimensionType::TENSORFLOW);
+    backend->onCopyBuffer(output_tensor_vector[i], host_tensor);
+    backend->onWaitFinish();
+  }
+  auto end = MNN::getTimeInUs();
+  for (int i = 0; i < loop_count; ++i) {
+    delete[] input_data_vector[i];
+  }
+  // input_tensor_vector[0]->print();
+  // printf("-----------------");
+  // output_tensor_vector[0]->print();
+  // MNN::Tensor* output_host_tensor = new MNN::Tensor(input_tensor,
+  // MNN::Tensor::DimensionType::CAFFE_C4, true);
+  // output_tensor->copyToHostTensor(output_host_tensor);
+  printf("Convert %d times %d,%d,%d,%d used %lu\n", loop_count, n, c, h, w,
+         (end - start)/loop_count);
+  // printf("Output host is %d,  device is %d\n", output_tensor->buffer().host
+  // != NULL, output_tensor->buffer().device != NULL);
+  // output_host_tensor->print();
+  // Destroy tensors
+
+  return 0;
+}
diff --git a/benchmark/merge_buffer_to_image_data.py b/benchmark/merge_buffer_to_image_data.py
new file mode 100644
index 0000000..a54f11e
--- /dev/null
+++ b/benchmark/merge_buffer_to_image_data.py
@@ -0,0 +1,19 @@
+
+f = open("redmi_nc4hw4_buffer_to_image.csv", 'r')
+shape_time_map = {}
+num_shapes = 30
+count = 0
+shape_list = []
+for line in f.readlines():
+    #print(line)
+    com = line.strip().split(' ')
+    count += 1
+    if com[0] in shape_time_map.keys():
+        shape_time_map[com[0]] = shape_time_map[com[0]] + int(com[1])
+    else:
+        shape_list.append(com[0])
+        shape_time_map[com[0]] = int(com[1])
+
+print(count)
+for shape in shape_list:
+    print("%s %d" % (shape, shape_time_map[shape]/(count/num_shapes)))
diff --git a/benchmark/redmi_nc4hw4_buffer_to_image.csv b/benchmark/redmi_nc4hw4_buffer_to_image.csv
new file mode 100644
index 0000000..80acc1d
--- /dev/null
+++ b/benchmark/redmi_nc4hw4_buffer_to_image.csv
@@ -0,0 +1,300 @@
+(1,32,149,149) 425779
+(1,32,147,147) 410459
+(1,64,147,147) 791266
+(1,64,73,73) 330767
+(1,80,73,73) 376793
+(1,192,71,71) 605193
+(1,192,35,35) 296446
+(1,64,35,35) 275839
+(1,48,35,35) 221221
+(1,32,35,35) 232124
+(1,96,35,35) 237600
+(1,256,35,35) 324612
+(1,288,35,35) 343965
+(1,288,17,17) 228464
+(1,384,17,17) 255114
+(1,96,17,17) 198113
+(1,768,17,17) 300570
+(1,128,17,17) 259468
+(1,192,17,17) 228813
+(1,160,17,17) 225798
+(1,768,8,8) 423304
+(1,320,8,8) 256902
+(1,192,8,8) 205320
+(1,1280,8,8) 241680
+(1,448,8,8) 395391
+(1,384,8,8) 249235
+(1,2048,8,8) 274189
+(1,2048,1,1) 199137
+(1,1001,1,1) 175216
+(1,1,1,1001) 393756
+(1,32,149,149) 419355
+(1,32,147,147) 406234
+(1,64,147,147) 815270
+(1,64,73,73) 334453
+(1,80,73,73) 361670
+(1,192,71,71) 590238
+(1,192,35,35) 298521
+(1,64,35,35) 233599
+(1,48,35,35) 213764
+(1,32,35,35) 237611
+(1,96,35,35) 229407
+(1,256,35,35) 315025
+(1,288,35,35) 353843
+(1,288,17,17) 233004
+(1,384,17,17) 249158
+(1,96,17,17) 240940
+(1,768,17,17) 304225
+(1,128,17,17) 235806
+(1,192,17,17) 415270
+(1,160,17,17) 257318
+(1,768,8,8) 415110
+(1,320,8,8) 271323
+(1,192,8,8) 196748
+(1,1280,8,8) 258350
+(1,448,8,8) 283461
+(1,384,8,8) 413128
+(1,2048,8,8) 284567
+(1,2048,1,1) 220477
+(1,1001,1,1) 193111
+(1,1,1,1001) 318699
+(1,32,149,149) 426639
+(1,32,147,147) 407206
+(1,64,147,147) 733416
+(1,64,73,73) 327798
+(1,80,73,73) 361210
+(1,192,71,71) 604192
+(1,192,35,35) 293846
+(1,64,35,35) 416537
+(1,48,35,35) 270804
+(1,32,35,35) 241771
+(1,96,35,35) 239940
+(1,256,35,35) 318254
+(1,288,35,35) 336934
+(1,288,17,17) 257712
+(1,384,17,17) 240327
+(1,96,17,17) 199244
+(1,768,17,17) 290795
+(1,128,17,17) 267203
+(1,192,17,17) 226969
+(1,160,17,17) 419750
+(1,768,8,8) 410830
+(1,320,8,8) 231991
+(1,192,8,8) 212696
+(1,1280,8,8) 233422
+(1,448,8,8) 402621
+(1,384,8,8) 395147
+(1,2048,8,8) 267733
+(1,2048,1,1) 264989
+(1,1001,1,1) 169450
+(1,1,1,1001) 248276
+(1,32,149,149) 423249
+(1,32,147,147) 403834
+(1,64,147,147) 732827
+(1,64,73,73) 323003
+(1,80,73,73) 365710
+(1,192,71,71) 605840
+(1,192,35,35) 280650
+(1,64,35,35) 221074
+(1,48,35,35) 228786
+(1,32,35,35) 236167
+(1,96,35,35) 247241
+(1,256,35,35) 294874
+(1,288,35,35) 309014
+(1,288,17,17) 228663
+(1,384,17,17) 241784
+(1,96,17,17) 223979
+(1,768,17,17) 299863
+(1,128,17,17) 233874
+(1,192,17,17) 415010
+(1,160,17,17) 432394
+(1,768,8,8) 231829
+(1,320,8,8) 251192
+(1,192,8,8) 221593
+(1,1280,8,8) 252483
+(1,448,8,8) 240482
+(1,384,8,8) 408466
+(1,2048,8,8) 286274
+(1,2048,1,1) 211443
+(1,1001,1,1) 170710
+(1,1,1,1001) 401209
+(1,32,149,149) 410866
+(1,32,147,147) 417375
+(1,64,147,147) 819916
+(1,64,73,73) 296583
+(1,80,73,73) 337255
+(1,192,71,71) 597577
+(1,192,35,35) 298697
+(1,64,35,35) 229898
+(1,48,35,35) 274560
+(1,32,35,35) 236310
+(1,96,35,35) 249601
+(1,256,35,35) 320703
+(1,288,35,35) 345916
+(1,288,17,17) 238213
+(1,384,17,17) 305368
+(1,96,17,17) 230616
+(1,768,17,17) 309452
+(1,128,17,17) 290076
+(1,192,17,17) 215884
+(1,160,17,17) 407694
+(1,768,8,8) 419197
+(1,320,8,8) 231612
+(1,192,8,8) 238167
+(1,1280,8,8) 431069
+(1,448,8,8) 397713
+(1,384,8,8) 393858
+(1,2048,8,8) 274331
+(1,2048,1,1) 217951
+(1,1001,1,1) 167115
+(1,1,1,1001) 296290
+(1,32,149,149) 426590
+(1,32,147,147) 411362
+(1,64,147,147) 788310
+(1,64,73,73) 298827
+(1,80,73,73) 335702
+(1,192,71,71) 679715
+(1,192,35,35) 287482
+(1,64,35,35) 422353
+(1,48,35,35) 409882
+(1,32,35,35) 228938
+(1,96,35,35) 242886
+(1,256,35,35) 315896
+(1,288,35,35) 339755
+(1,288,17,17) 230426
+(1,384,17,17) 243716
+(1,96,17,17) 238752
+(1,768,17,17) 309426
+(1,128,17,17) 243548
+(1,192,17,17) 417062
+(1,160,17,17) 288879
+(1,768,8,8) 427305
+(1,320,8,8) 218434
+(1,192,8,8) 214562
+(1,1280,8,8) 242805
+(1,448,8,8) 413325
+(1,384,8,8) 266614
+(1,2048,8,8) 286491
+(1,2048,1,1) 187055
+(1,1001,1,1) 158494
+(1,1,1,1001) 234914
+(1,32,149,149) 419371
+(1,32,147,147) 392778
+(1,64,147,147) 794590
+(1,64,73,73) 330751
+(1,80,73,73) 345917
+(1,192,71,71) 671838
+(1,192,35,35) 301819
+(1,64,35,35) 224438
+(1,48,35,35) 217972
+(1,32,35,35) 232155
+(1,96,35,35) 238211
+(1,256,35,35) 324537
+(1,288,35,35) 325663
+(1,288,17,17) 232578
+(1,384,17,17) 245488
+(1,96,17,17) 224713
+(1,768,17,17) 299466
+(1,128,17,17) 352359
+(1,192,17,17) 413494
+(1,160,17,17) 235383
+(1,768,8,8) 325519
+(1,320,8,8) 239699
+(1,192,8,8) 221193
+(1,1280,8,8) 249803
+(1,448,8,8) 404433
+(1,384,8,8) 328632
+(1,2048,8,8) 283162
+(1,2048,1,1) 220331
+(1,1001,1,1) 165593
+(1,1,1,1001) 237310
+(1,32,149,149) 410465
+(1,32,147,147) 413853
+(1,64,147,147) 745436
+(1,64,73,73) 314666
+(1,80,73,73) 354496
+(1,192,71,71) 585745
+(1,192,35,35) 281661
+(1,64,35,35) 264478
+(1,48,35,35) 414909
+(1,32,35,35) 196086
+(1,96,35,35) 249963
+(1,256,35,35) 317734
+(1,288,35,35) 313524
+(1,288,17,17) 228179
+(1,384,17,17) 250830
+(1,96,17,17) 231793
+(1,768,17,17) 303585
+(1,128,17,17) 257837
+(1,192,17,17) 221834
+(1,160,17,17) 407275
+(1,768,8,8) 222154
+(1,320,8,8) 240303
+(1,192,8,8) 204602
+(1,1280,8,8) 235457
+(1,448,8,8) 383747
+(1,384,8,8) 287105
+(1,2048,8,8) 280317
+(1,2048,1,1) 228142
+(1,1001,1,1) 166810
+(1,1,1,1001) 408038
+(1,32,149,149) 423527
+(1,32,147,147) 412564
+(1,64,147,147) 716913
+(1,64,73,73) 304459
+(1,80,73,73) 354487
+(1,192,71,71) 602091
+(1,192,35,35) 288479
+(1,64,35,35) 265986
+(1,48,35,35) 417977
+(1,32,35,35) 234998
+(1,96,35,35) 248259
+(1,256,35,35) 312768
+(1,288,35,35) 346935
+(1,288,17,17) 238103
+(1,384,17,17) 246111
+(1,96,17,17) 229869
+(1,768,17,17) 301023
+(1,128,17,17) 276912
+(1,192,17,17) 425118
+(1,160,17,17) 418984
+(1,768,8,8) 421445
+(1,320,8,8) 232331
+(1,192,8,8) 236145
+(1,1280,8,8) 431703
+(1,448,8,8) 359150
+(1,384,8,8) 406633
+(1,2048,8,8) 277105
+(1,2048,1,1) 221432
+(1,1001,1,1) 168637
+(1,1,1,1001) 389148
+(1,32,149,149) 427222
+(1,32,147,147) 414996
+(1,64,147,147) 731325
+(1,64,73,73) 325975
+(1,80,73,73) 367794
+(1,192,71,71) 596344
+(1,192,35,35) 264317
+(1,64,35,35) 239927
+(1,48,35,35) 419370
+(1,32,35,35) 230177
+(1,96,35,35) 234711
+(1,256,35,35) 294727
+(1,288,35,35) 343205
+(1,288,17,17) 231814
+(1,384,17,17) 244808
+(1,96,17,17) 235180
+(1,768,17,17) 302421
+(1,128,17,17) 237910
+(1,192,17,17) 421776
+(1,160,17,17) 300942
+(1,768,8,8) 413175
+(1,320,8,8) 227761
+(1,192,8,8) 218257
+(1,1280,8,8) 303135
+(1,448,8,8) 406156
+(1,384,8,8) 412176
+(1,2048,8,8) 286203
+(1,2048,1,1) 226222
+(1,1001,1,1) 191812
+(1,1,1,1001) 403507
diff --git a/benchmark/redmi_nc4hw4_buffer_to_image_v1.csv b/benchmark/redmi_nc4hw4_buffer_to_image_v1.csv
new file mode 100644
index 0000000..3c3fb3f
--- /dev/null
+++ b/benchmark/redmi_nc4hw4_buffer_to_image_v1.csv
@@ -0,0 +1,300 @@
+(1,32,149,149) 324150
+(1,32,147,147) 308382
+(1,64,147,147) 470139
+(1,64,73,73) 249329
+(1,80,73,73) 249256
+(1,192,71,71) 335755
+(1,192,35,35) 208241
+(1,64,35,35) 220390
+(1,48,35,35) 179478
+(1,32,35,35) 174291
+(1,96,35,35) 275660
+(1,256,35,35) 201421
+(1,288,35,35) 210428
+(1,288,17,17) 172660
+(1,384,17,17) 150256
+(1,96,17,17) 169760
+(1,768,17,17) 171821
+(1,128,17,17) 175120
+(1,192,17,17) 140615
+(1,160,17,17) 172346
+(1,768,8,8) 137866
+(1,320,8,8) 155594
+(1,192,8,8) 170195
+(1,1280,8,8) 141839
+(1,448,8,8) 183073
+(1,384,8,8) 178703
+(1,2048,8,8) 162976
+(1,2048,1,1) 185112
+(1,1001,1,1) 257613
+(1,1,1,1001) 248182
+(1,32,149,149) 335591
+(1,32,147,147) 330347
+(1,64,147,147) 469206
+(1,64,73,73) 221197
+(1,80,73,73) 232911
+(1,192,71,71) 330406
+(1,192,35,35) 184205
+(1,64,35,35) 155923
+(1,48,35,35) 144852
+(1,32,35,35) 186007
+(1,96,35,35) 145766
+(1,256,35,35) 199351
+(1,288,35,35) 209253
+(1,288,17,17) 170848
+(1,384,17,17) 143423
+(1,96,17,17) 161980
+(1,768,17,17) 195963
+(1,128,17,17) 158984
+(1,192,17,17) 136938
+(1,160,17,17) 169502
+(1,768,8,8) 136422
+(1,320,8,8) 159015
+(1,192,8,8) 170105
+(1,1280,8,8) 172203
+(1,448,8,8) 174056
+(1,384,8,8) 171221
+(1,2048,8,8) 168720
+(1,2048,1,1) 203221
+(1,1001,1,1) 198619
+(1,1,1,1001) 408995
+(1,32,149,149) 319073
+(1,32,147,147) 321032
+(1,64,147,147) 493246
+(1,64,73,73) 263670
+(1,80,73,73) 244765
+(1,192,71,71) 332409
+(1,192,35,35) 183797
+(1,64,35,35) 152538
+(1,48,35,35) 155876
+(1,32,35,35) 172782
+(1,96,35,35) 146284
+(1,256,35,35) 195565
+(1,288,35,35) 207950
+(1,288,17,17) 177306
+(1,384,17,17) 151741
+(1,96,17,17) 175656
+(1,768,17,17) 169912
+(1,128,17,17) 176715
+(1,192,17,17) 141818
+(1,160,17,17) 171585
+(1,768,8,8) 139937
+(1,320,8,8) 179771
+(1,192,8,8) 152842
+(1,1280,8,8) 139816
+(1,448,8,8) 169171
+(1,384,8,8) 241771
+(1,2048,8,8) 194391
+(1,2048,1,1) 196251
+(1,1001,1,1) 160725
+(1,1,1,1001) 417322
+(1,32,149,149) 342402
+(1,32,147,147) 329938
+(1,64,147,147) 475095
+(1,64,73,73) 234029
+(1,80,73,73) 248313
+(1,192,71,71) 334479
+(1,192,35,35) 182251
+(1,64,35,35) 158111
+(1,48,35,35) 143100
+(1,32,35,35) 193664
+(1,96,35,35) 198151
+(1,256,35,35) 197659
+(1,288,35,35) 211376
+(1,288,17,17) 149574
+(1,384,17,17) 146393
+(1,96,17,17) 168470
+(1,768,17,17) 168437
+(1,128,17,17) 178450
+(1,192,17,17) 141826
+(1,160,17,17) 173245
+(1,768,8,8) 146971
+(1,320,8,8) 177736
+(1,192,8,8) 180012
+(1,1280,8,8) 146371
+(1,448,8,8) 170743
+(1,384,8,8) 169355
+(1,2048,8,8) 165341
+(1,2048,1,1) 205251
+(1,1001,1,1) 169394
+(1,1,1,1001) 254862
+(1,32,149,149) 325493
+(1,32,147,147) 311099
+(1,64,147,147) 501595
+(1,64,73,73) 259433
+(1,80,73,73) 246604
+(1,192,71,71) 333253
+(1,192,35,35) 186372
+(1,64,35,35) 155965
+(1,48,35,35) 142917
+(1,32,35,35) 174675
+(1,96,35,35) 147564
+(1,256,35,35) 198485
+(1,288,35,35) 239304
+(1,288,17,17) 173397
+(1,384,17,17) 157431
+(1,96,17,17) 184955
+(1,768,17,17) 168743
+(1,128,17,17) 201422
+(1,192,17,17) 195236
+(1,160,17,17) 177910
+(1,768,8,8) 147927
+(1,320,8,8) 177433
+(1,192,8,8) 177636
+(1,1280,8,8) 153878
+(1,448,8,8) 168350
+(1,384,8,8) 207818
+(1,2048,8,8) 167328
+(1,2048,1,1) 209542
+(1,1001,1,1) 172910
+(1,1,1,1001) 405572
+(1,32,149,149) 345148
+(1,32,147,147) 333785
+(1,64,147,147) 488779
+(1,64,73,73) 240180
+(1,80,73,73) 246741
+(1,192,71,71) 334945
+(1,192,35,35) 184926
+(1,64,35,35) 154370
+(1,48,35,35) 139117
+(1,32,35,35) 213497
+(1,96,35,35) 143892
+(1,256,35,35) 197772
+(1,288,35,35) 208697
+(1,288,17,17) 145394
+(1,384,17,17) 145290
+(1,96,17,17) 162474
+(1,768,17,17) 171463
+(1,128,17,17) 173945
+(1,192,17,17) 139523
+(1,160,17,17) 173350
+(1,768,8,8) 168875
+(1,320,8,8) 184919
+(1,192,8,8) 183660
+(1,1280,8,8) 140690
+(1,448,8,8) 174035
+(1,384,8,8) 172636
+(1,2048,8,8) 164953
+(1,2048,1,1) 179995
+(1,1001,1,1) 157050
+(1,1,1,1001) 401709
+(1,32,149,149) 326067
+(1,32,147,147) 329810
+(1,64,147,147) 473093
+(1,64,73,73) 231398
+(1,80,73,73) 239708
+(1,192,71,71) 331255
+(1,192,35,35) 213238
+(1,64,35,35) 178643
+(1,48,35,35) 173193
+(1,32,35,35) 182301
+(1,96,35,35) 148121
+(1,256,35,35) 190094
+(1,288,35,35) 196964
+(1,288,17,17) 138332
+(1,384,17,17) 141571
+(1,96,17,17) 162872
+(1,768,17,17) 168307
+(1,128,17,17) 177148
+(1,192,17,17) 212274
+(1,160,17,17) 180830
+(1,768,8,8) 162260
+(1,320,8,8) 169254
+(1,192,8,8) 174494
+(1,1280,8,8) 140678
+(1,448,8,8) 170340
+(1,384,8,8) 170625
+(1,2048,8,8) 168737
+(1,2048,1,1) 202409
+(1,1001,1,1) 167230
+(1,1,1,1001) 400369
+(1,32,149,149) 324593
+(1,32,147,147) 328016
+(1,64,147,147) 478841
+(1,64,73,73) 235003
+(1,80,73,73) 246590
+(1,192,71,71) 331694
+(1,192,35,35) 197520
+(1,64,35,35) 190281
+(1,48,35,35) 177680
+(1,32,35,35) 173012
+(1,96,35,35) 148022
+(1,256,35,35) 224802
+(1,288,35,35) 211636
+(1,288,17,17) 160678
+(1,384,17,17) 152276
+(1,96,17,17) 183059
+(1,768,17,17) 164985
+(1,128,17,17) 204886
+(1,192,17,17) 190960
+(1,160,17,17) 198393
+(1,768,8,8) 180139
+(1,320,8,8) 173157
+(1,192,8,8) 176974
+(1,1280,8,8) 159284
+(1,448,8,8) 164616
+(1,384,8,8) 168404
+(1,2048,8,8) 168672
+(1,2048,1,1) 208007
+(1,1001,1,1) 170056
+(1,1,1,1001) 401011
+(1,32,149,149) 334220
+(1,32,147,147) 328281
+(1,64,147,147) 485089
+(1,64,73,73) 236240
+(1,80,73,73) 239866
+(1,192,71,71) 347677
+(1,192,35,35) 177792
+(1,64,35,35) 152135
+(1,48,35,35) 140938
+(1,32,35,35) 183700
+(1,96,35,35) 173304
+(1,256,35,35) 198176
+(1,288,35,35) 204689
+(1,288,17,17) 160268
+(1,384,17,17) 147263
+(1,96,17,17) 178018
+(1,768,17,17) 164971
+(1,128,17,17) 187256
+(1,192,17,17) 188479
+(1,160,17,17) 170127
+(1,768,8,8) 138766
+(1,320,8,8) 157148
+(1,192,8,8) 126032
+(1,1280,8,8) 139830
+(1,448,8,8) 182253
+(1,384,8,8) 175093
+(1,2048,8,8) 279678
+(1,2048,1,1) 199224
+(1,1001,1,1) 180485
+(1,1,1,1001) 401731
+(1,32,149,149) 324494
+(1,32,147,147) 319658
+(1,64,147,147) 486333
+(1,64,73,73) 234365
+(1,80,73,73) 246060
+(1,192,71,71) 333226
+(1,192,35,35) 183141
+(1,64,35,35) 153156
+(1,48,35,35) 233552
+(1,32,35,35) 173214
+(1,96,35,35) 194309
+(1,256,35,35) 195413
+(1,288,35,35) 209192
+(1,288,17,17) 139728
+(1,384,17,17) 142869
+(1,96,17,17) 164986
+(1,768,17,17) 176133
+(1,128,17,17) 173006
+(1,192,17,17) 142159
+(1,160,17,17) 174147
+(1,768,8,8) 136391
+(1,320,8,8) 155821
+(1,192,8,8) 172806
+(1,1280,8,8) 143530
+(1,448,8,8) 169583
+(1,384,8,8) 170666
+(1,2048,8,8) 196500
+(1,2048,1,1) 189256
+(1,1001,1,1) 168225
+(1,1,1,1001) 398126
diff --git a/benchmark/run_bench_buffer_to_image.sh b/benchmark/run_bench_buffer_to_image.sh
new file mode 100644
index 0000000..49fd349
--- /dev/null
+++ b/benchmark/run_bench_buffer_to_image.sh
@@ -0,0 +1,5 @@
+for i in $(seq 1 10)
+do
+cat inception-v3-output-tensor-shapes.txt | ./benchmarkTensorTransform >> oneplus_nc4hw4_buffer_to_image.txt;
+echo $i;
+done
\ No newline at end of file
diff --git a/benchmark/tmp.txt b/benchmark/tmp.txt
new file mode 100644
index 0000000..3aa59c5
--- /dev/null
+++ b/benchmark/tmp.txt
@@ -0,0 +1,168 @@
+MNN benchmark
+Forward type: **OpenCL** thread=2** precision=2** execution type=1
+--------> Benchmarking... loop = 7
+In Session::Session backup cpuBackend thread number is 2
+Open Profile file /data/local/tmp/profile.txt
+In Pipeline.cpp Pipeline::prepare deviceMapFilePath: ./mDevice_map_lanenet_serial_hybrid.txt
+lanenet_model/vgg_frontend/vgg16_decode_module/instance_seg_decode/decode_stage_3_fuse/deconv/deconv/conv2d_transpose need to wrap*****
+lanenet_model/vgg_frontend/vgg16_encode_module/conv3_3/relu___tr4lanenet_model/vgg_frontend/vgg16_decode_module/binary_seg_decode/decode_stage_3_fuse/fuse_feats need to wrap*****
+lanenet_model/vgg_frontend/vgg16_decode_module/binary_seg_decode/decode_stage_3_fuse/deconv_relu___tr4lanenet_model/vgg_frontend/vgg16_decode_module/binary_seg_decode/decode_stage_3_fuse/fuse_feats need to wrap*****
+lanenet_model/vgg_frontend/vgg16_encode_module/conv3_3/relu___tr4lanenet_model/vgg_frontend/vgg16_decode_module/instance_seg_decode/decode_stage_3_fuse/fuse_feats need to wrap*****
+lanenet_model/vgg_frontend/vgg16_encode_module/conv2_2/relu___tr4lanenet_model/vgg_frontend/vgg16_decode_module/binary_seg_decode/decode_stage_2_fuse/fuse_feats need to wrap*****
+lanenet_model/vgg_frontend/vgg16_encode_module/conv2_2/relu___tr4lanenet_model/vgg_frontend/vgg16_decode_module/instance_seg_decode/decode_stage_2_fuse/fuse_feats need to wrap*****
+lanenet_model/vgg_frontend/vgg16_encode_module/conv1_2/relu___tr4lanenet_model/vgg_frontend/vgg16_decode_module/binary_seg_decode/decode_stage_1_fuse/fuse_feats need to wrap*****
+lanenet_model/vgg_frontend/vgg16_encode_module/conv1_2/relu___tr4lanenet_model/vgg_frontend/vgg16_decode_module/instance_seg_decode/decode_stage_1_fuse/fuse_feats need to wrap*****
+lanenet_model/vgg_backend/instance_seg/pix_bn/FusedBatchNorm need to wrap*****
+lanenet_model/vgg_backend/binary_seg/Softmax need to wrap*****
+OpName Inputs Outputs ParentsIdx ChildIdx
+lanenet_model/vgg_frontend/vgg16_encode_module/conv1_1/conv/Conv2D 1,3,256,512; 1,64,256,512; none 1;
+lanenet_model/vgg_frontend/vgg16_encode_module/conv1_2/conv/Conv2D 1,64,256,512; 1,64,256,512; 0; 2;64;67;
+lanenet_model/vgg_frontend/vgg16_encode_module/pool1 1,64,256,512; 1,64,128,256; 1; 3;
+lanenet_model/vgg_frontend/vgg16_encode_module/conv2_1/conv/Conv2D 1,64,128,256; 1,128,128,256; 2; 4;
+lanenet_model/vgg_frontend/vgg16_encode_module/conv2_2/conv/Conv2D 1,128,128,256; 1,128,128,256; 3; 5;50;53;
+lanenet_model/vgg_frontend/vgg16_encode_module/pool2 1,128,128,256; 1,128,64,128; 4; 6;
+lanenet_model/vgg_frontend/vgg16_encode_module/conv3_1/conv/Conv2D 1,128,64,128; 1,256,64,128; 5; 7;
+lanenet_model/vgg_frontend/vgg16_encode_module/conv3_2/conv/Conv2D 1,256,64,128; 1,256,64,128; 6; 8;
+lanenet_model/vgg_frontend/vgg16_encode_module/conv3_3/conv/Conv2D 1,256,64,128; 1,256,64,128; 7; 9;36;39;
+lanenet_model/vgg_frontend/vgg16_encode_module/pool3 1,256,64,128; 1,256,32,64; 8; 10;
+lanenet_model/vgg_frontend/vgg16_encode_module/conv4_1/conv/Conv2D 1,256,32,64; 1,512,32,64; 9; 11;
+lanenet_model/vgg_frontend/vgg16_encode_module/conv4_2/conv/Conv2D 1,512,32,64; 1,512,32,64; 10; 12;
+lanenet_model/vgg_frontend/vgg16_encode_module/conv4_3/conv/Conv2D 1,512,32,64; 1,512,32,64; 11; 13;22;25;
+lanenet_model/vgg_frontend/vgg16_encode_module/pool4 1,512,32,64; 1,512,16,32; 12; 14;15;
+lanenet_model/vgg_frontend/vgg16_encode_module/conv5_1_binary/conv/Conv2D 1,512,16,32; 1,512,16,32; 13; 16;
+lanenet_model/vgg_frontend/vgg16_encode_module/conv5_1_instance/conv/Conv2D 1,512,16,32; 1,512,16,32; 13; 17;
+lanenet_model/vgg_frontend/vgg16_encode_module/conv5_2_binary/conv/Conv2D 1,512,16,32; 1,512,16,32; 14; 18;
+lanenet_model/vgg_frontend/vgg16_encode_module/conv5_2_instance/conv/Conv2D 1,512,16,32; 1,512,16,32; 15; 19;
+lanenet_model/vgg_frontend/vgg16_encode_module/conv5_3_binary/conv/Conv2D 1,512,16,32; 1,512,16,32; 16; 20;
+lanenet_model/vgg_frontend/vgg16_encode_module/conv5_3_instance/conv/Conv2D 1,512,16,32; 1,512,16,32; 17; 21;
+lanenet_model/vgg_frontend/vgg16_decode_module/binary_seg_decode/decode_stage_4_fuse/deconv/deconv/conv2d_transpose 1,512,16,32; 1,512,32,64; 18; 23;
+lanenet_model/vgg_frontend/vgg16_decode_module/instance_seg_decode/decode_stage_4_fuse/deconv/deconv/conv2d_transpose 1,512,16,32; 1,512,32,64; 19; 26;
+lanenet_model/vgg_frontend/vgg16_encode_module/conv4_3/relu___tr4lanenet_model/vgg_frontend/vgg16_decode_module/binary_seg_decode/decode_stage_4_fuse/fuse_feats 1,512,32,64; 1,512,32,64; 12; 24;
+lanenet_model/vgg_frontend/vgg16_decode_module/binary_seg_decode/decode_stage_4_fuse/deconv_relu___tr4lanenet_model/vgg_frontend/vgg16_decode_module/binary_seg_decode/decode_stage_4_fuse/fuse_feats 1,512,32,64; 1,512,32,64; 20; 24;
+lanenet_model/vgg_frontend/vgg16_decode_module/binary_seg_decode/decode_stage_4_fuse/fuse_feats 1,512,32,64;1,512,32,64; 1,512,32,64; 22;23; 28;
+lanenet_model/vgg_frontend/vgg16_encode_module/conv4_3/relu___tr4lanenet_model/vgg_frontend/vgg16_decode_module/instance_seg_decode/decode_stage_4_fuse/fuse_feats 1,512,32,64; 1,512,32,64; 12; 27;
+lanenet_model/vgg_frontend/vgg16_decode_module/instance_seg_decode/decode_stage_4_fuse/deconv_relu___tr4lanenet_model/vgg_frontend/vgg16_decode_module/instance_seg_decode/decode_stage_4_fuse/fuse_feats 1,512,32,64; 1,512,32,64; 21; 27;
+lanenet_model/vgg_frontend/vgg16_decode_module/instance_seg_decode/decode_stage_4_fuse/fuse_feats 1,512,32,64;1,512,32,64; 1,512,32,64; 25;26; 30;
+lanenet_model/vgg_frontend/vgg16_decode_module/binary_seg_decode/decode_stage_4_fuse/fuse_feats___tr4lanenet_model/vgg_frontend/vgg16_decode_module/binary_seg_decode/decode_stage_4_fuse/fuse_gn/FusedBatchNorm 1,512,32,64; 1,512,32,64; 24; 29;
+lanenet_model/vgg_frontend/vgg16_decode_module/binary_seg_decode/decode_stage_4_fuse/fuse_gn/FusedBatchNorm 1,512,32,64; 1,512,32,64; 28; 32;
+lanenet_model/vgg_frontend/vgg16_decode_module/instance_seg_decode/decode_stage_4_fuse/fuse_feats___tr4lanenet_model/vgg_frontend/vgg16_decode_module/instance_seg_decode/decode_stage_4_fuse/fuse_gn/FusedBatchNorm 1,512,32,64; 1,512,32,64; 27; 31;
+lanenet_model/vgg_frontend/vgg16_decode_module/instance_seg_decode/decode_stage_4_fuse/fuse_gn/FusedBatchNorm 1,512,32,64; 1,512,32,64; 30; 33;
+lanenet_model/vgg_frontend/vgg16_decode_module/binary_seg_decode/decode_stage_4_fuse/fuse_relu 1,512,32,64; 1,512,32,64; 29; 34;
+lanenet_model/vgg_frontend/vgg16_decode_module/instance_seg_decode/decode_stage_4_fuse/fuse_relu 1,512,32,64; 1,512,32,64; 31; 35;
+lanenet_model/vgg_frontend/vgg16_decode_module/binary_seg_decode/decode_stage_3_fuse/deconv/deconv/conv2d_transpose 1,512,32,64; 1,256,64,128; 32; 37;
+lanenet_model/vgg_frontend/vgg16_decode_module/instance_seg_decode/decode_stage_3_fuse/deconv/deconv/conv2d_transpose 1,512,32,64; 1,256,64,128; 33; 40;
+lanenet_model/vgg_frontend/vgg16_encode_module/conv3_3/relu___tr4lanenet_model/vgg_frontend/vgg16_decode_module/binary_seg_decode/decode_stage_3_fuse/fuse_feats 1,256,64,128; 1,256,64,128; 8; 38;
+lanenet_model/vgg_frontend/vgg16_decode_module/binary_seg_decode/decode_stage_3_fuse/deconv_relu___tr4lanenet_model/vgg_frontend/vgg16_decode_module/binary_seg_decode/decode_stage_3_fuse/fuse_feats 1,256,64,128; 1,256,64,128; 34; 38;
+lanenet_model/vgg_frontend/vgg16_decode_module/binary_seg_decode/decode_stage_3_fuse/fuse_feats 1,256,64,128;1,256,64,128; 1,256,64,128; 36;37; 42;
+lanenet_model/vgg_frontend/vgg16_encode_module/conv3_3/relu___tr4lanenet_model/vgg_frontend/vgg16_decode_module/instance_seg_decode/decode_stage_3_fuse/fuse_feats 1,256,64,128; 1,256,64,128; 8; 41;
+lanenet_model/vgg_frontend/vgg16_decode_module/instance_seg_decode/decode_stage_3_fuse/deconv_relu___tr4lanenet_model/vgg_frontend/vgg16_decode_module/instance_seg_decode/decode_stage_3_fuse/fuse_feats 1,256,64,128; 1,256,64,128; 35; 41;
+lanenet_model/vgg_frontend/vgg16_decode_module/instance_seg_decode/decode_stage_3_fuse/fuse_feats 1,256,64,128;1,256,64,128; 1,256,64,128; 39;40; 44;
+lanenet_model/vgg_frontend/vgg16_decode_module/binary_seg_decode/decode_stage_3_fuse/fuse_feats___tr4lanenet_model/vgg_frontend/vgg16_decode_module/binary_seg_decode/decode_stage_3_fuse/fuse_gn/FusedBatchNorm 1,256,64,128; 1,256,64,128; 38; 43;
+lanenet_model/vgg_frontend/vgg16_decode_module/binary_seg_decode/decode_stage_3_fuse/fuse_gn/FusedBatchNorm 1,256,64,128; 1,256,64,128; 42; 46;
+lanenet_model/vgg_frontend/vgg16_decode_module/instance_seg_decode/decode_stage_3_fuse/fuse_feats___tr4lanenet_model/vgg_frontend/vgg16_decode_module/instance_seg_decode/decode_stage_3_fuse/fuse_gn/FusedBatchNorm 1,256,64,128; 1,256,64,128; 41; 45;
+lanenet_model/vgg_frontend/vgg16_decode_module/instance_seg_decode/decode_stage_3_fuse/fuse_gn/FusedBatchNorm 1,256,64,128; 1,256,64,128; 44; 47;
+lanenet_model/vgg_frontend/vgg16_decode_module/binary_seg_decode/decode_stage_3_fuse/fuse_relu 1,256,64,128; 1,256,64,128; 43; 48;
+lanenet_model/vgg_frontend/vgg16_decode_module/instance_seg_decode/decode_stage_3_fuse/fuse_relu 1,256,64,128; 1,256,64,128; 45; 49;
+lanenet_model/vgg_frontend/vgg16_decode_module/binary_seg_decode/decode_stage_2_fuse/deconv/deconv/conv2d_transpose 1,256,64,128; 1,128,128,256; 46; 51;
+lanenet_model/vgg_frontend/vgg16_decode_module/instance_seg_decode/decode_stage_2_fuse/deconv/deconv/conv2d_transpose 1,256,64,128; 1,128,128,256; 47; 54;
+lanenet_model/vgg_frontend/vgg16_encode_module/conv2_2/relu___tr4lanenet_model/vgg_frontend/vgg16_decode_module/binary_seg_decode/decode_stage_2_fuse/fuse_feats 1,128,128,256; 1,128,128,256; 4; 52;
+lanenet_model/vgg_frontend/vgg16_decode_module/binary_seg_decode/decode_stage_2_fuse/deconv_relu___tr4lanenet_model/vgg_frontend/vgg16_decode_module/binary_seg_decode/decode_stage_2_fuse/fuse_feats 1,128,128,256; 1,128,128,256; 48; 52;
+lanenet_model/vgg_frontend/vgg16_decode_module/binary_seg_decode/decode_stage_2_fuse/fuse_feats 1,128,128,256;1,128,128,256; 1,128,128,256; 50;51; 56;
+lanenet_model/vgg_frontend/vgg16_encode_module/conv2_2/relu___tr4lanenet_model/vgg_frontend/vgg16_decode_module/instance_seg_decode/decode_stage_2_fuse/fuse_feats 1,128,128,256; 1,128,128,256; 4; 55;
+lanenet_model/vgg_frontend/vgg16_decode_module/instance_seg_decode/decode_stage_2_fuse/deconv_relu___tr4lanenet_model/vgg_frontend/vgg16_decode_module/instance_seg_decode/decode_stage_2_fuse/fuse_feats 1,128,128,256; 1,128,128,256; 49; 55;
+lanenet_model/vgg_frontend/vgg16_decode_module/instance_seg_decode/decode_stage_2_fuse/fuse_feats 1,128,128,256;1,128,128,256; 1,128,128,256; 53;54; 58;
+lanenet_model/vgg_frontend/vgg16_decode_module/binary_seg_decode/decode_stage_2_fuse/fuse_feats___tr4lanenet_model/vgg_frontend/vgg16_decode_module/binary_seg_decode/decode_stage_2_fuse/fuse_gn/FusedBatchNorm 1,128,128,256; 1,128,128,256; 52; 57;
+lanenet_model/vgg_frontend/vgg16_decode_module/binary_seg_decode/decode_stage_2_fuse/fuse_gn/FusedBatchNorm 1,128,128,256; 1,128,128,256; 56; 60;
+lanenet_model/vgg_frontend/vgg16_decode_module/instance_seg_decode/decode_stage_2_fuse/fuse_feats___tr4lanenet_model/vgg_frontend/vgg16_decode_module/instance_seg_decode/decode_stage_2_fuse/fuse_gn/FusedBatchNorm 1,128,128,256; 1,128,128,256; 55; 59;
+lanenet_model/vgg_frontend/vgg16_decode_module/instance_seg_decode/decode_stage_2_fuse/fuse_gn/FusedBatchNorm 1,128,128,256; 1,128,128,256; 58; 61;
+lanenet_model/vgg_frontend/vgg16_decode_module/binary_seg_decode/decode_stage_2_fuse/fuse_relu 1,128,128,256; 1,128,128,256; 57; 62;
+lanenet_model/vgg_frontend/vgg16_decode_module/instance_seg_decode/decode_stage_2_fuse/fuse_relu 1,128,128,256; 1,128,128,256; 59; 63;
+lanenet_model/vgg_frontend/vgg16_decode_module/binary_seg_decode/decode_stage_1_fuse/deconv/deconv/conv2d_transpose 1,128,128,256; 1,64,256,512; 60; 65;
+lanenet_model/vgg_frontend/vgg16_decode_module/instance_seg_decode/decode_stage_1_fuse/deconv/deconv/conv2d_transpose 1,128,128,256; 1,64,256,512; 61; 68;
+lanenet_model/vgg_frontend/vgg16_encode_module/conv1_2/relu___tr4lanenet_model/vgg_frontend/vgg16_decode_module/binary_seg_decode/decode_stage_1_fuse/fuse_feats 1,64,256,512; 1,64,256,512; 1; 66;
+lanenet_model/vgg_frontend/vgg16_decode_module/binary_seg_decode/decode_stage_1_fuse/deconv_relu___tr4lanenet_model/vgg_frontend/vgg16_decode_module/binary_seg_decode/decode_stage_1_fuse/fuse_feats 1,64,256,512; 1,64,256,512; 62; 66;
+lanenet_model/vgg_frontend/vgg16_decode_module/binary_seg_decode/decode_stage_1_fuse/fuse_feats 1,64,256,512;1,64,256,512; 1,64,256,512; 64;65; 70;
+lanenet_model/vgg_frontend/vgg16_encode_module/conv1_2/relu___tr4lanenet_model/vgg_frontend/vgg16_decode_module/instance_seg_decode/decode_stage_1_fuse/fuse_feats 1,64,256,512; 1,64,256,512; 1; 69;
+lanenet_model/vgg_frontend/vgg16_decode_module/instance_seg_decode/decode_stage_1_fuse/deconv_relu___tr4lanenet_model/vgg_frontend/vgg16_decode_module/instance_seg_decode/decode_stage_1_fuse/fuse_feats 1,64,256,512; 1,64,256,512; 63; 69;
+lanenet_model/vgg_frontend/vgg16_decode_module/instance_seg_decode/decode_stage_1_fuse/fuse_feats 1,64,256,512;1,64,256,512; 1,64,256,512; 67;68; 72;
+lanenet_model/vgg_frontend/vgg16_decode_module/binary_seg_decode/decode_stage_1_fuse/fuse_feats___tr4lanenet_model/vgg_frontend/vgg16_decode_module/binary_seg_decode/decode_stage_1_fuse/fuse_gn/FusedBatchNorm 1,64,256,512; 1,64,256,512; 66; 71;
+lanenet_model/vgg_frontend/vgg16_decode_module/binary_seg_decode/decode_stage_1_fuse/fuse_gn/FusedBatchNorm 1,64,256,512; 1,64,256,512; 70; 74;
+lanenet_model/vgg_frontend/vgg16_decode_module/instance_seg_decode/decode_stage_1_fuse/fuse_feats___tr4lanenet_model/vgg_backend/instance_seg/pix_bn/FusedBatchNorm 1,64,256,512; 1,64,256,512; 69; 73;
+lanenet_model/vgg_backend/instance_seg/pix_bn/FusedBatchNorm 1,64,256,512; 1,64,256,512; 72; 75;
+lanenet_model/vgg_frontend/vgg16_decode_module/binary_seg_decode/decode_stage_1_fuse/fuse_relu 1,64,256,512; 1,64,256,512; 71; 76;
+lanenet_model/vgg_backend/instance_seg/pix_relu 1,64,256,512; 1,64,256,512; 73; 77;
+lanenet_model/vgg_frontend/vgg16_decode_module/binary_seg_decode/binary_final_logits/Conv2D 1,64,256,512; 1,2,256,512; 74; 78;
+lanenet_model/vgg_backend/instance_seg/pix_embedding_conv/Conv2D 1,64,256,512; 1,4,256,512; 75; 80;
+lanenet_model/vgg_frontend/vgg16_decode_module/binary_seg_decode/binary_final_logits/Conv2D___tr4lanenet_model/vgg_backend/binary_seg/Softmax 1,2,256,512; 1,2,256,512; 76; 79;
+lanenet_model/vgg_backend/binary_seg/Softmax 1,2,256,512; 1,2,256,512; 78; 82;
+lanenet_model/vgg_backend/instance_seg/pix_embedding_conv/Conv2D___tr4lanenet/final_pixel_embedding_output 1,4,256,512; 1,4,256,512; 77; 81;
+lanenet/final_pixel_embedding_output 1,4,256,512; 256,0,512,4; 80; none
+lanenet_model/vgg_backend/binary_seg/ArgMax 1,2,256,512; 256,0,512,2; 79; 83;
+lanenet/Cast 256,0,512,2; 256,0,512,2; 82; 84;
+lanenet/final_binary_output 256,0,512,2; 512,0,2,0; 83; none
+Start run pipeline in serial hybrid.
+In Pipline::execute mUnits size: 85
+CPU pid is 6ca8 ret 0
+End execute lanenet_model/vgg_frontend/vgg16_encode_module/conv1_1/conv/Conv2D on CPU 69533 11 69544
+End execute lanenet_model/vgg_frontend/vgg16_encode_module/conv1_2/conv/Conv2D on CPU 205767 69575 275342
+End execute lanenet_model/vgg_frontend/vgg16_encode_module/pool1 on CPU 9084 275370 284454
+End execute lanenet_model/vgg_frontend/vgg16_encode_module/conv2_1/conv/Conv2D on CPU 94360 284479 378839
+End execute lanenet_model/vgg_frontend/vgg16_encode_module/conv2_2/conv/Conv2D on CPU 170796 378866 549662
+End execute lanenet_model/vgg_frontend/vgg16_encode_module/pool2 on CPU 4397 549689 554086
+End execute lanenet_model/vgg_frontend/vgg16_encode_module/conv3_1/conv/Conv2D on CPU 89567 554110 643677
+End execute lanenet_model/vgg_frontend/vgg16_encode_module/conv3_2/conv/Conv2D on CPU 171406 643705 815111
+End execute lanenet_model/vgg_frontend/vgg16_encode_module/conv3_3/conv/Conv2D on CPU 160811 815138 975949
+End execute lanenet_model/vgg_frontend/vgg16_encode_module/pool3 on CPU 2856 975977 978833
+End execute lanenet_model/vgg_frontend/vgg16_encode_module/conv4_1/conv/Conv2D on CPU 87671 978862 1066533
+End execute lanenet_model/vgg_frontend/vgg16_encode_module/conv4_2/conv/Conv2D on CPU 184040 1066557 1250597
+End execute lanenet_model/vgg_frontend/vgg16_encode_module/conv4_3/conv/Conv2D on CPU 184213 1250622 1434835
+End execute lanenet_model/vgg_frontend/vgg16_encode_module/pool4 on CPU 1424 1434860 1436284
+End execute lanenet_model/vgg_frontend/vgg16_encode_module/conv5_1_binary/conv/Conv2D on CPU 61469 1436307 1497776
+End execute lanenet_model/vgg_frontend/vgg16_encode_module/conv5_1_instance/conv/Conv2D on CPU 57470 1497799 1555269
+End execute lanenet_model/vgg_frontend/vgg16_encode_module/conv5_2_binary/conv/Conv2D on CPU 62261 1555359 1617620
+End execute lanenet_model/vgg_frontend/vgg16_encode_module/conv5_2_instance/conv/Conv2D on CPU 57974 1617648 1675622
+End execute lanenet_model/vgg_frontend/vgg16_encode_module/conv5_3_binary/conv/Conv2D on CPU 66504 1675649 1742153
+End execute lanenet_model/vgg_frontend/vgg16_encode_module/conv5_3_instance/conv/Conv2D on CPU 56366 1742177 1798543
+End execute lanenet_model/vgg_frontend/vgg16_decode_module/binary_seg_decode/decode_stage_4_fuse/deconv/deconv/conv2d_transpose on CPU 288518 1798569 2087087
+End execute lanenet_model/vgg_frontend/vgg16_decode_module/instance_seg_decode/decode_stage_4_fuse/deconv/deconv/conv2d_transpose on CPU 291187 2087118 2378305
+End execute lanenet_model/vgg_frontend/vgg16_encode_module/conv4_3/relu___tr4lanenet_model/vgg_frontend/vgg16_decode_module/binary_seg_decode/decode_stage_4_fuse/fuse_feats on CPU 4964 2378331 2383295
+End execute lanenet_model/vgg_frontend/vgg16_decode_module/binary_seg_decode/decode_stage_4_fuse/deconv_relu___tr4lanenet_model/vgg_frontend/vgg16_decode_module/binary_seg_decode/decode_stage_4_fuse/fuse_feats on CPU 4979 2383319 2388298
+End execute lanenet_model/vgg_frontend/vgg16_decode_module/binary_seg_decode/decode_stage_4_fuse/fuse_feats on CPU 795 2388325 2389120
+End execute lanenet_model/vgg_frontend/vgg16_encode_module/conv4_3/relu___tr4lanenet_model/vgg_frontend/vgg16_decode_module/instance_seg_decode/decode_stage_4_fuse/fuse_feats on CPU 4912 2389144 2394056
+End execute lanenet_model/vgg_frontend/vgg16_decode_module/instance_seg_decode/decode_stage_4_fuse/deconv_relu___tr4lanenet_model/vgg_frontend/vgg16_decode_module/instance_seg_decode/decode_stage_4_fuse/fuse_feats on CPU 4897 2394070 2398967
+End execute lanenet_model/vgg_frontend/vgg16_decode_module/instance_seg_decode/decode_stage_4_fuse/fuse_feats on CPU 760 2398992 2399752
+End execute lanenet_model/vgg_frontend/vgg16_decode_module/binary_seg_decode/decode_stage_4_fuse/fuse_feats___tr4lanenet_model/vgg_frontend/vgg16_decode_module/binary_seg_decode/decode_stage_4_fuse/fuse_gn/FusedBatchNorm on CPU 4861 2399776 2404637
+End execute lanenet_model/vgg_frontend/vgg16_decode_module/binary_seg_decode/decode_stage_4_fuse/fuse_gn/FusedBatchNorm on CPU 689 2404652 2405341
+End execute lanenet_model/vgg_frontend/vgg16_decode_module/instance_seg_decode/decode_stage_4_fuse/fuse_feats___tr4lanenet_model/vgg_frontend/vgg16_decode_module/instance_seg_decode/decode_stage_4_fuse/fuse_gn/FusedBatchNorm on CPU 4911 2405357 2410268
+End execute lanenet_model/vgg_frontend/vgg16_decode_module/instance_seg_decode/decode_stage_4_fuse/fuse_gn/FusedBatchNorm on CPU 690 2410292 2410982
+End execute lanenet_model/vgg_frontend/vgg16_decode_module/binary_seg_decode/decode_stage_4_fuse/fuse_relu on CPU 703 2410997 2411700
+End execute lanenet_model/vgg_frontend/vgg16_decode_module/instance_seg_decode/decode_stage_4_fuse/fuse_relu on CPU 705 2411722 2412427
+End execute lanenet_model/vgg_frontend/vgg16_decode_module/binary_seg_decode/decode_stage_3_fuse/deconv/deconv/conv2d_transpose on CPU 482791 2412442 2895233
+End execute lanenet_model/vgg_frontend/vgg16_decode_module/instance_seg_decode/decode_stage_3_fuse/deconv/deconv/conv2d_transpose on GPU 1325657 2895258 4220915
+End execute lanenet_model/vgg_frontend/vgg16_encode_module/conv3_3/relu___tr4lanenet_model/vgg_frontend/vgg16_decode_module/binary_seg_decode/decode_stage_3_fuse/fuse_feats on GPU 31829 4220992 4252821
+End execute lanenet_model/vgg_frontend/vgg16_decode_module/binary_seg_decode/decode_stage_3_fuse/deconv_relu___tr4lanenet_model/vgg_frontend/vgg16_decode_module/binary_seg_decode/decode_stage_3_fuse/fuse_feats on GPU 4743 4252877 4257620
+End execute lanenet_model/vgg_frontend/vgg16_decode_module/binary_seg_decode/decode_stage_3_fuse/fuse_feats on GPU 1826 4257668 4259494
+End execute lanenet_model/vgg_frontend/vgg16_encode_module/conv3_3/relu___tr4lanenet_model/vgg_frontend/vgg16_decode_module/instance_seg_decode/decode_stage_3_fuse/fuse_feats on GPU 2641 4259516 4262157
+End execute lanenet_model/vgg_frontend/vgg16_decode_module/instance_seg_decode/decode_stage_3_fuse/deconv_relu___tr4lanenet_model/vgg_frontend/vgg16_decode_module/instance_seg_decode/decode_stage_3_fuse/fuse_feats on GPU 1145 4262282 4263427
+End execute lanenet_model/vgg_frontend/vgg16_decode_module/instance_seg_decode/decode_stage_3_fuse/fuse_feats on GPU 1889 4263450 4265339
+End execute lanenet_model/vgg_frontend/vgg16_decode_module/binary_seg_decode/decode_stage_3_fuse/fuse_feats___tr4lanenet_model/vgg_frontend/vgg16_decode_module/binary_seg_decode/decode_stage_3_fuse/fuse_gn/FusedBatchNorm on GPU 1119 4265366 4266485
+End execute lanenet_model/vgg_frontend/vgg16_decode_module/binary_seg_decode/decode_stage_3_fuse/fuse_gn/FusedBatchNorm on GPU 1262 4266508 4267770
+End execute lanenet_model/vgg_frontend/vgg16_decode_module/instance_seg_decode/decode_stage_3_fuse/fuse_feats___tr4lanenet_model/vgg_frontend/vgg16_decode_module/instance_seg_decode/decode_stage_3_fuse/fuse_gn/FusedBatchNorm on GPU 1154 4267801 4268955
+End execute lanenet_model/vgg_frontend/vgg16_decode_module/instance_seg_decode/decode_stage_3_fuse/fuse_gn/FusedBatchNorm on GPU 1230 4268973 4270203
+End execute lanenet_model/vgg_frontend/vgg16_decode_module/binary_seg_decode/decode_stage_3_fuse/fuse_relu on GPU 1081 4270222 4271303
+End execute lanenet_model/vgg_frontend/vgg16_decode_module/instance_seg_decode/decode_stage_3_fuse/fuse_relu on GPU 1040 4271322 4272362
+End execute lanenet_model/vgg_frontend/vgg16_decode_module/binary_seg_decode/decode_stage_2_fuse/deconv/deconv/conv2d_transpose on GPU 272429 4272380 4544809
+End execute lanenet_model/vgg_frontend/vgg16_decode_module/instance_seg_decode/decode_stage_2_fuse/deconv/deconv/conv2d_transpose on GPU 273328 4544905 4818233
+End execute lanenet_model/vgg_frontend/vgg16_encode_module/conv2_2/relu___tr4lanenet_model/vgg_frontend/vgg16_decode_module/binary_seg_decode/decode_stage_2_fuse/fuse_feats on GPU 40617 4818315 4858932
+End execute lanenet_model/vgg_frontend/vgg16_decode_module/binary_seg_decode/decode_stage_2_fuse/deconv_relu___tr4lanenet_model/vgg_frontend/vgg16_decode_module/binary_seg_decode/decode_stage_2_fuse/fuse_feats on GPU 2545 4858966 4861511
+End execute lanenet_model/vgg_frontend/vgg16_decode_module/binary_seg_decode/decode_stage_2_fuse/fuse_feats on GPU 3210 4861525 4864735
+End execute lanenet_model/vgg_frontend/vgg16_encode_module/conv2_2/relu___tr4lanenet_model/vgg_frontend/vgg16_decode_module/instance_seg_decode/decode_stage_2_fuse/fuse_feats on GPU 4958 4864749 4869707
+End execute lanenet_model/vgg_frontend/vgg16_decode_module/instance_seg_decode/decode_stage_2_fuse/deconv_relu___tr4lanenet_model/vgg_frontend/vgg16_decode_module/instance_seg_decode/decode_stage_2_fuse/fuse_feats on GPU 2306 4869735 4872041
+End execute lanenet_model/vgg_frontend/vgg16_decode_module/instance_seg_decode/decode_stage_2_fuse/fuse_feats on GPU 3191 4872055 4875246
+End execute lanenet_model/vgg_frontend/vgg16_decode_module/binary_seg_decode/decode_stage_2_fuse/fuse_feats___tr4lanenet_model/vgg_frontend/vgg16_decode_module/binary_seg_decode/decode_stage_2_fuse/fuse_gn/FusedBatchNorm on GPU 2870 4875259 4878129
+End execute lanenet_model/vgg_frontend/vgg16_decode_module/binary_seg_decode/decode_stage_2_fuse/fuse_gn/FusedBatchNorm on GPU 3350 4878197 4881547
+End execute lanenet_model/vgg_frontend/vgg16_decode_module/instance_seg_decode/decode_stage_2_fuse/fuse_feats___tr4lanenet_model/vgg_frontend/vgg16_decode_module/instance_seg_decode/decode_stage_2_fuse/fuse_gn/FusedBatchNorm on GPU 3131 4881602 4884733
+End execute lanenet_model/vgg_frontend/vgg16_decode_module/instance_seg_decode/decode_stage_2_fuse/fuse_gn/FusedBatchNorm on GPU 3210 4884781 4887991
+End execute lanenet_model/vgg_frontend/vgg16_decode_module/binary_seg_decode/decode_stage_2_fuse/fuse_relu on GPU 3200 4888041 4891241
+End execute lanenet_model/vgg_frontend/vgg16_decode_module/instance_seg_decode/decode_stage_2_fuse/fuse_relu on GPU 2936 4891291 4894227
+End execute lanenet_model/vgg_frontend/vgg16_decode_
\ No newline at end of file
diff --git a/demo/android/app/includes/MNN/Interpreter.hpp b/demo/android/app/includes/MNN/Interpreter.hpp
index b5eae79..cd7bb05 100644
--- a/demo/android/app/includes/MNN/Interpreter.hpp
+++ b/demo/android/app/includes/MNN/Interpreter.hpp
@@ -26,7 +26,10 @@ struct ScheduleConfig {
     MNNForwardType type = MNN_FORWARD_CPU;
     /** number of threads in parallel */
     int numThread = 4;
-
+    /** Set is parallel */
+    MNNExecutionType executionType = false;
+    /** Set op to device map file */
+    char * deviceMapFilePath = nullptr;
     /** subpath to run */
     struct Path {
         std::vector<std::string> inputs;
diff --git a/demo/android/app/includes/MNN/Tensor.hpp b/demo/android/app/includes/MNN/Tensor.hpp
index 33f8d88..8487604 100644
--- a/demo/android/app/includes/MNN/Tensor.hpp
+++ b/demo/android/app/includes/MNN/Tensor.hpp
@@ -15,6 +15,7 @@
 
 namespace MNN {
 
+
 /**
  * data container.
  * data for host tensor is saved in `host` field. its memory is allocated malloc directly.
diff --git a/include/MNN/Interpreter.hpp b/include/MNN/Interpreter.hpp
index b3f547b..63cf135 100644
--- a/include/MNN/Interpreter.hpp
+++ b/include/MNN/Interpreter.hpp
@@ -15,6 +15,7 @@
 #include <MNN/ErrorCode.hpp>
 #include <MNN/MNNForwardType.h>
 #include <MNN/Tensor.hpp>
+#include "core/MUtils.hpp"
 
 namespace MNN {
 
@@ -26,6 +27,12 @@ struct ScheduleConfig {
     MNNForwardType type = MNN_FORWARD_CPU;
     /** number of threads in parallel */
     int numThread = 4;
+    /** Set is parallel */
+    MNNExecutionType executionType = MNNExecutionType::MNN_EXECUTION_DEFAULT;
+    /** Set op to device map file */
+    char * deviceMapFilePath = nullptr;
+    /** When run model with Big&Little, use this as little thread number*/
+    int CPULittleNumThread = 1;
 
     /** subpath to run */
     struct Path {
@@ -166,7 +173,7 @@ public:
      * @return result of running.
      */
     ErrorCode runSession(Session* session) const;
-
+    ErrorCode runSession(Session* session, std::vector<RunMetadata> *net_stats);
     /*
      * @brief run session.
      * @param session   given session.
diff --git a/include/MNN/MNNForwardType.h b/include/MNN/MNNForwardType.h
index f9059af..e7d5cef 100644
--- a/include/MNN/MNNForwardType.h
+++ b/include/MNN/MNNForwardType.h
@@ -39,9 +39,37 @@ typedef enum {
     MNN_FORWARD_USER_1 = 9,
     MNN_FORWARD_USER_2 = 10,
     MNN_FORWARD_USER_3 = 11,
+    MNN_FORWARD_CONVERT = 12,
 
     MNN_FORWARD_ALL
 } MNNForwardType;
+
+
+typedef enum {
+    /* DEFAULT: Execute units in serial using CPU or GPU backend */
+    MNN_EXECUTION_DEFAULT = 0,
+    /* SERIAL_HYBRID: Execute units in serial, but execute one unit on CPU or GPU backend */
+    MNN_EXECUTION_SERIAL_HYBRID = 1,
+    /* PARALLEL: Execute units in parallel using CPU and GPU */
+    MNN_EXECUTION_PARALLEL = 2,
+    /* BIG_LITTEL: Execute units in parallel using big cores and little core, Not Support yet */
+    MNN_EXECUTION_BIG_LITTLE = 3,
+} MNNExecutionType;
+
+/**
+ * The concept of big/little is not strictly aligned with the real Big/LITTLE archtecture
+ * On symmetrical CPU cores (like 8-core CPU) we split different CPU cores to big/little
+ * */ 
+typedef enum {
+    /* BIG; Execute units one CPU's big cluster */
+    MNN_CPU_BIG = 0,
+    /* MEDIUM; Execute units one CPU's medium cluster. */
+    MNN_CPU_MEDIUM = 1,
+    /* BIG; Execute units one CPU's big cluster */
+    MNN_CPU_LITTLE = 2,
+} MNNCpuClusterType;
+
+
 #ifdef __cplusplus
 namespace MNN {
 struct BackendConfig {
diff --git a/parallelmnn.patch b/parallelmnn.patch
new file mode 100644
index 0000000..2b3c653
--- /dev/null
+++ b/parallelmnn.patch
@@ -0,0 +1,1386 @@
+diff --git a/3rd_party/OpenCLHeaders/CL/cl2.hpp b/3rd_party/OpenCLHeaders/CL/cl2.hpp
+index 6023234..64a78ed 100644
+--- a/3rd_party/OpenCLHeaders/CL/cl2.hpp
++++ b/3rd_party/OpenCLHeaders/CL/cl2.hpp
+@@ -3675,6 +3675,18 @@ public:
+         }
+     }
+ 
++    Buffer(
++        const Context& context,
++        cl_mem_flags flags,
++        size_type size,
++        void* host_ptr,
++        float, float, float, float)
++    {
++        cl_int error;
++        object_ = ::clCreateBuffer(context(), flags, size, host_ptr, &error);
++
++        detail::errHandler(error, __CREATE_BUFFER_ERR);
++    }
+     /*! \brief Constructs a Buffer in the default context.
+      *
+      *  Wraps clCreateBuffer().
+diff --git a/CMakeLists.txt b/CMakeLists.txt
+index 9c97b4a..8f6c143 100644
+--- a/CMakeLists.txt
++++ b/CMakeLists.txt
+@@ -58,7 +58,7 @@ option(MNN_BUILD_HARD "Build -mfloat-abi=hard or not" OFF)
+ option(MNN_BUILD_SHARED_LIBS "MNN build shared or static lib" ON)
+ option(MNN_FORBID_MULTI_THREAD "Disable Multi Thread" OFF)
+ option(MNN_OPENMP "Enable Multiple Thread Linux|Android" ON)
+-option(MNN_USE_THREAD_POOL "Use Multiple Thread by Self ThreadPool" ON)
++option(MNN_USE_THREAD_POOL "Use Multiple Thread by Self ThreadPool" OFF)
+ option(MNN_BUILD_TRAIN "Build Train Tools" OFF)
+ option(MNN_BUILD_DEMO "Build demo/exec or not" OFF)
+ option(MNN_BUILD_QUANTOOLS "Build Quantized Tools or not" OFF)
+@@ -75,7 +75,7 @@ option(MNN_USE_SSE "Enable SSE Optimizations" ON)
+ option(MNN_USE_AVX "Enable AVX Optimizations" ON)
+ option(NATIVE_LIBRARY_OUTPUT "Native Library Path" OFF)
+ option(NATIVE_INCLUDE_OUTPUT "Native Include Path" OFF)
+-
++message(STATUS, ${MNN_USE_THREAD_POOL})
+ 
+ set(MNN_SCHEMA_SUFFIX "default" CACHE STRING "MNN Schema Source Path Suffix")
+ IF(APPLE AND MNN_AAPL_FMWK AND MNN_SEP_BUILD)
+@@ -89,7 +89,6 @@ if (MNN_USE_THREAD_POOL)
+     set(MNN_OPENMP OFF)
+     add_definitions(-DMNN_USE_THREAD_POOL)
+ endif()
+-
+ if(MNN_FORBID_MULTI_THREAD)
+     add_definitions(-DMNN_FORBIT_MULTI_THREADS)
+ endif()
+@@ -128,13 +127,16 @@ if (MNN_METAL)
+ endif()
+ 
+ # target options
+-option(MNN_BUILD_BENCHMARK "Build benchmark or not" OFF)
+-option(MNN_BUILD_TEST "Build tests or not" OFF)
++option(MNN_BUILD_BENCHMARK "Build benchmark or not" ON)
++IF(MNN_BUILD_BENCHMARK)
++  add_definitions(-DMNN_BENCHMARK)
++ENDIF()
++option(MNN_BUILD_TEST "Build tests or not" ON)
+ option(MNN_BUILD_FOR_ANDROID_COMMAND "Build from command" OFF)
+ set (MNN_HIDDEN FALSE)
+ IF(CMAKE_BUILD_TYPE MATCHES Debug)
+ ELSE()
+-    set(MNN_HIDDEN TRUE)
++    set(MNN_HIDDEN FALSE)
+ ENDIF(CMAKE_BUILD_TYPE MATCHES Debug)
+ 
+ 
+@@ -406,7 +408,7 @@ if(APPLE)
+       ELSEIF(${CMAKE_SYSTEM_NAME} MATCHES "Android")
+           add_definitions(-DMNN_USE_OPENCL_WRAPPER)
+           #add_definitions(-DENABLE_OPENCL_TURNING_PROFILER)
+-          #add_definitions(-DLOG_VERBOSE)
++          add_definitions(-DLOG_VERBOSE)
+       ENDIF()
+     ENDIF()
+ endif()
+@@ -486,6 +488,7 @@ if (NOT APPLE)
+           set(OpenMP_CXX_FLAGS "/openmp ${OpenMP_CXX_FLAGS}")
+       endif()
+       FOREACH(TARGET ${MNN_TARGETS})
++          message("target_link_libraries ${TARGET} ${OpenMP_CXX_LIBRARIES}")
+        target_link_libraries(${TARGET} PUBLIC ${OpenMP_CXX_LIBRARIES})
+        IF(WIN32)
+             target_compile_options(${TARGET} PUBLIC /openmp ${OpenMP_CXX_FLAGS} ${OpenMP_C_FLAGS})
+@@ -500,6 +503,7 @@ list(APPEND MNN_TARGETS MNN)
+     add_dependencies(${TARGET} MNN_SCHEMA_GEN)
+     IF((NOT MSVC) AND (NOT WIN32))
+       target_compile_options(${TARGET} PUBLIC -fomit-frame-pointer -fstrict-aliasing -ffunction-sections -fdata-sections -ffast-math -fno-rtti)
++      # target_compile_options(${TARGET} PUBLIC -fomit-frame-pointer -fstrict-aliasing -ffunction-sections -fdata-sections -ffast-math)
+       target_compile_options(${TARGET} PRIVATE -fno-exceptions)
+       if(MNN_HIDDEN)
+           target_compile_options(${TARGET} PRIVATE -fvisibility-inlines-hidden -fvisibility=hidden)
+diff --git a/benchmark/CMakeLists.txt b/benchmark/CMakeLists.txt
+index 25c2e91..835d237 100644
+--- a/benchmark/CMakeLists.txt
++++ b/benchmark/CMakeLists.txt
+@@ -1,8 +1,12 @@
+ IF(MNN_BUILD_BENCHMARK)
++
+   add_executable(benchmark.out ${CMAKE_CURRENT_LIST_DIR}/benchmark.cpp ${CMAKE_CURRENT_SOURCE_DIR}/tools/cpp/revertMNNModel.cpp)
+   target_include_directories(benchmark.out PRIVATE ${CMAKE_CURRENT_SOURCE_DIR}/tools/cpp/ ${CMAKE_CURRENT_SOURCE_DIR}/tools/)
+   target_link_libraries(benchmark.out ${MNN_DEPS})
+ 
++  add_executable(benchmarkTensorTransform ${CMAKE_CURRENT_LIST_DIR}/benchmarkTensorTransform.cpp)
++  target_link_libraries(benchmarkTensorTransform ${MNN_DEPS})
++
+   file(GLOB_RECURSE SRC_FILES ${CMAKE_CURRENT_LIST_DIR}/exprModels/*.cpp)
+   add_executable(benchmarkExprModels.out ${CMAKE_CURRENT_LIST_DIR}/benchmarkExprModels.cpp ${SRC_FILES})
+   target_include_directories(benchmarkExprModels.out PRIVATE "${CMAKE_CURRENT_LIST_DIR}/exprModels" ${CMAKE_CURRENT_SOURCE_DIR}/)
+diff --git a/benchmark/bench_android.sh b/benchmark/bench_android.sh
+index fef57d5..462708a 100755
+--- a/benchmark/bench_android.sh
++++ b/benchmark/bench_android.sh
+@@ -1,7 +1,7 @@
+ set -e
+ ABI="armeabi-v7a"
+ OPENMP="ON"
+-VULKAN="ON"
++VULKAN="OFF"
+ OPENCL="ON"
+ OPENGL="OFF"
+ RUN_LOOP=10
+@@ -32,16 +32,17 @@ function clean_build() {
+     rm -rf $1
+     mkdir $1
+ }
+-
++# -DCMAKE_BUILD_TYPE=Debug \
+ function build_android_bench() {
+     if [ "-c" == "$CLEAN" ]; then
+         clean_build $BUILD_DIR
+     fi
+     mkdir -p build
+     cd $BUILD_DIR
++    rm ../../source/backend/opencl/execution/cl/opencl_program.cc
++    echo "Removed opencl_program.cc"
+     cmake ../../ \
+           -DCMAKE_TOOLCHAIN_FILE=$ANDROID_NDK/build/cmake/android.toolchain.cmake \
+-          -DCMAKE_BUILD_TYPE=Release \
+           -DANDROID_ABI="${ABI}" \
+           -DANDROID_STL=c++_static \
+           -DCMAKE_BUILD_TYPE=Release \
+@@ -51,22 +52,30 @@ function build_android_bench() {
+           -DMNN_OPENCL:BOOL=$OPENCL \
+           -DMNN_OPENMP:BOOL=$OPENMP \
+           -DMNN_OPENGL:BOOL=$OPENGL \
++          -DMNN_BUILD_TEST:BOOL=ON \
+           -DMNN_DEBUG:BOOL=OFF \
++          -DMNN_USE_THREAD_POOL:BOOL=OFF \
+           -DMNN_BUILD_BENCHMARK:BOOL=ON \
+           -DMNN_BUILD_FOR_ANDROID_COMMAND=true \
+           -DNATIVE_LIBRARY_OUTPUT=.
+-    make -j8 benchmark.out timeProfile.out
++    #make -j8 benchmark.out timeProfile.out benchmarkTensorTransform run_test.out
++    make -j8 benchmark.out timeProfile.out benchmarkTensorTransform
+ }
+ 
+ function bench_android() {
+     build_android_bench
++    cd arm64-v8a
+     find . -name "*.so" | while read solib; do
++        echo $solib
+         adb push $solib  $ANDROID_DIR
+     done
++    cd -
+     adb push benchmark.out $ANDROID_DIR
+     adb push timeProfile.out $ANDROID_DIR
+     adb shell chmod 0777 $ANDROID_DIR/benchmark.out
+-
++    adb push benchmarkTensorTransform $ANDROID_DIR
++    # adb push run_test.out $ANDROID_DIR
++    exit 0
+     if [ "" != "$PUSH_MODEL" ]; then
+         adb shell "rm -rf $ANDROID_DIR/benchmark_models"
+         adb push $BENCHMARK_MODEL_DIR $ANDROID_DIR/benchmark_models
+@@ -75,9 +84,9 @@ function bench_android() {
+     adb shell "echo >> $ANDROID_DIR/benchmark.txt"
+     adb shell "echo Build Flags: ABI=$ABI  OpenMP=$OPENMP Vulkan=$VULKAN OpenCL=$OPENCL >> $ANDROID_DIR/benchmark.txt"
+     #benchmark  CPU
+-    adb shell "LD_LIBRARY_PATH=$ANDROID_DIR $ANDROID_DIR/benchmark.out $ANDROID_DIR/benchmark_models $RUN_LOOP $FORWARD_TYPE 2>$ANDROID_DIR/benchmark.err >> $ANDROID_DIR/benchmark.txt"
++    # adb shell "LD_LIBRARY_PATH=$ANDROID_DIR $ANDROID_DIR/benchmark.out $ANDROID_DIR/benchmark_models $RUN_LOOP $FORWARD_TYPE 2>$ANDROID_DIR/benchmark.err >> $ANDROID_DIR/benchmark.txt"
+     #benchmark  Vulkan
+-    adb shell "LD_LIBRARY_PATH=$ANDROID_DIR $ANDROID_DIR/benchmark.out $ANDROID_DIR/benchmark_models $RUN_LOOP 7 2>$ANDROID_DIR/benchmark.err >> $ANDROID_DIR/benchmark.txt"
++    # adb shell "LD_LIBRARY_PATH=$ANDROID_DIR $ANDROID_DIR/benchmark.out $ANDROID_DIR/benchmark_models $RUN_LOOP 7 2>$ANDROID_DIR/benchmark.err >> $ANDROID_DIR/benchmark.txt"
+     #benchmark OpenGL
+     #adb shell "LD_LIBRARY_PATH=$ANDROID_DIR $ANDROID_DIR/benchmark.out $ANDROID_DIR/benchmark_models 10 6 2>$ANDROID_DIR/benchmark.err >> $ANDROID_DIR/benchmark.txt"
+     #benchmark OpenCL
+diff --git a/benchmark/benchmark.cpp b/benchmark/benchmark.cpp
+index 5646df5..ee09d24 100644
+--- a/benchmark/benchmark.cpp
++++ b/benchmark/benchmark.cpp
+@@ -16,6 +16,7 @@
+ #include <fstream>
+ #include <iostream>
+ #include <vector>
++#include <unistd.h>
+ #if defined(_MSC_VER)
+ #include <Windows.h>
+ #undef min
+@@ -32,6 +33,7 @@
+ #include <MNN/MNNDefine.h>
+ #include <MNN/Tensor.hpp>
+ #include "revertMNNModel.hpp"
++#include "core/MUtils.hpp"
+ /**
+  TODOs:
+  1. dynamically get CPU related info.
+@@ -99,25 +101,11 @@ void setInputData(MNN::Tensor* tensor) {
+     }
+ }
+ 
+-static inline uint64_t getTimeInUs() {
+-    uint64_t time;
+-#if defined(_MSC_VER)
+-    LARGE_INTEGER now, freq;
+-    QueryPerformanceCounter(&now);
+-    QueryPerformanceFrequency(&freq);
+-    uint64_t sec = now.QuadPart / freq.QuadPart;
+-    uint64_t usec = (now.QuadPart % freq.QuadPart) * 1000000 / freq.QuadPart;
+-    time = sec * 1000000 + usec;
+-#else
+-    struct timeval tv;
+-    gettimeofday(&tv, nullptr);
+-    time = static_cast<uint64_t>(tv.tv_sec) * 1000000 + tv.tv_usec;
+-#endif
+-    return time;
+-}
+ 
+ std::vector<float> doBench(Model& model, int loop, int forward = MNN_FORWARD_CPU, bool only_inference = true,
+-                           int numberThread = 4, int precision = 2) {
++                           int numberThread = 4, int precision = 2,
++                           MNNExecutionType executionType = MNNExecutionType::MNN_EXECUTION_DEFAULT, 
++                           char * filePath = nullptr, int CPULittleNumThread = 1) {
+     auto revertor = std::unique_ptr<Revert>(new Revert(model.model_file.c_str()));
+     revertor->initialize();
+     auto modelBuffer      = revertor->getBuffer();
+@@ -127,6 +115,9 @@ std::vector<float> doBench(Model& model, int loop, int forward = MNN_FORWARD_CPU
+     MNN::ScheduleConfig config;
+     config.numThread = numberThread;
+     config.type      = static_cast<MNNForwardType>(forward);
++    config.executionType = executionType;
++    config.deviceMapFilePath = filePath;
++    config.CPULittleNumThread = CPULittleNumThread;
+     MNN::BackendConfig backendConfig;
+     backendConfig.precision = (MNN::BackendConfig::PrecisionMode)precision;
+     backendConfig.power = MNN::BackendConfig::Power_High;
+@@ -149,21 +140,25 @@ std::vector<float> doBench(Model& model, int loop, int forward = MNN_FORWARD_CPU
+     auto outputTensor = net->getSessionOutput(session, NULL);
+     std::shared_ptr<MNN::Tensor> expectTensor(MNN::Tensor::createHostTensorFromDevice(outputTensor, false));
+     // Warming up...
++    MNN_PRINT("Run 3 times to warm up\n");
+     for (int i = 0; i < 3; ++i) {
+         input->copyFromHostTensor(givenTensor.get());
+         net->runSession(session);
+         outputTensor->copyToHostTensor(expectTensor.get());
+     }
++    MNN_PRINT("Warm up done\n");
+ 
+     for (int round = 0; round < loop; round++) {
+-        auto timeBegin = getTimeInUs();
++        auto timeBegin = MNN::getTimeInUs();
+ 
+         input->copyFromHostTensor(givenTensor.get());
+         net->runSession(session);
+         outputTensor->copyToHostTensor(expectTensor.get());
+ 
+-        auto timeEnd = getTimeInUs();
++        auto timeEnd = MNN::getTimeInUs();
+         costs.push_back((timeEnd - timeBegin) / 1000.0);
++        MNN_PRINT("\n<-- Run %d iterations -->\n\n", round + 1);
++        // sleep(1);
+     }
+     return costs;
+ }
+@@ -176,7 +171,7 @@ void displayStats(const std::string& name, const std::vector<float>& costs) {
+         sum += v;
+     }
+     avg = costs.size() > 0 ? sum / costs.size() : 0;
+-    printf("[ - ] %-24s    max = %8.3fms  min = %8.3fms  avg = %8.3fms\n", name.c_str(), max, avg == 0 ? 0 : min, avg);
++    printf("[ - ] %-24s    max = %8.3f ms  min = %8.3f ms  avg = %8.3f ms\n", name.c_str(), max, avg == 0 ? 0 : min, avg);
+ }
+ static inline std::string forwardType(MNNForwardType type) {
+     switch (type) {
+@@ -193,11 +188,14 @@ static inline std::string forwardType(MNNForwardType type) {
+     }
+     return "N/A";
+ }
++
+ int main(int argc, const char* argv[]) {
+     std::cout << "MNN benchmark" << std::endl;
+     int loop               = 10;
+     MNNForwardType forward = MNN_FORWARD_CPU;
+     int numberThread       = 4;
++    MNNExecutionType executionType = MNNExecutionType::MNN_EXECUTION_DEFAULT;
++    char* deviceMapFilePath = (char*)"/data/local/tmp/redmi_cpu-4-device-placement.txt";
+     if (argc <= 2) {
+         std::cout << "Usage: " << argv[0] << " models_folder [loop_count] [forwardtype]" << std::endl;
+         return 1;
+@@ -215,12 +213,41 @@ int main(int argc, const char* argv[]) {
+     if (argc >= 6) {
+         precision = atoi(argv[5]);
+     }
+-    std::cout << "Forward type: **" << forwardType(forward) << "** thread=" << numberThread << "** precision=" <<precision << std::endl;
++    if(argc >= 7){
++        auto execution = atoi(argv[6]);
++        switch(execution){
++            case MNNExecutionType::MNN_EXECUTION_DEFAULT:
++                executionType = MNNExecutionType::MNN_EXECUTION_DEFAULT;
++                break;
++            case MNNExecutionType::MNN_EXECUTION_SERIAL_HYBRID:
++                executionType = MNNExecutionType::MNN_EXECUTION_SERIAL_HYBRID;
++                break;
++            case MNNExecutionType::MNN_EXECUTION_PARALLEL:
++                executionType = MNNExecutionType::MNN_EXECUTION_PARALLEL;
++                break;
++            case MNNExecutionType::MNN_EXECUTION_BIG_LITTLE:
++                executionType = MNNExecutionType::MNN_EXECUTION_BIG_LITTLE;
++                break;
++            default:
++            MNN_PRINT("Invalid execution type %d\n", execution);
++        }
++
++    }
++    if(argc >= 8){
++        deviceMapFilePath = (char*)argv[7];
++    }
++    int CPULittleThreadNum = 1;
++    if(argc >=9){
++        CPULittleThreadNum = atoi(argv[8]);
++    }
++    std::cout << "Forward type: **" << forwardType(forward) << "** thread=" << numberThread 
++      << "** precision=" <<precision << "** execution type=" << executionType <<std::endl;
+     std::vector<Model> models = findModelFiles(argv[1]);
+ 
+     std::cout << "--------> Benchmarking... loop = " << argv[2] << std::endl;
+     for (auto& m : models) {
+-        std::vector<float> costs = doBench(m, loop, forward, false, numberThread, precision);
++        std::vector<float> costs = doBench(m, loop, forward, false, numberThread, 
++          precision, executionType, deviceMapFilePath, CPULittleThreadNum);
+         displayStats(m.name, costs);
+     }
+ }
+diff --git a/benchmark/benchmarkExecution.cpp b/benchmark/benchmarkExecution.cpp
+new file mode 100644
+index 0000000..0c6ff10
+--- /dev/null
++++ b/benchmark/benchmarkExecution.cpp
+@@ -0,0 +1,9 @@
++
++#include "backend/cpu/CPUMatMul.hpp"
++
++int main(int argc, char* argv[]){
++    
++    MNN::CPUBackend* cpuBackend = new MNN::CPUBackend();
++    
++    return 0;
++}
+\ No newline at end of file
+diff --git a/benchmark/benchmarkTensorTransform.cpp b/benchmark/benchmarkTensorTransform.cpp
+new file mode 100644
+index 0000000..5e02480
+--- /dev/null
++++ b/benchmark/benchmarkTensorTransform.cpp
+@@ -0,0 +1,94 @@
++
++#include <memory.h>
++#include <stdio.h>
++#include <stdlib.h>
++#include <sys/time.h>
++#include <unistd.h>
++#include <assert.h>
++
++#include <vector>
++
++#include <MNN/Tensor.hpp>
++#include "backend/opencl/core/ImageBufferConvertor.hpp"
++#include "backend/opencl/core/OpenCLBackend.hpp"
++#include "core/MUtils.hpp"
++
++
++int main(int argc, const char *argv[]) {
++  int n = 1, c = 8, h = 16, w = 16;
++  int loop_count = 100;
++  
++  if (argc > 4) {
++    n = atoi(argv[1]);
++    c = atoi(argv[2]);
++    h = atoi(argv[3]);
++    w = atoi(argv[4]);
++  }
++  if (argc > 5) {
++    loop_count = atoi(argv[5]);
++  }
++  
++  const std::vector<int> input_shape = {n, c, h, w};
++  std::vector<float *> input_data_vector(loop_count);
++  std::vector<MNN::Tensor *> input_tensor_vector(loop_count);
++  printf("Start initialize\n");
++  // Create and initialize input tensors
++  for (int i = 0; i < loop_count; ++i) {
++    input_data_vector[i] = new float[n * c * h * w];
++    input_tensor_vector[i] = MNN::Tensor::create(
++        input_shape, halide_type_of<float>(), (void *)input_data_vector[i],
++        MNN::Tensor::DimensionType::TENSORFLOW);
++    auto data = input_tensor_vector[i]->host<float>();
++    for (int j = 0; j < input_tensor_vector[i]->elementSize(); ++j) {
++      data[j] = (float)j;
++    }
++    // MNN_PRINT("loop %d\n", i);
++    // printf("%d set value\n", i);
++    // if(n*c*h*w <= 1*16*32*32){
++    //     input_tensor_vector[i]->print();
++    // }
++  }
++  
++  auto backend = new MNN::OpenCL::OpenCLBackend(
++      MNN::BackendConfig::BackendConfig::Precision_Low,
++      MNN::BackendConfig::PowerMode::Power_High);
++  
++  std::vector<MNN::Tensor *> output_tensor_vector(loop_count);
++  for (int i = 0; i < loop_count; ++i) {
++    // MNN_PRINT("loop %d\n", i);
++    output_tensor_vector[i] = MNN::Tensor::createDevice<float>(
++        input_shape, MNN::Tensor::DimensionType::TENSORFLOW);
++    backend->onAcquireBuffer(output_tensor_vector[i],
++                             MNN::Backend::StorageType::DYNAMIC_SEPERATE);
++   // MNN_PRINT("loop %d\n", i);
++  }
++
++  // Warm up
++  auto start = MNN::getTimeInUs();
++  for (int i = 0; i < loop_count; ++i) {
++      //  MNN_PRINT("loop %d\n", i);
++    assert(output_tensor_vector[i]->buffer().host == NULL);
++    backend->onCopyBuffer(input_tensor_vector[i], output_tensor_vector[i]);
++    auto host_tensor = MNN::Tensor::create<float>(input_shape, NULL, MNN::Tensor::DimensionType::TENSORFLOW);
++    backend->onCopyBuffer(output_tensor_vector[i], host_tensor);
++    backend->onWaitFinish();
++  }
++  auto end = MNN::getTimeInUs();
++  for (int i = 0; i < loop_count; ++i) {
++    delete[] input_data_vector[i];
++  }
++  // input_tensor_vector[0]->print();
++  // printf("-----------------");
++  // output_tensor_vector[0]->print();
++  // MNN::Tensor* output_host_tensor = new MNN::Tensor(input_tensor,
++  // MNN::Tensor::DimensionType::CAFFE_C4, true);
++  // output_tensor->copyToHostTensor(output_host_tensor);
++  printf("Convert %d times %d,%d,%d,%d used %lu\n", loop_count, n, c, h, w,
++         (end - start)/loop_count);
++  // printf("Output host is %d,  device is %d\n", output_tensor->buffer().host
++  // != NULL, output_tensor->buffer().device != NULL);
++  // output_host_tensor->print();
++  // Destroy tensors
++
++  return 0;
++}
+diff --git a/benchmark/merge_buffer_to_image_data.py b/benchmark/merge_buffer_to_image_data.py
+new file mode 100644
+index 0000000..a54f11e
+--- /dev/null
++++ b/benchmark/merge_buffer_to_image_data.py
+@@ -0,0 +1,19 @@
++
++f = open("redmi_nc4hw4_buffer_to_image.csv", 'r')
++shape_time_map = {}
++num_shapes = 30
++count = 0
++shape_list = []
++for line in f.readlines():
++    #print(line)
++    com = line.strip().split(' ')
++    count += 1
++    if com[0] in shape_time_map.keys():
++        shape_time_map[com[0]] = shape_time_map[com[0]] + int(com[1])
++    else:
++        shape_list.append(com[0])
++        shape_time_map[com[0]] = int(com[1])
++
++print(count)
++for shape in shape_list:
++    print("%s %d" % (shape, shape_time_map[shape]/(count/num_shapes)))
+diff --git a/benchmark/redmi_nc4hw4_buffer_to_image.csv b/benchmark/redmi_nc4hw4_buffer_to_image.csv
+new file mode 100644
+index 0000000..80acc1d
+--- /dev/null
++++ b/benchmark/redmi_nc4hw4_buffer_to_image.csv
+@@ -0,0 +1,300 @@
++(1,32,149,149) 425779
++(1,32,147,147) 410459
++(1,64,147,147) 791266
++(1,64,73,73) 330767
++(1,80,73,73) 376793
++(1,192,71,71) 605193
++(1,192,35,35) 296446
++(1,64,35,35) 275839
++(1,48,35,35) 221221
++(1,32,35,35) 232124
++(1,96,35,35) 237600
++(1,256,35,35) 324612
++(1,288,35,35) 343965
++(1,288,17,17) 228464
++(1,384,17,17) 255114
++(1,96,17,17) 198113
++(1,768,17,17) 300570
++(1,128,17,17) 259468
++(1,192,17,17) 228813
++(1,160,17,17) 225798
++(1,768,8,8) 423304
++(1,320,8,8) 256902
++(1,192,8,8) 205320
++(1,1280,8,8) 241680
++(1,448,8,8) 395391
++(1,384,8,8) 249235
++(1,2048,8,8) 274189
++(1,2048,1,1) 199137
++(1,1001,1,1) 175216
++(1,1,1,1001) 393756
++(1,32,149,149) 419355
++(1,32,147,147) 406234
++(1,64,147,147) 815270
++(1,64,73,73) 334453
++(1,80,73,73) 361670
++(1,192,71,71) 590238
++(1,192,35,35) 298521
++(1,64,35,35) 233599
++(1,48,35,35) 213764
++(1,32,35,35) 237611
++(1,96,35,35) 229407
++(1,256,35,35) 315025
++(1,288,35,35) 353843
++(1,288,17,17) 233004
++(1,384,17,17) 249158
++(1,96,17,17) 240940
++(1,768,17,17) 304225
++(1,128,17,17) 235806
++(1,192,17,17) 415270
++(1,160,17,17) 257318
++(1,768,8,8) 415110
++(1,320,8,8) 271323
++(1,192,8,8) 196748
++(1,1280,8,8) 258350
++(1,448,8,8) 283461
++(1,384,8,8) 413128
++(1,2048,8,8) 284567
++(1,2048,1,1) 220477
++(1,1001,1,1) 193111
++(1,1,1,1001) 318699
++(1,32,149,149) 426639
++(1,32,147,147) 407206
++(1,64,147,147) 733416
++(1,64,73,73) 327798
++(1,80,73,73) 361210
++(1,192,71,71) 604192
++(1,192,35,35) 293846
++(1,64,35,35) 416537
++(1,48,35,35) 270804
++(1,32,35,35) 241771
++(1,96,35,35) 239940
++(1,256,35,35) 318254
++(1,288,35,35) 336934
++(1,288,17,17) 257712
++(1,384,17,17) 240327
++(1,96,17,17) 199244
++(1,768,17,17) 290795
++(1,128,17,17) 267203
++(1,192,17,17) 226969
++(1,160,17,17) 419750
++(1,768,8,8) 410830
++(1,320,8,8) 231991
++(1,192,8,8) 212696
++(1,1280,8,8) 233422
++(1,448,8,8) 402621
++(1,384,8,8) 395147
++(1,2048,8,8) 267733
++(1,2048,1,1) 264989
++(1,1001,1,1) 169450
++(1,1,1,1001) 248276
++(1,32,149,149) 423249
++(1,32,147,147) 403834
++(1,64,147,147) 732827
++(1,64,73,73) 323003
++(1,80,73,73) 365710
++(1,192,71,71) 605840
++(1,192,35,35) 280650
++(1,64,35,35) 221074
++(1,48,35,35) 228786
++(1,32,35,35) 236167
++(1,96,35,35) 247241
++(1,256,35,35) 294874
++(1,288,35,35) 309014
++(1,288,17,17) 228663
++(1,384,17,17) 241784
++(1,96,17,17) 223979
++(1,768,17,17) 299863
++(1,128,17,17) 233874
++(1,192,17,17) 415010
++(1,160,17,17) 432394
++(1,768,8,8) 231829
++(1,320,8,8) 251192
++(1,192,8,8) 221593
++(1,1280,8,8) 252483
++(1,448,8,8) 240482
++(1,384,8,8) 408466
++(1,2048,8,8) 286274
++(1,2048,1,1) 211443
++(1,1001,1,1) 170710
++(1,1,1,1001) 401209
++(1,32,149,149) 410866
++(1,32,147,147) 417375
++(1,64,147,147) 819916
++(1,64,73,73) 296583
++(1,80,73,73) 337255
++(1,192,71,71) 597577
++(1,192,35,35) 298697
++(1,64,35,35) 229898
++(1,48,35,35) 274560
++(1,32,35,35) 236310
++(1,96,35,35) 249601
++(1,256,35,35) 320703
++(1,288,35,35) 345916
++(1,288,17,17) 238213
++(1,384,17,17) 305368
++(1,96,17,17) 230616
++(1,768,17,17) 309452
++(1,128,17,17) 290076
++(1,192,17,17) 215884
++(1,160,17,17) 407694
++(1,768,8,8) 419197
++(1,320,8,8) 231612
++(1,192,8,8) 238167
++(1,1280,8,8) 431069
++(1,448,8,8) 397713
++(1,384,8,8) 393858
++(1,2048,8,8) 274331
++(1,2048,1,1) 217951
++(1,1001,1,1) 167115
++(1,1,1,1001) 296290
++(1,32,149,149) 426590
++(1,32,147,147) 411362
++(1,64,147,147) 788310
++(1,64,73,73) 298827
++(1,80,73,73) 335702
++(1,192,71,71) 679715
++(1,192,35,35) 287482
++(1,64,35,35) 422353
++(1,48,35,35) 409882
++(1,32,35,35) 228938
++(1,96,35,35) 242886
++(1,256,35,35) 315896
++(1,288,35,35) 339755
++(1,288,17,17) 230426
++(1,384,17,17) 243716
++(1,96,17,17) 238752
++(1,768,17,17) 309426
++(1,128,17,17) 243548
++(1,192,17,17) 417062
++(1,160,17,17) 288879
++(1,768,8,8) 427305
++(1,320,8,8) 218434
++(1,192,8,8) 214562
++(1,1280,8,8) 242805
++(1,448,8,8) 413325
++(1,384,8,8) 266614
++(1,2048,8,8) 286491
++(1,2048,1,1) 187055
++(1,1001,1,1) 158494
++(1,1,1,1001) 234914
++(1,32,149,149) 419371
++(1,32,147,147) 392778
++(1,64,147,147) 794590
++(1,64,73,73) 330751
++(1,80,73,73) 345917
++(1,192,71,71) 671838
++(1,192,35,35) 301819
++(1,64,35,35) 224438
++(1,48,35,35) 217972
++(1,32,35,35) 232155
++(1,96,35,35) 238211
++(1,256,35,35) 324537
++(1,288,35,35) 325663
++(1,288,17,17) 232578
++(1,384,17,17) 245488
++(1,96,17,17) 224713
++(1,768,17,17) 299466
++(1,128,17,17) 352359
++(1,192,17,17) 413494
++(1,160,17,17) 235383
++(1,768,8,8) 325519
++(1,320,8,8) 239699
++(1,192,8,8) 221193
++(1,1280,8,8) 249803
++(1,448,8,8) 404433
++(1,384,8,8) 328632
++(1,2048,8,8) 283162
++(1,2048,1,1) 220331
++(1,1001,1,1) 165593
++(1,1,1,1001) 237310
++(1,32,149,149) 410465
++(1,32,147,147) 413853
++(1,64,147,147) 745436
++(1,64,73,73) 314666
++(1,80,73,73) 354496
++(1,192,71,71) 585745
++(1,192,35,35) 281661
++(1,64,35,35) 264478
++(1,48,35,35) 414909
++(1,32,35,35) 196086
++(1,96,35,35) 249963
++(1,256,35,35) 317734
++(1,288,35,35) 313524
++(1,288,17,17) 228179
++(1,384,17,17) 250830
++(1,96,17,17) 231793
++(1,768,17,17) 303585
++(1,128,17,17) 257837
++(1,192,17,17) 221834
++(1,160,17,17) 407275
++(1,768,8,8) 222154
++(1,320,8,8) 240303
++(1,192,8,8) 204602
++(1,1280,8,8) 235457
++(1,448,8,8) 383747
++(1,384,8,8) 287105
++(1,2048,8,8) 280317
++(1,2048,1,1) 228142
++(1,1001,1,1) 166810
++(1,1,1,1001) 408038
++(1,32,149,149) 423527
++(1,32,147,147) 412564
++(1,64,147,147) 716913
++(1,64,73,73) 304459
++(1,80,73,73) 354487
++(1,192,71,71) 602091
++(1,192,35,35) 288479
++(1,64,35,35) 265986
++(1,48,35,35) 417977
++(1,32,35,35) 234998
++(1,96,35,35) 248259
++(1,256,35,35) 312768
++(1,288,35,35) 346935
++(1,288,17,17) 238103
++(1,384,17,17) 246111
++(1,96,17,17) 229869
++(1,768,17,17) 301023
++(1,128,17,17) 276912
++(1,192,17,17) 425118
++(1,160,17,17) 418984
++(1,768,8,8) 421445
++(1,320,8,8) 232331
++(1,192,8,8) 236145
++(1,1280,8,8) 431703
++(1,448,8,8) 359150
++(1,384,8,8) 406633
++(1,2048,8,8) 277105
++(1,2048,1,1) 221432
++(1,1001,1,1) 168637
++(1,1,1,1001) 389148
++(1,32,149,149) 427222
++(1,32,147,147) 414996
++(1,64,147,147) 731325
++(1,64,73,73) 325975
++(1,80,73,73) 367794
++(1,192,71,71) 596344
++(1,192,35,35) 264317
++(1,64,35,35) 239927
++(1,48,35,35) 419370
++(1,32,35,35) 230177
++(1,96,35,35) 234711
++(1,256,35,35) 294727
++(1,288,35,35) 343205
++(1,288,17,17) 231814
++(1,384,17,17) 244808
++(1,96,17,17) 235180
++(1,768,17,17) 302421
++(1,128,17,17) 237910
++(1,192,17,17) 421776
++(1,160,17,17) 300942
++(1,768,8,8) 413175
++(1,320,8,8) 227761
++(1,192,8,8) 218257
++(1,1280,8,8) 303135
++(1,448,8,8) 406156
++(1,384,8,8) 412176
++(1,2048,8,8) 286203
++(1,2048,1,1) 226222
++(1,1001,1,1) 191812
++(1,1,1,1001) 403507
+diff --git a/benchmark/redmi_nc4hw4_buffer_to_image_v1.csv b/benchmark/redmi_nc4hw4_buffer_to_image_v1.csv
+new file mode 100644
+index 0000000..3c3fb3f
+--- /dev/null
++++ b/benchmark/redmi_nc4hw4_buffer_to_image_v1.csv
+@@ -0,0 +1,300 @@
++(1,32,149,149) 324150
++(1,32,147,147) 308382
++(1,64,147,147) 470139
++(1,64,73,73) 249329
++(1,80,73,73) 249256
++(1,192,71,71) 335755
++(1,192,35,35) 208241
++(1,64,35,35) 220390
++(1,48,35,35) 179478
++(1,32,35,35) 174291
++(1,96,35,35) 275660
++(1,256,35,35) 201421
++(1,288,35,35) 210428
++(1,288,17,17) 172660
++(1,384,17,17) 150256
++(1,96,17,17) 169760
++(1,768,17,17) 171821
++(1,128,17,17) 175120
++(1,192,17,17) 140615
++(1,160,17,17) 172346
++(1,768,8,8) 137866
++(1,320,8,8) 155594
++(1,192,8,8) 170195
++(1,1280,8,8) 141839
++(1,448,8,8) 183073
++(1,384,8,8) 178703
++(1,2048,8,8) 162976
++(1,2048,1,1) 185112
++(1,1001,1,1) 257613
++(1,1,1,1001) 248182
++(1,32,149,149) 335591
++(1,32,147,147) 330347
++(1,64,147,147) 469206
++(1,64,73,73) 221197
++(1,80,73,73) 232911
++(1,192,71,71) 330406
++(1,192,35,35) 184205
++(1,64,35,35) 155923
++(1,48,35,35) 144852
++(1,32,35,35) 186007
++(1,96,35,35) 145766
++(1,256,35,35) 199351
++(1,288,35,35) 209253
++(1,288,17,17) 170848
++(1,384,17,17) 143423
++(1,96,17,17) 161980
++(1,768,17,17) 195963
++(1,128,17,17) 158984
++(1,192,17,17) 136938
++(1,160,17,17) 169502
++(1,768,8,8) 136422
++(1,320,8,8) 159015
++(1,192,8,8) 170105
++(1,1280,8,8) 172203
++(1,448,8,8) 174056
++(1,384,8,8) 171221
++(1,2048,8,8) 168720
++(1,2048,1,1) 203221
++(1,1001,1,1) 198619
++(1,1,1,1001) 408995
++(1,32,149,149) 319073
++(1,32,147,147) 321032
++(1,64,147,147) 493246
++(1,64,73,73) 263670
++(1,80,73,73) 244765
++(1,192,71,71) 332409
++(1,192,35,35) 183797
++(1,64,35,35) 152538
++(1,48,35,35) 155876
++(1,32,35,35) 172782
++(1,96,35,35) 146284
++(1,256,35,35) 195565
++(1,288,35,35) 207950
++(1,288,17,17) 177306
++(1,384,17,17) 151741
++(1,96,17,17) 175656
++(1,768,17,17) 169912
++(1,128,17,17) 176715
++(1,192,17,17) 141818
++(1,160,17,17) 171585
++(1,768,8,8) 139937
++(1,320,8,8) 179771
++(1,192,8,8) 152842
++(1,1280,8,8) 139816
++(1,448,8,8) 169171
++(1,384,8,8) 241771
++(1,2048,8,8) 194391
++(1,2048,1,1) 196251
++(1,1001,1,1) 160725
++(1,1,1,1001) 417322
++(1,32,149,149) 342402
++(1,32,147,147) 329938
++(1,64,147,147) 475095
++(1,64,73,73) 234029
++(1,80,73,73) 248313
++(1,192,71,71) 334479
++(1,192,35,35) 182251
++(1,64,35,35) 158111
++(1,48,35,35) 143100
++(1,32,35,35) 193664
++(1,96,35,35) 198151
++(1,256,35,35) 197659
++(1,288,35,35) 211376
++(1,288,17,17) 149574
++(1,384,17,17) 146393
++(1,96,17,17) 168470
++(1,768,17,17) 168437
++(1,128,17,17) 178450
++(1,192,17,17) 141826
++(1,160,17,17) 173245
++(1,768,8,8) 146971
++(1,320,8,8) 177736
++(1,192,8,8) 180012
++(1,1280,8,8) 146371
++(1,448,8,8) 170743
++(1,384,8,8) 169355
++(1,2048,8,8) 165341
++(1,2048,1,1) 205251
++(1,1001,1,1) 169394
++(1,1,1,1001) 254862
++(1,32,149,149) 325493
++(1,32,147,147) 311099
++(1,64,147,147) 501595
++(1,64,73,73) 259433
++(1,80,73,73) 246604
++(1,192,71,71) 333253
++(1,192,35,35) 186372
++(1,64,35,35) 155965
++(1,48,35,35) 142917
++(1,32,35,35) 174675
++(1,96,35,35) 147564
++(1,256,35,35) 198485
++(1,288,35,35) 239304
++(1,288,17,17) 173397
++(1,384,17,17) 157431
++(1,96,17,17) 184955
++(1,768,17,17) 168743
++(1,128,17,17) 201422
++(1,192,17,17) 195236
++(1,160,17,17) 177910
++(1,768,8,8) 147927
++(1,320,8,8) 177433
++(1,192,8,8) 177636
++(1,1280,8,8) 153878
++(1,448,8,8) 168350
++(1,384,8,8) 207818
++(1,2048,8,8) 167328
++(1,2048,1,1) 209542
++(1,1001,1,1) 172910
++(1,1,1,1001) 405572
++(1,32,149,149) 345148
++(1,32,147,147) 333785
++(1,64,147,147) 488779
++(1,64,73,73) 240180
++(1,80,73,73) 246741
++(1,192,71,71) 334945
++(1,192,35,35) 184926
++(1,64,35,35) 154370
++(1,48,35,35) 139117
++(1,32,35,35) 213497
++(1,96,35,35) 143892
++(1,256,35,35) 197772
++(1,288,35,35) 208697
++(1,288,17,17) 145394
++(1,384,17,17) 145290
++(1,96,17,17) 162474
++(1,768,17,17) 171463
++(1,128,17,17) 173945
++(1,192,17,17) 139523
++(1,160,17,17) 173350
++(1,768,8,8) 168875
++(1,320,8,8) 184919
++(1,192,8,8) 183660
++(1,1280,8,8) 140690
++(1,448,8,8) 174035
++(1,384,8,8) 172636
++(1,2048,8,8) 164953
++(1,2048,1,1) 179995
++(1,1001,1,1) 157050
++(1,1,1,1001) 401709
++(1,32,149,149) 326067
++(1,32,147,147) 329810
++(1,64,147,147) 473093
++(1,64,73,73) 231398
++(1,80,73,73) 239708
++(1,192,71,71) 331255
++(1,192,35,35) 213238
++(1,64,35,35) 178643
++(1,48,35,35) 173193
++(1,32,35,35) 182301
++(1,96,35,35) 148121
++(1,256,35,35) 190094
++(1,288,35,35) 196964
++(1,288,17,17) 138332
++(1,384,17,17) 141571
++(1,96,17,17) 162872
++(1,768,17,17) 168307
++(1,128,17,17) 177148
++(1,192,17,17) 212274
++(1,160,17,17) 180830
++(1,768,8,8) 162260
++(1,320,8,8) 169254
++(1,192,8,8) 174494
++(1,1280,8,8) 140678
++(1,448,8,8) 170340
++(1,384,8,8) 170625
++(1,2048,8,8) 168737
++(1,2048,1,1) 202409
++(1,1001,1,1) 167230
++(1,1,1,1001) 400369
++(1,32,149,149) 324593
++(1,32,147,147) 328016
++(1,64,147,147) 478841
++(1,64,73,73) 235003
++(1,80,73,73) 246590
++(1,192,71,71) 331694
++(1,192,35,35) 197520
++(1,64,35,35) 190281
++(1,48,35,35) 177680
++(1,32,35,35) 173012
++(1,96,35,35) 148022
++(1,256,35,35) 224802
++(1,288,35,35) 211636
++(1,288,17,17) 160678
++(1,384,17,17) 152276
++(1,96,17,17) 183059
++(1,768,17,17) 164985
++(1,128,17,17) 204886
++(1,192,17,17) 190960
++(1,160,17,17) 198393
++(1,768,8,8) 180139
++(1,320,8,8) 173157
++(1,192,8,8) 176974
++(1,1280,8,8) 159284
++(1,448,8,8) 164616
++(1,384,8,8) 168404
++(1,2048,8,8) 168672
++(1,2048,1,1) 208007
++(1,1001,1,1) 170056
++(1,1,1,1001) 401011
++(1,32,149,149) 334220
++(1,32,147,147) 328281
++(1,64,147,147) 485089
++(1,64,73,73) 236240
++(1,80,73,73) 239866
++(1,192,71,71) 347677
++(1,192,35,35) 177792
++(1,64,35,35) 152135
++(1,48,35,35) 140938
++(1,32,35,35) 183700
++(1,96,35,35) 173304
++(1,256,35,35) 198176
++(1,288,35,35) 204689
++(1,288,17,17) 160268
++(1,384,17,17) 147263
++(1,96,17,17) 178018
++(1,768,17,17) 164971
++(1,128,17,17) 187256
++(1,192,17,17) 188479
++(1,160,17,17) 170127
++(1,768,8,8) 138766
++(1,320,8,8) 157148
++(1,192,8,8) 126032
++(1,1280,8,8) 139830
++(1,448,8,8) 182253
++(1,384,8,8) 175093
++(1,2048,8,8) 279678
++(1,2048,1,1) 199224
++(1,1001,1,1) 180485
++(1,1,1,1001) 401731
++(1,32,149,149) 324494
++(1,32,147,147) 319658
++(1,64,147,147) 486333
++(1,64,73,73) 234365
++(1,80,73,73) 246060
++(1,192,71,71) 333226
++(1,192,35,35) 183141
++(1,64,35,35) 153156
++(1,48,35,35) 233552
++(1,32,35,35) 173214
++(1,96,35,35) 194309
++(1,256,35,35) 195413
++(1,288,35,35) 209192
++(1,288,17,17) 139728
++(1,384,17,17) 142869
++(1,96,17,17) 164986
++(1,768,17,17) 176133
++(1,128,17,17) 173006
++(1,192,17,17) 142159
++(1,160,17,17) 174147
++(1,768,8,8) 136391
++(1,320,8,8) 155821
++(1,192,8,8) 172806
++(1,1280,8,8) 143530
++(1,448,8,8) 169583
++(1,384,8,8) 170666
++(1,2048,8,8) 196500
++(1,2048,1,1) 189256
++(1,1001,1,1) 168225
++(1,1,1,1001) 398126
+diff --git a/benchmark/run_bench_buffer_to_image.sh b/benchmark/run_bench_buffer_to_image.sh
+new file mode 100644
+index 0000000..49fd349
+--- /dev/null
++++ b/benchmark/run_bench_buffer_to_image.sh
+@@ -0,0 +1,5 @@
++for i in $(seq 1 10)
++do
++cat inception-v3-output-tensor-shapes.txt | ./benchmarkTensorTransform >> oneplus_nc4hw4_buffer_to_image.txt;
++echo $i;
++done
+\ No newline at end of file
+diff --git a/benchmark/tmp.txt b/benchmark/tmp.txt
+new file mode 100644
+index 0000000..3aa59c5
+--- /dev/null
++++ b/benchmark/tmp.txt
+@@ -0,0 +1,168 @@
++MNN benchmark
++Forward type: **OpenCL** thread=2** precision=2** execution type=1
++--------> Benchmarking... loop = 7
++In Session::Session backup cpuBackend thread number is 2
++Open Profile file /data/local/tmp/profile.txt
++In Pipeline.cpp Pipeline::prepare deviceMapFilePath: ./mDevice_map_lanenet_serial_hybrid.txt
++lanenet_model/vgg_frontend/vgg16_decode_module/instance_seg_decode/decode_stage_3_fuse/deconv/deconv/conv2d_transpose need to wrap*****
++lanenet_model/vgg_frontend/vgg16_encode_module/conv3_3/relu___tr4lanenet_model/vgg_frontend/vgg16_decode_module/binary_seg_decode/decode_stage_3_fuse/fuse_feats need to wrap*****
++lanenet_model/vgg_frontend/vgg16_decode_module/binary_seg_decode/decode_stage_3_fuse/deconv_relu___tr4lanenet_model/vgg_frontend/vgg16_decode_module/binary_seg_decode/decode_stage_3_fuse/fuse_feats need to wrap*****
++lanenet_model/vgg_frontend/vgg16_encode_module/conv3_3/relu___tr4lanenet_model/vgg_frontend/vgg16_decode_module/instance_seg_decode/decode_stage_3_fuse/fuse_feats need to wrap*****
++lanenet_model/vgg_frontend/vgg16_encode_module/conv2_2/relu___tr4lanenet_model/vgg_frontend/vgg16_decode_module/binary_seg_decode/decode_stage_2_fuse/fuse_feats need to wrap*****
++lanenet_model/vgg_frontend/vgg16_encode_module/conv2_2/relu___tr4lanenet_model/vgg_frontend/vgg16_decode_module/instance_seg_decode/decode_stage_2_fuse/fuse_feats need to wrap*****
++lanenet_model/vgg_frontend/vgg16_encode_module/conv1_2/relu___tr4lanenet_model/vgg_frontend/vgg16_decode_module/binary_seg_decode/decode_stage_1_fuse/fuse_feats need to wrap*****
++lanenet_model/vgg_frontend/vgg16_encode_module/conv1_2/relu___tr4lanenet_model/vgg_frontend/vgg16_decode_module/instance_seg_decode/decode_stage_1_fuse/fuse_feats need to wrap*****
++lanenet_model/vgg_backend/instance_seg/pix_bn/FusedBatchNorm need to wrap*****
++lanenet_model/vgg_backend/binary_seg/Softmax need to wrap*****
++OpName Inputs Outputs ParentsIdx ChildIdx
++lanenet_model/vgg_frontend/vgg16_encode_module/conv1_1/conv/Conv2D 1,3,256,512; 1,64,256,512; none 1;
++lanenet_model/vgg_frontend/vgg16_encode_module/conv1_2/conv/Conv2D 1,64,256,512; 1,64,256,512; 0; 2;64;67;
++lanenet_model/vgg_frontend/vgg16_encode_module/pool1 1,64,256,512; 1,64,128,256; 1; 3;
++lanenet_model/vgg_frontend/vgg16_encode_module/conv2_1/conv/Conv2D 1,64,128,256; 1,128,128,256; 2; 4;
++lanenet_model/vgg_frontend/vgg16_encode_module/conv2_2/conv/Conv2D 1,128,128,256; 1,128,128,256; 3; 5;50;53;
++lanenet_model/vgg_frontend/vgg16_encode_module/pool2 1,128,128,256; 1,128,64,128; 4; 6;
++lanenet_model/vgg_frontend/vgg16_encode_module/conv3_1/conv/Conv2D 1,128,64,128; 1,256,64,128; 5; 7;
++lanenet_model/vgg_frontend/vgg16_encode_module/conv3_2/conv/Conv2D 1,256,64,128; 1,256,64,128; 6; 8;
++lanenet_model/vgg_frontend/vgg16_encode_module/conv3_3/conv/Conv2D 1,256,64,128; 1,256,64,128; 7; 9;36;39;
++lanenet_model/vgg_frontend/vgg16_encode_module/pool3 1,256,64,128; 1,256,32,64; 8; 10;
++lanenet_model/vgg_frontend/vgg16_encode_module/conv4_1/conv/Conv2D 1,256,32,64; 1,512,32,64; 9; 11;
++lanenet_model/vgg_frontend/vgg16_encode_module/conv4_2/conv/Conv2D 1,512,32,64; 1,512,32,64; 10; 12;
++lanenet_model/vgg_frontend/vgg16_encode_module/conv4_3/conv/Conv2D 1,512,32,64; 1,512,32,64; 11; 13;22;25;
++lanenet_model/vgg_frontend/vgg16_encode_module/pool4 1,512,32,64; 1,512,16,32; 12; 14;15;
++lanenet_model/vgg_frontend/vgg16_encode_module/conv5_1_binary/conv/Conv2D 1,512,16,32; 1,512,16,32; 13; 16;
++lanenet_model/vgg_frontend/vgg16_encode_module/conv5_1_instance/conv/Conv2D 1,512,16,32; 1,512,16,32; 13; 17;
++lanenet_model/vgg_frontend/vgg16_encode_module/conv5_2_binary/conv/Conv2D 1,512,16,32; 1,512,16,32; 14; 18;
++lanenet_model/vgg_frontend/vgg16_encode_module/conv5_2_instance/conv/Conv2D 1,512,16,32; 1,512,16,32; 15; 19;
++lanenet_model/vgg_frontend/vgg16_encode_module/conv5_3_binary/conv/Conv2D 1,512,16,32; 1,512,16,32; 16; 20;
++lanenet_model/vgg_frontend/vgg16_encode_module/conv5_3_instance/conv/Conv2D 1,512,16,32; 1,512,16,32; 17; 21;
++lanenet_model/vgg_frontend/vgg16_decode_module/binary_seg_decode/decode_stage_4_fuse/deconv/deconv/conv2d_transpose 1,512,16,32; 1,512,32,64; 18; 23;
++lanenet_model/vgg_frontend/vgg16_decode_module/instance_seg_decode/decode_stage_4_fuse/deconv/deconv/conv2d_transpose 1,512,16,32; 1,512,32,64; 19; 26;
++lanenet_model/vgg_frontend/vgg16_encode_module/conv4_3/relu___tr4lanenet_model/vgg_frontend/vgg16_decode_module/binary_seg_decode/decode_stage_4_fuse/fuse_feats 1,512,32,64; 1,512,32,64; 12; 24;
++lanenet_model/vgg_frontend/vgg16_decode_module/binary_seg_decode/decode_stage_4_fuse/deconv_relu___tr4lanenet_model/vgg_frontend/vgg16_decode_module/binary_seg_decode/decode_stage_4_fuse/fuse_feats 1,512,32,64; 1,512,32,64; 20; 24;
++lanenet_model/vgg_frontend/vgg16_decode_module/binary_seg_decode/decode_stage_4_fuse/fuse_feats 1,512,32,64;1,512,32,64; 1,512,32,64; 22;23; 28;
++lanenet_model/vgg_frontend/vgg16_encode_module/conv4_3/relu___tr4lanenet_model/vgg_frontend/vgg16_decode_module/instance_seg_decode/decode_stage_4_fuse/fuse_feats 1,512,32,64; 1,512,32,64; 12; 27;
++lanenet_model/vgg_frontend/vgg16_decode_module/instance_seg_decode/decode_stage_4_fuse/deconv_relu___tr4lanenet_model/vgg_frontend/vgg16_decode_module/instance_seg_decode/decode_stage_4_fuse/fuse_feats 1,512,32,64; 1,512,32,64; 21; 27;
++lanenet_model/vgg_frontend/vgg16_decode_module/instance_seg_decode/decode_stage_4_fuse/fuse_feats 1,512,32,64;1,512,32,64; 1,512,32,64; 25;26; 30;
++lanenet_model/vgg_frontend/vgg16_decode_module/binary_seg_decode/decode_stage_4_fuse/fuse_feats___tr4lanenet_model/vgg_frontend/vgg16_decode_module/binary_seg_decode/decode_stage_4_fuse/fuse_gn/FusedBatchNorm 1,512,32,64; 1,512,32,64; 24; 29;
++lanenet_model/vgg_frontend/vgg16_decode_module/binary_seg_decode/decode_stage_4_fuse/fuse_gn/FusedBatchNorm 1,512,32,64; 1,512,32,64; 28; 32;
++lanenet_model/vgg_frontend/vgg16_decode_module/instance_seg_decode/decode_stage_4_fuse/fuse_feats___tr4lanenet_model/vgg_frontend/vgg16_decode_module/instance_seg_decode/decode_stage_4_fuse/fuse_gn/FusedBatchNorm 1,512,32,64; 1,512,32,64; 27; 31;
++lanenet_model/vgg_frontend/vgg16_decode_module/instance_seg_decode/decode_stage_4_fuse/fuse_gn/FusedBatchNorm 1,512,32,64; 1,512,32,64; 30; 33;
++lanenet_model/vgg_frontend/vgg16_decode_module/binary_seg_decode/decode_stage_4_fuse/fuse_relu 1,512,32,64; 1,512,32,64; 29; 34;
++lanenet_model/vgg_frontend/vgg16_decode_module/instance_seg_decode/decode_stage_4_fuse/fuse_relu 1,512,32,64; 1,512,32,64; 31; 35;
++lanenet_model/vgg_frontend/vgg16_decode_module/binary_seg_decode/decode_stage_3_fuse/deconv/deconv/conv2d_transpose 1,512,32,64; 1,256,64,128; 32; 37;
++lanenet_model/vgg_frontend/vgg16_decode_module/instance_seg_decode/decode_stage_3_fuse/deconv/deconv/conv2d_transpose 1,512,32,64; 1,256,64,128; 33; 40;
++lanenet_model/vgg_frontend/vgg16_encode_module/conv3_3/relu___tr4lanenet_model/vgg_frontend/vgg16_decode_module/binary_seg_decode/decode_stage_3_fuse/fuse_feats 1,256,64,128; 1,256,64,128; 8; 38;
++lanenet_model/vgg_frontend/vgg16_decode_module/binary_seg_decode/decode_stage_3_fuse/deconv_relu___tr4lanenet_model/vgg_frontend/vgg16_decode_module/binary_seg_decode/decode_stage_3_fuse/fuse_feats 1,256,64,128; 1,256,64,128; 34; 38;
++lanenet_model/vgg_frontend/vgg16_decode_module/binary_seg_decode/decode_stage_3_fuse/fuse_feats 1,256,64,128;1,256,64,128; 1,256,64,128; 36;37; 42;
++lanenet_model/vgg_frontend/vgg16_encode_module/conv3_3/relu___tr4lanenet_model/vgg_frontend/vgg16_decode_module/instance_seg_decode/decode_stage_3_fuse/fuse_feats 1,256,64,128; 1,256,64,128; 8; 41;
++lanenet_model/vgg_frontend/vgg16_decode_module/instance_seg_decode/decode_stage_3_fuse/deconv_relu___tr4lanenet_model/vgg_frontend/vgg16_decode_module/instance_seg_decode/decode_stage_3_fuse/fuse_feats 1,256,64,128; 1,256,64,128; 35; 41;
++lanenet_model/vgg_frontend/vgg16_decode_module/instance_seg_decode/decode_stage_3_fuse/fuse_feats 1,256,64,128;1,256,64,128; 1,256,64,128; 39;40; 44;
++lanenet_model/vgg_frontend/vgg16_decode_module/binary_seg_decode/decode_stage_3_fuse/fuse_feats___tr4lanenet_model/vgg_frontend/vgg16_decode_module/binary_seg_decode/decode_stage_3_fuse/fuse_gn/FusedBatchNorm 1,256,64,128; 1,256,64,128; 38; 43;
++lanenet_model/vgg_frontend/vgg16_decode_module/binary_seg_decode/decode_stage_3_fuse/fuse_gn/FusedBatchNorm 1,256,64,128; 1,256,64,128; 42; 46;
++lanenet_model/vgg_frontend/vgg16_decode_module/instance_seg_decode/decode_stage_3_fuse/fuse_feats___tr4lanenet_model/vgg_frontend/vgg16_decode_module/instance_seg_decode/decode_stage_3_fuse/fuse_gn/FusedBatchNorm 1,256,64,128; 1,256,64,128; 41; 45;
++lanenet_model/vgg_frontend/vgg16_decode_module/instance_seg_decode/decode_stage_3_fuse/fuse_gn/FusedBatchNorm 1,256,64,128; 1,256,64,128; 44; 47;
++lanenet_model/vgg_frontend/vgg16_decode_module/binary_seg_decode/decode_stage_3_fuse/fuse_relu 1,256,64,128; 1,256,64,128; 43; 48;
++lanenet_model/vgg_frontend/vgg16_decode_module/instance_seg_decode/decode_stage_3_fuse/fuse_relu 1,256,64,128; 1,256,64,128; 45; 49;
++lanenet_model/vgg_frontend/vgg16_decode_module/binary_seg_decode/decode_stage_2_fuse/deconv/deconv/conv2d_transpose 1,256,64,128; 1,128,128,256; 46; 51;
++lanenet_model/vgg_frontend/vgg16_decode_module/instance_seg_decode/decode_stage_2_fuse/deconv/deconv/conv2d_transpose 1,256,64,128; 1,128,128,256; 47; 54;
++lanenet_model/vgg_frontend/vgg16_encode_module/conv2_2/relu___tr4lanenet_model/vgg_frontend/vgg16_decode_module/binary_seg_decode/decode_stage_2_fuse/fuse_feats 1,128,128,256; 1,128,128,256; 4; 52;
++lanenet_model/vgg_frontend/vgg16_decode_module/binary_seg_decode/decode_stage_2_fuse/deconv_relu___tr4lanenet_model/vgg_frontend/vgg16_decode_module/binary_seg_decode/decode_stage_2_fuse/fuse_feats 1,128,128,256; 1,128,128,256; 48; 52;
++lanenet_model/vgg_frontend/vgg16_decode_module/binary_seg_decode/decode_stage_2_fuse/fuse_feats 1,128,128,256;1,128,128,256; 1,128,128,256; 50;51; 56;
++lanenet_model/vgg_frontend/vgg16_encode_module/conv2_2/relu___tr4lanenet_model/vgg_frontend/vgg16_decode_module/instance_seg_decode/decode_stage_2_fuse/fuse_feats 1,128,128,256; 1,128,128,256; 4; 55;
++lanenet_model/vgg_frontend/vgg16_decode_module/instance_seg_decode/decode_stage_2_fuse/deconv_relu___tr4lanenet_model/vgg_frontend/vgg16_decode_module/instance_seg_decode/decode_stage_2_fuse/fuse_feats 1,128,128,256; 1,128,128,256; 49; 55;
++lanenet_model/vgg_frontend/vgg16_decode_module/instance_seg_decode/decode_stage_2_fuse/fuse_feats 1,128,128,256;1,128,128,256; 1,128,128,256; 53;54; 58;
++lanenet_model/vgg_frontend/vgg16_decode_module/binary_seg_decode/decode_stage_2_fuse/fuse_feats___tr4lanenet_model/vgg_frontend/vgg16_decode_module/binary_seg_decode/decode_stage_2_fuse/fuse_gn/FusedBatchNorm 1,128,128,256; 1,128,128,256; 52; 57;
++lanenet_model/vgg_frontend/vgg16_decode_module/binary_seg_decode/decode_stage_2_fuse/fuse_gn/FusedBatchNorm 1,128,128,256; 1,128,128,256; 56; 60;
++lanenet_model/vgg_frontend/vgg16_decode_module/instance_seg_decode/decode_stage_2_fuse/fuse_feats___tr4lanenet_model/vgg_frontend/vgg16_decode_module/instance_seg_decode/decode_stage_2_fuse/fuse_gn/FusedBatchNorm 1,128,128,256; 1,128,128,256; 55; 59;
++lanenet_model/vgg_frontend/vgg16_decode_module/instance_seg_decode/decode_stage_2_fuse/fuse_gn/FusedBatchNorm 1,128,128,256; 1,128,128,256; 58; 61;
++lanenet_model/vgg_frontend/vgg16_decode_module/binary_seg_decode/decode_stage_2_fuse/fuse_relu 1,128,128,256; 1,128,128,256; 57; 62;
++lanenet_model/vgg_frontend/vgg16_decode_module/instance_seg_decode/decode_stage_2_fuse/fuse_relu 1,128,128,256; 1,128,128,256; 59; 63;
++lanenet_model/vgg_frontend/vgg16_decode_module/binary_seg_decode/decode_stage_1_fuse/deconv/deconv/conv2d_transpose 1,128,128,256; 1,64,256,512; 60; 65;
++lanenet_model/vgg_frontend/vgg16_decode_module/instance_seg_decode/decode_stage_1_fuse/deconv/deconv/conv2d_transpose 1,128,128,256; 1,64,256,512; 61; 68;
++lanenet_model/vgg_frontend/vgg16_encode_module/conv1_2/relu___tr4lanenet_model/vgg_frontend/vgg16_decode_module/binary_seg_decode/decode_stage_1_fuse/fuse_feats 1,64,256,512; 1,64,256,512; 1; 66;
++lanenet_model/vgg_frontend/vgg16_decode_module/binary_seg_decode/decode_stage_1_fuse/deconv_relu___tr4lanenet_model/vgg_frontend/vgg16_decode_module/binary_seg_decode/decode_stage_1_fuse/fuse_feats 1,64,256,512; 1,64,256,512; 62; 66;
++lanenet_model/vgg_frontend/vgg16_decode_module/binary_seg_decode/decode_stage_1_fuse/fuse_feats 1,64,256,512;1,64,256,512; 1,64,256,512; 64;65; 70;
++lanenet_model/vgg_frontend/vgg16_encode_module/conv1_2/relu___tr4lanenet_model/vgg_frontend/vgg16_decode_module/instance_seg_decode/decode_stage_1_fuse/fuse_feats 1,64,256,512; 1,64,256,512; 1; 69;
++lanenet_model/vgg_frontend/vgg16_decode_module/instance_seg_decode/decode_stage_1_fuse/deconv_relu___tr4lanenet_model/vgg_frontend/vgg16_decode_module/instance_seg_decode/decode_stage_1_fuse/fuse_feats 1,64,256,512; 1,64,256,512; 63; 69;
++lanenet_model/vgg_frontend/vgg16_decode_module/instance_seg_decode/decode_stage_1_fuse/fuse_feats 1,64,256,512;1,64,256,512; 1,64,256,512; 67;68; 72;
++lanenet_model/vgg_frontend/vgg16_decode_module/binary_seg_decode/decode_stage_1_fuse/fuse_feats___tr4lanenet_model/vgg_frontend/vgg16_decode_module/binary_seg_decode/decode_stage_1_fuse/fuse_gn/FusedBatchNorm 1,64,256,512; 1,64,256,512; 66; 71;
++lanenet_model/vgg_frontend/vgg16_decode_module/binary_seg_decode/decode_stage_1_fuse/fuse_gn/FusedBatchNorm 1,64,256,512; 1,64,256,512; 70; 74;
++lanenet_model/vgg_frontend/vgg16_decode_module/instance_seg_decode/decode_stage_1_fuse/fuse_feats___tr4lanenet_model/vgg_backend/instance_seg/pix_bn/FusedBatchNorm 1,64,256,512; 1,64,256,512; 69; 73;
++lanenet_model/vgg_backend/instance_seg/pix_bn/FusedBatchNorm 1,64,256,512; 1,64,256,512; 72; 75;
++lanenet_model/vgg_frontend/vgg16_decode_module/binary_seg_decode/decode_stage_1_fuse/fuse_relu 1,64,256,512; 1,64,256,512; 71; 76;
++lanenet_model/vgg_backend/instance_seg/pix_relu 1,64,256,512; 1,64,256,512; 73; 77;
++lanenet_model/vgg_frontend/vgg16_decode_module/binary_seg_decode/binary_final_logits/Conv2D 1,64,256,512; 1,2,256,512; 74; 78;
++lanenet_model/vgg_backend/instance_seg/pix_embedding_conv/Conv2D 1,64,256,512; 1,4,256,512; 75; 80;
++lanenet_model/vgg_frontend/vgg16_decode_module/binary_seg_decode/binary_final_logits/Conv2D___tr4lanenet_model/vgg_backend/binary_seg/Softmax 1,2,256,512; 1,2,256,512; 76; 79;
++lanenet_model/vgg_backend/binary_seg/Softmax 1,2,256,512; 1,2,256,512; 78; 82;
++lanenet_model/vgg_backend/instance_seg/pix_embedding_conv/Conv2D___tr4lanenet/final_pixel_embedding_output 1,4,256,512; 1,4,256,512; 77; 81;
++lanenet/final_pixel_embedding_output 1,4,256,512; 256,0,512,4; 80; none
++lanenet_model/vgg_backend/binary_seg/ArgMax 1,2,256,512; 256,0,512,2; 79; 83;
++lanenet/Cast 256,0,512,2; 256,0,512,2; 82; 84;
++lanenet/final_binary_output 256,0,512,2; 512,0,2,0; 83; none
++Start run pipeline in serial hybrid.
++In Pipline::execute mUnits size: 85
++CPU pid is 6ca8 ret 0
++End execute lanenet_model/vgg_frontend/vgg16_encode_module/conv1_1/conv/Conv2D on CPU 69533 11 69544
++End execute lanenet_model/vgg_frontend/vgg16_encode_module/conv1_2/conv/Conv2D on CPU 205767 69575 275342
++End execute lanenet_model/vgg_frontend/vgg16_encode_module/pool1 on CPU 9084 275370 284454
++End execute lanenet_model/vgg_frontend/vgg16_encode_module/conv2_1/conv/Conv2D on CPU 94360 284479 378839
++End execute lanenet_model/vgg_frontend/vgg16_encode_module/conv2_2/conv/Conv2D on CPU 170796 378866 549662
++End execute lanenet_model/vgg_frontend/vgg16_encode_module/pool2 on CPU 4397 549689 554086
++End execute lanenet_model/vgg_frontend/vgg16_encode_module/conv3_1/conv/Conv2D on CPU 89567 554110 643677
++End execute lanenet_model/vgg_frontend/vgg16_encode_module/conv3_2/conv/Conv2D on CPU 171406 643705 815111
++End execute lanenet_model/vgg_frontend/vgg16_encode_module/conv3_3/conv/Conv2D on CPU 160811 815138 975949
++End execute lanenet_model/vgg_frontend/vgg16_encode_module/pool3 on CPU 2856 975977 978833
++End execute lanenet_model/vgg_frontend/vgg16_encode_module/conv4_1/conv/Conv2D on CPU 87671 978862 1066533
++End execute lanenet_model/vgg_frontend/vgg16_encode_module/conv4_2/conv/Conv2D on CPU 184040 1066557 1250597
++End execute lanenet_model/vgg_frontend/vgg16_encode_module/conv4_3/conv/Conv2D on CPU 184213 1250622 1434835
++End execute lanenet_model/vgg_frontend/vgg16_encode_module/pool4 on CPU 1424 1434860 1436284
++End execute lanenet_model/vgg_frontend/vgg16_encode_module/conv5_1_binary/conv/Conv2D on CPU 61469 1436307 1497776
++End execute lanenet_model/vgg_frontend/vgg16_encode_module/conv5_1_instance/conv/Conv2D on CPU 57470 1497799 1555269
++End execute lanenet_model/vgg_frontend/vgg16_encode_module/conv5_2_binary/conv/Conv2D on CPU 62261 1555359 1617620
++End execute lanenet_model/vgg_frontend/vgg16_encode_module/conv5_2_instance/conv/Conv2D on CPU 57974 1617648 1675622
++End execute lanenet_model/vgg_frontend/vgg16_encode_module/conv5_3_binary/conv/Conv2D on CPU 66504 1675649 1742153
++End execute lanenet_model/vgg_frontend/vgg16_encode_module/conv5_3_instance/conv/Conv2D on CPU 56366 1742177 1798543
++End execute lanenet_model/vgg_frontend/vgg16_decode_module/binary_seg_decode/decode_stage_4_fuse/deconv/deconv/conv2d_transpose on CPU 288518 1798569 2087087
++End execute lanenet_model/vgg_frontend/vgg16_decode_module/instance_seg_decode/decode_stage_4_fuse/deconv/deconv/conv2d_transpose on CPU 291187 2087118 2378305
++End execute lanenet_model/vgg_frontend/vgg16_encode_module/conv4_3/relu___tr4lanenet_model/vgg_frontend/vgg16_decode_module/binary_seg_decode/decode_stage_4_fuse/fuse_feats on CPU 4964 2378331 2383295
++End execute lanenet_model/vgg_frontend/vgg16_decode_module/binary_seg_decode/decode_stage_4_fuse/deconv_relu___tr4lanenet_model/vgg_frontend/vgg16_decode_module/binary_seg_decode/decode_stage_4_fuse/fuse_feats on CPU 4979 2383319 2388298
++End execute lanenet_model/vgg_frontend/vgg16_decode_module/binary_seg_decode/decode_stage_4_fuse/fuse_feats on CPU 795 2388325 2389120
++End execute lanenet_model/vgg_frontend/vgg16_encode_module/conv4_3/relu___tr4lanenet_model/vgg_frontend/vgg16_decode_module/instance_seg_decode/decode_stage_4_fuse/fuse_feats on CPU 4912 2389144 2394056
++End execute lanenet_model/vgg_frontend/vgg16_decode_module/instance_seg_decode/decode_stage_4_fuse/deconv_relu___tr4lanenet_model/vgg_frontend/vgg16_decode_module/instance_seg_decode/decode_stage_4_fuse/fuse_feats on CPU 4897 2394070 2398967
++End execute lanenet_model/vgg_frontend/vgg16_decode_module/instance_seg_decode/decode_stage_4_fuse/fuse_feats on CPU 760 2398992 2399752
++End execute lanenet_model/vgg_frontend/vgg16_decode_module/binary_seg_decode/decode_stage_4_fuse/fuse_feats___tr4lanenet_model/vgg_frontend/vgg16_decode_module/binary_seg_decode/decode_stage_4_fuse/fuse_gn/FusedBatchNorm on CPU 4861 2399776 2404637
++End execute lanenet_model/vgg_frontend/vgg16_decode_module/binary_seg_decode/decode_stage_4_fuse/fuse_gn/FusedBatchNorm on CPU 689 2404652 2405341
++End execute lanenet_model/vgg_frontend/vgg16_decode_module/instance_seg_decode/decode_stage_4_fuse/fuse_feats___tr4lanenet_model/vgg_frontend/vgg16_decode_module/instance_seg_decode/decode_stage_4_fuse/fuse_gn/FusedBatchNorm on CPU 4911 2405357 2410268
++End execute lanenet_model/vgg_frontend/vgg16_decode_module/instance_seg_decode/decode_stage_4_fuse/fuse_gn/FusedBatchNorm on CPU 690 2410292 2410982
++End execute lanenet_model/vgg_frontend/vgg16_decode_module/binary_seg_decode/decode_stage_4_fuse/fuse_relu on CPU 703 2410997 2411700
++End execute lanenet_model/vgg_frontend/vgg16_decode_module/instance_seg_decode/decode_stage_4_fuse/fuse_relu on CPU 705 2411722 2412427
++End execute lanenet_model/vgg_frontend/vgg16_decode_module/binary_seg_decode/decode_stage_3_fuse/deconv/deconv/conv2d_transpose on CPU 482791 2412442 2895233
++End execute lanenet_model/vgg_frontend/vgg16_decode_module/instance_seg_decode/decode_stage_3_fuse/deconv/deconv/conv2d_transpose on GPU 1325657 2895258 4220915
++End execute lanenet_model/vgg_frontend/vgg16_encode_module/conv3_3/relu___tr4lanenet_model/vgg_frontend/vgg16_decode_module/binary_seg_decode/decode_stage_3_fuse/fuse_feats on GPU 31829 4220992 4252821
++End execute lanenet_model/vgg_frontend/vgg16_decode_module/binary_seg_decode/decode_stage_3_fuse/deconv_relu___tr4lanenet_model/vgg_frontend/vgg16_decode_module/binary_seg_decode/decode_stage_3_fuse/fuse_feats on GPU 4743 4252877 4257620
++End execute lanenet_model/vgg_frontend/vgg16_decode_module/binary_seg_decode/decode_stage_3_fuse/fuse_feats on GPU 1826 4257668 4259494
++End execute lanenet_model/vgg_frontend/vgg16_encode_module/conv3_3/relu___tr4lanenet_model/vgg_frontend/vgg16_decode_module/instance_seg_decode/decode_stage_3_fuse/fuse_feats on GPU 2641 4259516 4262157
++End execute lanenet_model/vgg_frontend/vgg16_decode_module/instance_seg_decode/decode_stage_3_fuse/deconv_relu___tr4lanenet_model/vgg_frontend/vgg16_decode_module/instance_seg_decode/decode_stage_3_fuse/fuse_feats on GPU 1145 4262282 4263427
++End execute lanenet_model/vgg_frontend/vgg16_decode_module/instance_seg_decode/decode_stage_3_fuse/fuse_feats on GPU 1889 4263450 4265339
++End execute lanenet_model/vgg_frontend/vgg16_decode_module/binary_seg_decode/decode_stage_3_fuse/fuse_feats___tr4lanenet_model/vgg_frontend/vgg16_decode_module/binary_seg_decode/decode_stage_3_fuse/fuse_gn/FusedBatchNorm on GPU 1119 4265366 4266485
++End execute lanenet_model/vgg_frontend/vgg16_decode_module/binary_seg_decode/decode_stage_3_fuse/fuse_gn/FusedBatchNorm on GPU 1262 4266508 4267770
++End execute lanenet_model/vgg_frontend/vgg16_decode_module/instance_seg_decode/decode_stage_3_fuse/fuse_feats___tr4lanenet_model/vgg_frontend/vgg16_decode_module/instance_seg_decode/decode_stage_3_fuse/fuse_gn/FusedBatchNorm on GPU 1154 4267801 4268955
++End execute lanenet_model/vgg_frontend/vgg16_decode_module/instance_seg_decode/decode_stage_3_fuse/fuse_gn/FusedBatchNorm on GPU 1230 4268973 4270203
++End execute lanenet_model/vgg_frontend/vgg16_decode_module/binary_seg_decode/decode_stage_3_fuse/fuse_relu on GPU 1081 4270222 4271303
++End execute lanenet_model/vgg_frontend/vgg16_decode_module/instance_seg_decode/decode_stage_3_fuse/fuse_relu on GPU 1040 4271322 4272362
++End execute lanenet_model/vgg_frontend/vgg16_decode_module/binary_seg_decode/decode_stage_2_fuse/deconv/deconv/conv2d_transpose on GPU 272429 4272380 4544809
++End execute lanenet_model/vgg_frontend/vgg16_decode_module/instance_seg_decode/decode_stage_2_fuse/deconv/deconv/conv2d_transpose on GPU 273328 4544905 4818233
++End execute lanenet_model/vgg_frontend/vgg16_encode_module/conv2_2/relu___tr4lanenet_model/vgg_frontend/vgg16_decode_module/binary_seg_decode/decode_stage_2_fuse/fuse_feats on GPU 40617 4818315 4858932
++End execute lanenet_model/vgg_frontend/vgg16_decode_module/binary_seg_decode/decode_stage_2_fuse/deconv_relu___tr4lanenet_model/vgg_frontend/vgg16_decode_module/binary_seg_decode/decode_stage_2_fuse/fuse_feats on GPU 2545 4858966 4861511
++End execute lanenet_model/vgg_frontend/vgg16_decode_module/binary_seg_decode/decode_stage_2_fuse/fuse_feats on GPU 3210 4861525 4864735
++End execute lanenet_model/vgg_frontend/vgg16_encode_module/conv2_2/relu___tr4lanenet_model/vgg_frontend/vgg16_decode_module/instance_seg_decode/decode_stage_2_fuse/fuse_feats on GPU 4958 4864749 4869707
++End execute lanenet_model/vgg_frontend/vgg16_decode_module/instance_seg_decode/decode_stage_2_fuse/deconv_relu___tr4lanenet_model/vgg_frontend/vgg16_decode_module/instance_seg_decode/decode_stage_2_fuse/fuse_feats on GPU 2306 4869735 4872041
++End execute lanenet_model/vgg_frontend/vgg16_decode_module/instance_seg_decode/decode_stage_2_fuse/fuse_feats on GPU 3191 4872055 4875246
++End execute lanenet_model/vgg_frontend/vgg16_decode_module/binary_seg_decode/decode_stage_2_fuse/fuse_feats___tr4lanenet_model/vgg_frontend/vgg16_decode_module/binary_seg_decode/decode_stage_2_fuse/fuse_gn/FusedBatchNorm on GPU 2870 4875259 4878129
++End execute lanenet_model/vgg_frontend/vgg16_decode_module/binary_seg_decode/decode_stage_2_fuse/fuse_gn/FusedBatchNorm on GPU 3350 4878197 4881547
++End execute lanenet_model/vgg_frontend/vgg16_decode_module/instance_seg_decode/decode_stage_2_fuse/fuse_feats___tr4lanenet_model/vgg_frontend/vgg16_decode_module/instance_seg_decode/decode_stage_2_fuse/fuse_gn/FusedBatchNorm on GPU 3131 4881602 4884733
++End execute lanenet_model/vgg_frontend/vgg16_decode_module/instance_seg_decode/decode_stage_2_fuse/fuse_gn/FusedBatchNorm on GPU 3210 4884781 4887991
++End execute lanenet_model/vgg_frontend/vgg16_decode_module/binary_seg_decode/decode_stage_2_fuse/fuse_relu on GPU 3200 4888041 4891241
++End execute lanenet_model/vgg_frontend/vgg16_decode_module/instance_seg_decode/decode_stage_2_fuse/fuse_relu on GPU 2936 4891291 4894227
++End execute lanenet_model/vgg_frontend/vgg16_decode_
+\ No newline at end of file
+diff --git a/demo/android/app/includes/MNN/Interpreter.hpp b/demo/android/app/includes/MNN/Interpreter.hpp
+index b5eae79..cd7bb05 100644
+--- a/demo/android/app/includes/MNN/Interpreter.hpp
++++ b/demo/android/app/includes/MNN/Interpreter.hpp
+@@ -26,7 +26,10 @@ struct ScheduleConfig {
+     MNNForwardType type = MNN_FORWARD_CPU;
+     /** number of threads in parallel */
+     int numThread = 4;
+-
++    /** Set is parallel */
++    MNNExecutionType executionType = false;
++    /** Set op to device map file */
++    char * deviceMapFilePath = nullptr;
+     /** subpath to run */
+     struct Path {
+         std::vector<std::string> inputs;
+diff --git a/demo/android/app/includes/MNN/Tensor.hpp b/demo/android/app/includes/MNN/Tensor.hpp
+index 33f8d88..8487604 100644
+--- a/demo/android/app/includes/MNN/Tensor.hpp
++++ b/demo/android/app/includes/MNN/Tensor.hpp
+@@ -15,6 +15,7 @@
+ 
+ namespace MNN {
+ 
++
+ /**
+  * data container.
+  * data for host tensor is saved in `host` field. its memory is allocated malloc directly.
+diff --git a/include/MNN/Interpreter.hpp b/include/MNN/Interpreter.hpp
+index b3f547b..63cf135 100644
+--- a/include/MNN/Interpreter.hpp
++++ b/include/MNN/Interpreter.hpp
+@@ -15,6 +15,7 @@
+ #include <MNN/ErrorCode.hpp>
+ #include <MNN/MNNForwardType.h>
+ #include <MNN/Tensor.hpp>
++#include "core/MUtils.hpp"
+ 
+ namespace MNN {
+ 
+@@ -26,6 +27,12 @@ struct ScheduleConfig {
+     MNNForwardType type = MNN_FORWARD_CPU;
+     /** number of threads in parallel */
+     int numThread = 4;
++    /** Set is parallel */
++    MNNExecutionType executionType = MNNExecutionType::MNN_EXECUTION_DEFAULT;
++    /** Set op to device map file */
++    char * deviceMapFilePath = nullptr;
++    /** When run model with Big&Little, use this as little thread number*/
++    int CPULittleNumThread = 1;
+ 
+     /** subpath to run */
+     struct Path {
+@@ -166,7 +173,7 @@ public:
+      * @return result of running.
+      */
+     ErrorCode runSession(Session* session) const;
+-
++    ErrorCode runSession(Session* session, std::vector<RunMetadata> *net_stats);
+     /*
+      * @brief run session.
+      * @param session   given session.
+diff --git a/include/MNN/MNNForwardType.h b/include/MNN/MNNForwardType.h
+index f9059af..e7d5cef 100644
+--- a/include/MNN/MNNForwardType.h
++++ b/include/MNN/MNNForwardType.h
+@@ -39,9 +39,37 @@ typedef enum {
+     MNN_FORWARD_USER_1 = 9,
+     MNN_FORWARD_USER_2 = 10,
+     MNN_FORWARD_USER_3 = 11,
++    MNN_FORWARD_CONVERT = 12,
+ 
+     MNN_FORWARD_ALL
+ } MNNForwardType;
++
++
++typedef enum {
++    /* DEFAULT: Execute units in serial using CPU or GPU backend */
++    MNN_EXECUTION_DEFAULT = 0,
++    /* SERIAL_HYBRID: Execute units in serial, but execute one unit on CPU or GPU backend */
++    MNN_EXECUTION_SERIAL_HYBRID = 1,
++    /* PARALLEL: Execute units in parallel using CPU and GPU */
++    MNN_EXECUTION_PARALLEL = 2,
++    /* BIG_LITTEL: Execute units in parallel using big cores and little core, Not Support yet */
++    MNN_EXECUTION_BIG_LITTLE = 3,
++} MNNExecutionType;
++
++/**
++ * The concept of big/little is not strictly aligned with the real Big/LITTLE archtecture
++ * On symmetrical CPU cores (like 8-core CPU) we split different CPU cores to big/little
++ * */ 
++typedef enum {
++    /* BIG; Execute units one CPU's big cluster */
++    MNN_CPU_BIG = 0,
++    /* MEDIUM; Execute units one CPU's medium cluster. */
++    MNN_CPU_MEDIUM = 1,
++    /* BIG; Execute units one CPU's big cluster */
++
\ No newline at end of file
diff --git a/schema/current/BasicOptimizer_generated.h b/schema/current/BasicOptimizer_generated.h
index c8647be..6bb12f7 100644
--- a/schema/current/BasicOptimizer_generated.h
+++ b/schema/current/BasicOptimizer_generated.h
@@ -4,7 +4,6 @@
 #ifndef FLATBUFFERS_GENERATED_BASICOPTIMIZER_MNN_OPTIMIZER_H_
 #define FLATBUFFERS_GENERATED_BASICOPTIMIZER_MNN_OPTIMIZER_H_
 
-#include "flatbuffers/flatbuffers.h"
 
 #include "CaffeOp_generated.h"
 #include "GpuLibrary_generated.h"
diff --git a/schema/current/CaffeOp_generated.h b/schema/current/CaffeOp_generated.h
index 816564b..218aa7d 100644
--- a/schema/current/CaffeOp_generated.h
+++ b/schema/current/CaffeOp_generated.h
@@ -4,7 +4,6 @@
 #ifndef FLATBUFFERS_GENERATED_CAFFEOP_MNN_H_
 #define FLATBUFFERS_GENERATED_CAFFEOP_MNN_H_
 
-#include "flatbuffers/flatbuffers.h"
 
 #include "Tensor_generated.h"
 #include "Type_generated.h"
@@ -205,7 +204,7 @@ inline const char * const *EnumNamesPadMode() {
 
 inline const char *EnumNamePadMode(PadMode e) {
   if (e < PadMode_CAFFE || e > PadMode_SAME) return "";
-  const size_t index = static_cast<size_t>(e);
+  const size_t index = static_cast<int>(e);
   return EnumNamesPadMode()[index];
 }
 
@@ -235,7 +234,7 @@ inline const char * const *EnumNamesPoolType() {
 
 inline const char *EnumNamePoolType(PoolType e) {
   if (e < PoolType_MAXPOOL || e > PoolType_AVEPOOL) return "";
-  const size_t index = static_cast<size_t>(e);
+  const size_t index = static_cast<int>(e);
   return EnumNamesPoolType()[index];
 }
 
@@ -268,7 +267,7 @@ inline const char * const *EnumNamesPoolPadType() {
 
 inline const char *EnumNamePoolPadType(PoolPadType e) {
   if (e < PoolPadType_CAFFE || e > PoolPadType_SAME) return "";
-  const size_t index = static_cast<size_t>(e);
+  const size_t index = static_cast<int>(e);
   return EnumNamesPoolPadType()[index];
 }
 
@@ -304,7 +303,7 @@ inline const char * const *EnumNamesEltwiseType() {
 
 inline const char *EnumNameEltwiseType(EltwiseType e) {
   if (e < EltwiseType_PROD || e > EltwiseType_SUB) return "";
-  const size_t index = static_cast<size_t>(e);
+  const size_t index = static_cast<int>(e);
   return EnumNamesEltwiseType()[index];
 }
 
diff --git a/schema/current/GpuLibrary_generated.h b/schema/current/GpuLibrary_generated.h
index eb29567..321f816 100644
--- a/schema/current/GpuLibrary_generated.h
+++ b/schema/current/GpuLibrary_generated.h
@@ -4,7 +4,6 @@
 #ifndef FLATBUFFERS_GENERATED_GPULIBRARY_MNN_H_
 #define FLATBUFFERS_GENERATED_GPULIBRARY_MNN_H_
 
-#include "flatbuffers/flatbuffers.h"
 
 #include "Tensor_generated.h"
 #include "Type_generated.h"
@@ -65,7 +64,7 @@ inline const char * const *EnumNamesSTORAGE_TYPE() {
 
 inline const char *EnumNameSTORAGE_TYPE(STORAGE_TYPE e) {
   if (e < STORAGE_TYPE_BUFFER || e > STORAGE_TYPE_IMAGE) return "";
-  const size_t index = static_cast<size_t>(e);
+  const size_t index = static_cast<int>(e);
   return EnumNamesSTORAGE_TYPE()[index];
 }
 
@@ -98,7 +97,7 @@ inline const char * const *EnumNamesACCESS_TYPE() {
 
 inline const char *EnumNameACCESS_TYPE(ACCESS_TYPE e) {
   if (e < ACCESS_TYPE_READ_ONLY || e > ACCESS_TYPE_READ_WRITE) return "";
-  const size_t index = static_cast<size_t>(e);
+  const size_t index = static_cast<int>(e);
   return EnumNamesACCESS_TYPE()[index];
 }
 
diff --git a/schema/current/MNN_generated.h b/schema/current/MNN_generated.h
index 658e1a2..96cbf63 100644
--- a/schema/current/MNN_generated.h
+++ b/schema/current/MNN_generated.h
@@ -4,7 +4,6 @@
 #ifndef FLATBUFFERS_GENERATED_MNN_MNN_H_
 #define FLATBUFFERS_GENERATED_MNN_MNN_H_
 
-#include "flatbuffers/flatbuffers.h"
 
 #include "CaffeOp_generated.h"
 #include "GpuLibrary_generated.h"
@@ -870,7 +869,7 @@ inline const char * const *EnumNamesOpType() {
 
 inline const char *EnumNameOpType(OpType e) {
   if (e < OpType_AbsVal || e > OpType_EltwiseInt8) return "";
-  const size_t index = static_cast<size_t>(e);
+  const size_t index = static_cast<int>(e);
   return EnumNamesOpType()[index];
 }
 
@@ -1149,7 +1148,7 @@ inline const char * const *EnumNamesOpParameter() {
 
 inline const char *EnumNameOpParameter(OpParameter e) {
   if (e < OpParameter_NONE || e > OpParameter_PadParam) return "";
-  const size_t index = static_cast<size_t>(e);
+  const size_t index = static_cast<int>(e);
   return EnumNamesOpParameter()[index];
 }
 
@@ -1513,11 +1512,10 @@ struct OpParameterUnion {
 #ifndef FLATBUFFERS_CPP98_STL
   template <typename T>
   void Set(T&& val) {
-    using RT = typename std::remove_reference<T>::type;
     Reset();
-    type = OpParameterTraits<typename RT::TableType>::enum_value;
+    type = OpParameterTraits<typename T::TableType>::enum_value;
     if (type != OpParameter_NONE) {
-      value = new RT(std::forward<T>(val));
+      value = new T(std::forward<T>(val));
     }
   }
 #endif  // FLATBUFFERS_CPP98_STL
@@ -2237,7 +2235,7 @@ inline const char * const *EnumNamesForwardType() {
 
 inline const char *EnumNameForwardType(ForwardType e) {
   if (e < ForwardType_CPU || e > ForwardType_VULKAN) return "";
-  const size_t index = static_cast<size_t>(e);
+  const size_t index = static_cast<int>(e);
   return EnumNamesForwardType()[index];
 }
 
@@ -2267,7 +2265,7 @@ inline const char * const *EnumNamesUsage() {
 
 inline const char *EnumNameUsage(Usage e) {
   if (e < Usage_INFERENCE || e > Usage_TRAIN) return "";
-  const size_t index = static_cast<size_t>(e);
+  const size_t index = static_cast<int>(e);
   return EnumNamesUsage()[index];
 }
 
diff --git a/schema/current/TFQuantizeOp_generated.h b/schema/current/TFQuantizeOp_generated.h
index aede81f..18360b9 100644
--- a/schema/current/TFQuantizeOp_generated.h
+++ b/schema/current/TFQuantizeOp_generated.h
@@ -4,7 +4,6 @@
 #ifndef FLATBUFFERS_GENERATED_TFQUANTIZEOP_MNN_H_
 #define FLATBUFFERS_GENERATED_TFQUANTIZEOP_MNN_H_
 
-#include "flatbuffers/flatbuffers.h"
 
 #include "CaffeOp_generated.h"
 #include "Tensor_generated.h"
@@ -138,7 +137,7 @@ inline const char * const *EnumNamesFusedActivation() {
 
 inline const char *EnumNameFusedActivation(FusedActivation e) {
   if (e < FusedActivation_kTfLiteActNone || e > FusedActivation_kTfLiteActSigmoid) return "";
-  const size_t index = static_cast<size_t>(e);
+  const size_t index = static_cast<int>(e);
   return EnumNamesFusedActivation()[index];
 }
 
@@ -168,7 +167,7 @@ inline const char * const *EnumNamesModeFormat() {
 
 inline const char *EnumNameModeFormat(ModeFormat e) {
   if (e < ModeFormat_TENSORFLOW || e > ModeFormat_TFLITE) return "";
-  const size_t index = static_cast<size_t>(e);
+  const size_t index = static_cast<int>(e);
   return EnumNamesModeFormat()[index];
 }
 
@@ -201,7 +200,7 @@ inline const char * const *EnumNamesQuantizeMode() {
 
 inline const char *EnumNameQuantizeMode(QuantizeMode e) {
   if (e < QuantizeMode_MIN_COMBINED || e > QuantizeMode_SCALED) return "";
-  const size_t index = static_cast<size_t>(e);
+  const size_t index = static_cast<int>(e);
   return EnumNamesQuantizeMode()[index];
 }
 
@@ -231,7 +230,7 @@ inline const char * const *EnumNamesQuantizeRoundMode() {
 
 inline const char *EnumNameQuantizeRoundMode(QuantizeRoundMode e) {
   if (e < QuantizeRoundMode_HALF_AWAY_FROM_ZERO || e > QuantizeRoundMode_HALF_TO_EVEN) return "";
-  const size_t index = static_cast<size_t>(e);
+  const size_t index = static_cast<int>(e);
   return EnumNamesQuantizeRoundMode()[index];
 }
 
diff --git a/schema/current/Tensor_generated.h b/schema/current/Tensor_generated.h
index e66d4d0..f12b7b9 100644
--- a/schema/current/Tensor_generated.h
+++ b/schema/current/Tensor_generated.h
@@ -4,7 +4,6 @@
 #ifndef FLATBUFFERS_GENERATED_TENSOR_MNN_H_
 #define FLATBUFFERS_GENERATED_TENSOR_MNN_H_
 
-#include "flatbuffers/flatbuffers.h"
 
 #include "Type_generated.h"
 
@@ -60,7 +59,7 @@ inline const char * const *EnumNamesMNN_DATA_FORMAT() {
 
 inline const char *EnumNameMNN_DATA_FORMAT(MNN_DATA_FORMAT e) {
   if (e < MNN_DATA_FORMAT_NCHW || e > MNN_DATA_FORMAT_UNKNOWN) return "";
-  const size_t index = static_cast<size_t>(e);
+  const size_t index = static_cast<int>(e);
   return EnumNamesMNN_DATA_FORMAT()[index];
 }
 
diff --git a/schema/current/TensorflowOp_generated.h b/schema/current/TensorflowOp_generated.h
index 14a2597..8183e4b 100644
--- a/schema/current/TensorflowOp_generated.h
+++ b/schema/current/TensorflowOp_generated.h
@@ -4,7 +4,6 @@
 #ifndef FLATBUFFERS_GENERATED_TENSORFLOWOP_MNN_H_
 #define FLATBUFFERS_GENERATED_TENSORFLOWOP_MNN_H_
 
-#include "flatbuffers/flatbuffers.h"
 
 #include "Tensor_generated.h"
 #include "Type_generated.h"
@@ -268,7 +267,7 @@ inline const char * const *EnumNamesBinaryOpOperation() {
 
 inline const char *EnumNameBinaryOpOperation(BinaryOpOperation e) {
   if (e < BinaryOpOperation_ADD || e > BinaryOpOperation_NOTEQUAL) return "";
-  const size_t index = static_cast<size_t>(e);
+  const size_t index = static_cast<int>(e);
   return EnumNamesBinaryOpOperation()[index];
 }
 
@@ -319,7 +318,7 @@ inline const char * const *EnumNamesReductionType() {
 
 inline const char *EnumNameReductionType(ReductionType e) {
   if (e < ReductionType_SUM || e > ReductionType_ALL) return "";
-  const size_t index = static_cast<size_t>(e);
+  const size_t index = static_cast<int>(e);
   return EnumNamesReductionType()[index];
 }
 
@@ -430,7 +429,7 @@ inline const char * const *EnumNamesUnaryOpOperation() {
 
 inline const char *EnumNameUnaryOpOperation(UnaryOpOperation e) {
   if (e < UnaryOpOperation_ABS || e > UnaryOpOperation_EXPM1) return "";
-  const size_t index = static_cast<size_t>(e);
+  const size_t index = static_cast<int>(e);
   return EnumNamesUnaryOpOperation()[index];
 }
 
@@ -460,7 +459,7 @@ inline const char * const *EnumNamesCropAndResizeMethod() {
 
 inline const char *EnumNameCropAndResizeMethod(CropAndResizeMethod e) {
   if (e < CropAndResizeMethod_BILINEAR || e > CropAndResizeMethod_NEAREST) return "";
-  const size_t index = static_cast<size_t>(e);
+  const size_t index = static_cast<int>(e);
   return EnumNamesCropAndResizeMethod()[index];
 }
 
@@ -493,7 +492,7 @@ inline const char * const *EnumNamesPadValueMode() {
 
 inline const char *EnumNamePadValueMode(PadValueMode e) {
   if (e < PadValueMode_CONSTANT || e > PadValueMode_SYMMETRIC) return "";
-  const size_t index = static_cast<size_t>(e);
+  const size_t index = static_cast<int>(e);
   return EnumNamesPadValueMode()[index];
 }
 
diff --git a/schema/current/Type_generated.h b/schema/current/Type_generated.h
index e68bb8f..f444072 100644
--- a/schema/current/Type_generated.h
+++ b/schema/current/Type_generated.h
@@ -40,7 +40,7 @@ inline const char * const *EnumNamesNetSource() {
 
 inline const char *EnumNameNetSource(NetSource e) {
   if (e < NetSource_CAFFE || e > NetSource_ONNX) return "";
-  const size_t index = static_cast<size_t>(e);
+  const size_t index = static_cast<int>(e);
   return EnumNamesNetSource()[index];
 }
 
@@ -130,7 +130,7 @@ inline const char * const *EnumNamesDataType() {
 
 inline const char *EnumNameDataType(DataType e) {
   if (e < DataType_DT_INVALID || e > DataType_DT_VARIANT) return "";
-  const size_t index = static_cast<size_t>(e);
+  const size_t index = static_cast<int>(e);
   return EnumNamesDataType()[index];
 }
 
diff --git a/schema/current/UserDefine_generated.h b/schema/current/UserDefine_generated.h
index 10e461c..7935cf2 100644
--- a/schema/current/UserDefine_generated.h
+++ b/schema/current/UserDefine_generated.h
@@ -4,7 +4,6 @@
 #ifndef FLATBUFFERS_GENERATED_USERDEFINE_MNN_H_
 #define FLATBUFFERS_GENERATED_USERDEFINE_MNN_H_
 
-#include "flatbuffers/flatbuffers.h"
 
 #include "Tensor_generated.h"
 #include "Type_generated.h"
diff --git a/source/backend/cpu/CPUBackend.cpp b/source/backend/cpu/CPUBackend.cpp
index b000dad..9ea1f48 100644
--- a/source/backend/cpu/CPUBackend.cpp
+++ b/source/backend/cpu/CPUBackend.cpp
@@ -59,7 +59,9 @@ CPUBackend::CPUBackend(int numberThread, BackendConfig::MemoryMode memory, Backe
             MNNSetCPUThreadsMode(MNN_CPU_MODE_LITTLE);
             break;
         case BackendConfig::Power_High:
-            MNNSetCPUThreadsMode(MNN_CPU_MODE_POWER_FRI);
+            MNN_PRINT("CPUBackend BackendConfig::Power_High\n");
+            // MNNSetCPUThreadsMode(MNN_CPU_MODE_POWER_FRI);
+            MNNSetCPUThreadsMode(MNN_CPU_MODE_BIG);
             break;
         default:
             break;
@@ -103,6 +105,7 @@ void CPUBackend::onExecuteBegin() const {
     }
 #else
 #ifdef _OPENMP
+    MNN_PRINT("In OpenMP set thread number: %d\n", mThreadNumber);
     omp_set_dynamic(0);
     omp_set_num_threads(mThreadNumber);
 #endif
@@ -185,6 +188,12 @@ std::pair<float, bool> CPUBackend::onMeasure(const std::vector<Tensor*>& inputs,
     return std::make_pair(computeFlops / mFlops * 1000.0f, true);
 }
 
+bool CPUBackend::isSupportOpType(const MNN::Op* op){
+    auto map  = getCreatorMap();
+    auto iter = map->find(op->type());
+    return (iter != map->end());
+}
+
 /// get execution
 Execution* CPUBackend::onCreate(const std::vector<Tensor*>& inputs, const std::vector<Tensor*>& outputs,
                                 const MNN::Op* op) {
diff --git a/source/backend/cpu/CPUBackend.hpp b/source/backend/cpu/CPUBackend.hpp
index 3ce6b38..f591710 100644
--- a/source/backend/cpu/CPUBackend.hpp
+++ b/source/backend/cpu/CPUBackend.hpp
@@ -33,7 +33,7 @@ public:
     virtual void onCopyBuffer(const Tensor* srcTensor, const Tensor* dstTensor) const override;
     virtual std::pair<float, bool> onMeasure(const std::vector<Tensor*>& inputs, const std::vector<Tensor*>& outputs,
                                             const MNN::Op* op) override;
-
+    virtual bool isSupportOpType(const MNN::Op* op) override;
     virtual Execution* onCreate(const std::vector<Tensor*>& inputs, const std::vector<Tensor*>& outputs,
                                 const MNN::Op* op) override;
     virtual void onExecuteBegin() const override;
@@ -76,6 +76,15 @@ private:
     const BackendConfig::PowerMode mPower;
     bool mCheckNAN = false;
     float mFlops = 0.0f;
+public:
+    /** In parallel execution, a tensor may be transformed multi-times, 
+    * we use this to cache the tensor's data transformation
+    */
+    std::map<Tensor *, Tensor *> mDataTransBuff;
+    void clearDataTransBuff() {
+        mDataTransBuff.clear();
+    }
+
 };
 
 #ifdef MNN_CODEGEN_REGISTER
diff --git a/source/backend/cpu/CPUConvolutionDepthwise.cpp b/source/backend/cpu/CPUConvolutionDepthwise.cpp
index 35a637f..25a836f 100644
--- a/source/backend/cpu/CPUConvolutionDepthwise.cpp
+++ b/source/backend/cpu/CPUConvolutionDepthwise.cpp
@@ -74,7 +74,7 @@ CPUConvolutionDepthwise::CPUConvolutionDepthwise(const Op* op, Backend* backend)
         originWeight     = conv2d->weight()->data();
         originWeightSize = conv2d->weight()->size();
     }
-
+    //MNN_PRINT("CPUConvolutionDepthwise::CPUConvolutionDepthwise new FloatExecution\n");
     mSubExecution.reset(new FloatExecution(conv2d->common(), backend, originWeight, originWeightSize,
                                            conv2d->bias()->data(), conv2d->bias()->size()));
 }
diff --git a/source/backend/cpu/CPUDeconvolution.cpp b/source/backend/cpu/CPUDeconvolution.cpp
index 6a8a319..a300e2a 100644
--- a/source/backend/cpu/CPUDeconvolution.cpp
+++ b/source/backend/cpu/CPUDeconvolution.cpp
@@ -152,6 +152,7 @@ ErrorCode CPUDeconvolutionMultiInput::onResize(const std::vector<Tensor*>& input
 }
 
 ErrorCode CPUDeconvolutionOrigin::onResize(const std::vector<Tensor*>& inputs, const std::vector<Tensor*>& outputs) {
+    MNN_PRINT("In CPUDeconvolutionOrigin::onResize\n");
     CPUDeconvolutionBasic::onResize(inputs, outputs);
     auto input  = inputs[0];
     auto output = outputs[0];
@@ -264,8 +265,10 @@ ErrorCode CPUDeconvolutionOrigin::onResize(const std::vector<Tensor*>& inputs, c
 }
 
 ErrorCode CPUDeconvolutionOrigin::onExecute(const std::vector<Tensor*>& inputs, const std::vector<Tensor*>& outputs) {
+    
     for (auto& unit : mComputors) {
         MNN_CONCURRENCY_BEGIN(tId, unit.matrixMulti.size()) {
+            MNN_PRINT("CPUDeconvolutionOrigin %d, size %lu\n", tId, unit.matrixMulti.size());
             unit.matrixMulti[tId]->onExecute();
         }
         MNN_CONCURRENCY_END();
diff --git a/source/backend/cpu/CPUPool.cpp b/source/backend/cpu/CPUPool.cpp
index ca35b13..d2af67c 100644
--- a/source/backend/cpu/CPUPool.cpp
+++ b/source/backend/cpu/CPUPool.cpp
@@ -360,7 +360,7 @@ ErrorCode CPUPool::onResize(const std::vector<Tensor *> &inputs, const std::vect
     int strideHeight = layer->strideY();
     int padWidth     = layer->padX();
     int padHeight    = layer->padY();
-
+    // MNN_PRINT("CPUPool::onResize\n");
     // edit const if global
     auto input       = inputs[0];
     auto output      = outputs[0];
@@ -401,12 +401,15 @@ ErrorCode CPUPool::onResize(const std::vector<Tensor *> &inputs, const std::vect
                           outputData + outputPlaneStride * channel, output->width(), output->height(), kernelWidth,
                           kernelHeight, strideWidth, strideHeight, padWidth, padHeight, padType);
         }
+        // MNN_PRINT("In CPUPool.cpp execute %d end\n", tId);
     });
     return NO_ERROR;
 }
 
 ErrorCode CPUPool::onExecute(const std::vector<Tensor *> &inputs, const std::vector<Tensor *> &outputs) {
+    // MNN_PRINT("In CPUPool.cpp CPUPool onExecute start\n");
     MNN_CONCURRENCY_BEGIN(tId, mFunction.first) {
+        // MNN_PRINT("In CPUPool.cpp CPUPool onExecute %d end\n", tId);
         mFunction.second((int)tId);
     }
     MNN_CONCURRENCY_END();
@@ -442,6 +445,7 @@ CPUPool3D::CPUPool3D(Backend *b, const Pool3D *param) : MNN::Execution(b) {
 ErrorCode CPUPool3D::onResize(const std::vector<Tensor *> &inputs, const std::vector<Tensor *> &outputs) {
     auto input = inputs[0];
     auto output = outputs[0];
+    MNN_PRINT("CPUPool3D::onResize\n");
     if (mPadType == PoolPadType_SAME) {
         mPads.clear();
         for (unsigned int i = 0; i < output->dimensions() - 2; ++i) {
@@ -465,7 +469,7 @@ ErrorCode CPUPool3D::onExecute(const std::vector<Tensor *> &inputs, const std::v
     auto input = inputs[0];
     auto output = outputs[0];
     MNN_ASSERT(input->dimensions() == 5);
-
+    // MNN_PRINT("In CPUPool.cpp CPUPool3D onExecute start\n");
     const int kernelDepth = mKernels[0], kernelHeight = mKernels[1], kernelWidth = mKernels[2];
     const int strideDepth = mStrides[0], strideHeight = mStrides[1], strideWidth = mStrides[2];
     const int outputDepth = output->length(2), outputHeight = output->length(3), outputWidth = output->length(4);
@@ -473,7 +477,7 @@ ErrorCode CPUPool3D::onExecute(const std::vector<Tensor *> &inputs, const std::v
     const int channel = input->length(1), batch = input->length(0);
     const int padDepth = mPads[0], padHeight = mPads[1], padWidth = mPads[2];
     const int threadNumber = ((CPUBackend*)backend())->threadNumber();
-
+    
     {
         auto planeFunction = poolingMax;
         if (mType == PoolType_AVEPOOL) {
@@ -493,6 +497,7 @@ ErrorCode CPUPool3D::onExecute(const std::vector<Tensor *> &inputs, const std::v
             }
         };
         MNN_CONCURRENCY_BEGIN(tId, threadNumber) {
+            // MNN_PRINT("In CPUPool.cpp CPUPool3D onExecute %d end\n", tId);
             planeFunc((int)tId);
         }
         MNN_CONCURRENCY_END();
diff --git a/source/backend/cpu/CPURuntime.cpp b/source/backend/cpu/CPURuntime.cpp
index d8522dc..6fe4b6c 100644
--- a/source/backend/cpu/CPURuntime.cpp
+++ b/source/backend/cpu/CPURuntime.cpp
@@ -13,6 +13,7 @@
 #include <stdint.h>
 #include <sys/syscall.h>
 #include <unistd.h>
+#include <algorithm>
 #endif
 
 #if __APPLE__
@@ -210,6 +211,7 @@ int MNNSetCPUThreadsMode(MNNCPUThreadsMode mode) {
 #pragma omp parallel for
     for (int i = 0; i < threadsNumber; ++i) {
         result[i] = setSchedAffinity(cpuAttachIDs);
+        MNN_PRINT("CPURuntime thread %d cpuAttachIDs %d\n", i, cpuAttachIDs[i]);
     }
     for (int i = 0; i < threadsNumber; ++i) {
         if (result[i] != 0) {
diff --git a/source/backend/cpu/compute/Convolution1x1Strassen.cpp b/source/backend/cpu/compute/Convolution1x1Strassen.cpp
index c54aa4c..6710033 100644
--- a/source/backend/cpu/compute/Convolution1x1Strassen.cpp
+++ b/source/backend/cpu/compute/Convolution1x1Strassen.cpp
@@ -5,6 +5,11 @@
 //  Created by MNN on 2019/02/12.
 //  Copyright  2018, Alibaba Group Holding Limited
 //
+#include <sched.h>
+#include <sys/stat.h>
+#include <pthread.h>
+#include <sched.h>
+#include <unistd.h>
 
 #include "backend/cpu/compute/Convolution1x1Strassen.hpp"
 #include <string.h>
@@ -66,6 +71,7 @@ ErrorCode Convolution1x1Strassen::onResize(const std::vector<Tensor *> &inputs,
     auto input       = inputs[0];
     auto output      = outputs[0];
     int numberThread = ((CPUBackend *)backend())->threadNumber();
+    mThreadNumber = numberThread;
     auto icC4        = UP_DIV(input->channel(), 4);
     auto ocC4        = UP_DIV(output->channel(), 4);
     auto outputPlane = output->height() * output->width();
@@ -242,8 +248,20 @@ ErrorCode Convolution1x1Strassen::onExecute(const std::vector<Tensor *> &inputs,
     auto size   = mUnits.size();
     auto input  = inputs[0];
     auto output = outputs[0];
+    
     if (!mNeedPretreat) {
         MNN_CONCURRENCY_BEGIN(tId, size) {
+            //if(setAffinityCount / mThreadNumber <= 3){
+                // cpu_set_t cpusetOfCPU;
+                // CPU_ZERO(&cpusetOfCPU);
+                // CPU_SET((4+tId)%8, &cpusetOfCPU);
+                // pid_t pid = pthread_gettid_np(pthread_self());
+                // int rc = sched_setaffinity(pid, sizeof(cpusetOfCPU), &cpusetOfCPU);
+                // MNN_PRINT("Convolution1x1Strassen::OnExecute: %d\n", omp_get_num_threads());
+                // MNN_PRINT("Convolution1x1Strassen::OnExecute tId %d pid %d on core %d ret %d\n", tId, pid, sched_getcpu(), rc);
+                // MNN_PRINT("Convolution1x1Strassen::OnExecute tId %d on core %d\n", tId, sched_getcpu());
+                // setAffinityCount++;
+            //}
             auto &unit = mUnits[tId];
             if (unit.mValid) {
                 unit.mStracssenComputor->onExecute();
diff --git a/source/backend/cpu/compute/Convolution1x1Strassen.hpp b/source/backend/cpu/compute/Convolution1x1Strassen.hpp
index c7b7cea..ca40b01 100644
--- a/source/backend/cpu/compute/Convolution1x1Strassen.hpp
+++ b/source/backend/cpu/compute/Convolution1x1Strassen.hpp
@@ -47,6 +47,8 @@ private:
     std::shared_ptr<Tensor> mTempOutputBatch;
     bool mNeedPretreat = false;
     std::function<void(const float *srcBatch, float *dstBatch)> mPretreatFunction;
+    // static int setAffinityCount;
+    int mThreadNumber = 1;
 };
 } // namespace MNN
 
diff --git a/source/backend/cpu/compute/Convolution3x3.cpp b/source/backend/cpu/compute/Convolution3x3.cpp
index 4b12d48..e09f44f 100644
--- a/source/backend/cpu/compute/Convolution3x3.cpp
+++ b/source/backend/cpu/compute/Convolution3x3.cpp
@@ -5,6 +5,11 @@
 //  Created by MNN on 2018/07/16.
 //  Copyright  2018, Alibaba Group Holding Limited
 //
+#include <sched.h>
+#include <sys/stat.h>
+#include <pthread.h>
+#include <sched.h>
+#include <unistd.h>
 
 #include "backend/cpu/compute/Convolution3x3.hpp"
 #include <MNN/AutoTime.hpp>
@@ -236,7 +241,7 @@ ErrorCode Convolution3x3::onResize(const std::vector<Tensor*>& inputs, const std
 ErrorCode Convolution3x3::onExecute(const std::vector<Tensor*>& inputs, const std::vector<Tensor*>& outputs) {
     auto input  = inputs[0];
     auto output = outputs[0];
-
+    // MNN_PRINT("3*3\n");
     int ow   = output->width();
     int oh   = output->height();
     int iw   = input->width();
@@ -251,7 +256,6 @@ ErrorCode Convolution3x3::onExecute(const std::vector<Tensor*>& inputs, const st
     const int totalCount = hUnit * wUnit;
     const int tileCount = UP_DIV(totalCount, CONVOLUTION_TILED_NUMBER);
     const int threadNumber = ((CPUBackend*)backend())->threadNumber();
-
     auto postFunction = mPostFunction;
 
     auto sourceTransformFunc = [=](int xIndex, int xC, const float* srcOrigin, float* dstOrigin, float* dstBlock) {
@@ -341,6 +345,8 @@ ErrorCode Convolution3x3::onExecute(const std::vector<Tensor*>& inputs, const st
 
     auto gemmConcurrencyFunc = [=, &gemmFunc](int xC, const float* srcOrigin, const float* weight, float* dstOrigin) {
         MNN_CONCURRENCY_BEGIN(tId, threadNumber) {
+            MNN_PRINT("CPU->RunUnits set tId %d", tId);
+            // MNN_PRINT("CPU->RunUnits set tId %d pid %d set-on-core %d real-on-core %d ret %d\n", tId, pid, (bigCoreIndex+tId), sched_getcpu(), rc);
             const int step = UP_DIV(BLOCK_UNIT2, threadNumber);
             gemmFunc(xC, tId * step, ALIMIN((tId + 1) * step, BLOCK_UNIT2), srcOrigin, weight, dstOrigin);
         }
@@ -371,9 +377,15 @@ ErrorCode Convolution3x3::onExecute(const std::vector<Tensor*>& inputs, const st
     for (int batchIndex = 0; batchIndex < input->batch(); ++batchIndex) {
         auto srcOrigin = input->host<float>() + iw * ih * ic_4 * 4 * batchIndex;
         auto dstOrigin = output->host<float>() + ow * oh * dc_4 * 4 * batchIndex;
-
+        
         if (tileCount >= threadNumber) {
             MNN_CONCURRENCY_BEGIN(tId, threadNumber) {
+                // cpu_set_t cpusetOfCPU;
+                // CPU_ZERO(&cpusetOfCPU);
+                // CPU_SET(8-1-tId, &cpusetOfCPU);
+                // pid_t pid = pthread_gettid_np(pthread_self());
+                // int rc = sched_setaffinity(pid, sizeof(cpusetOfCPU), &cpusetOfCPU);
+                // MNN_PRINT("Convlution3x3::OnExecute tId %d pid %d on core %d ret %d\n", tId, pid, sched_getcpu(), rc);
                 tFunction((int)tId, (int)tId, threadNumber, tileCount / threadNumber * threadNumber, srcOrigin, dstOrigin);
             }
             MNN_CONCURRENCY_END();
diff --git a/source/backend/cpu/compute/ConvolutionFloatFactory.cpp b/source/backend/cpu/compute/ConvolutionFloatFactory.cpp
index 8352cb2..4f2ffe2 100644
--- a/source/backend/cpu/compute/ConvolutionFloatFactory.cpp
+++ b/source/backend/cpu/compute/ConvolutionFloatFactory.cpp
@@ -21,6 +21,7 @@ namespace MNN {
 static Execution* _createUnit(const Tensor* input, const Tensor* output, Backend* backend,
                               const Convolution2DCommon* common, const float* originWeight, size_t originWeightSize,
                               const float* bias, size_t biasSize) {
+    // MNN_PRINT("Convolution kernel size (%d, %d)\n", common->kernelX(), common->kernelY());
     auto layer   = common;
     bool fastWay = layer->kernelY() == 1 && layer->kernelX() == 1;
     if (fastWay) {
diff --git a/source/backend/opencl/core/ImageBufferConvertor.cpp b/source/backend/opencl/core/ImageBufferConvertor.cpp
index 82c6eb3..e62b134 100644
--- a/source/backend/opencl/core/ImageBufferConvertor.cpp
+++ b/source/backend/opencl/core/ImageBufferConvertor.cpp
@@ -6,7 +6,10 @@
 //  Copyright  2018, Alibaba Group Holding Limited
 //
 
+#include <arm_neon.h>
 #include "backend/opencl/core/ImageBufferConvertor.hpp"
+#include "core/MUtils.hpp"
+#include "backend/opencl/core/ImageHostConvertor.hpp"
 
 namespace MNN {
 namespace OpenCL {
@@ -28,7 +31,7 @@ bool convertNCHWBufferToImage(const Tensor *input, Tensor *output, cl::Kernel &b
     bufferToImageKernel.setArg(idx++, static_cast<uint32_t>(outputShape[2]));
     bufferToImageKernel.setArg(idx++, static_cast<uint32_t>(outputShape[3]));
     bufferToImageKernel.setArg(idx++, openCLImage(output));
-
+    
     const uint32_t maxWorkGroupSize = static_cast<uint32_t>(runtime->getMaxWorkGroupSize(bufferToImageKernel));
     const std::vector<uint32_t> lws = {16, std::max((uint32_t)1, maxWorkGroupSize / 16)};
     cl::Event event;
@@ -39,7 +42,10 @@ bool convertNCHWBufferToImage(const Tensor *input, Tensor *output, cl::Kernel &b
     }
     error = runtime->commandQueue().enqueueNDRangeKernel(bufferToImageKernel, cl::NullRange,
                                                          cl::NDRange(roundUpGroupWorkSize[0], roundUpGroupWorkSize[1]),
-                                                         cl::NDRange(lws[0], lws[1]), nullptr, &event);
+                                                         cl::NDRange(lws[0], lws[1]),
+                                                        // cl::NDRange(roundUpGroupWorkSize[1]),
+                                                        // cl::NDRange(lws[0]),
+                                                         nullptr, &event);
     MNN_CHECK_CL_SUCCESS(error);
     if (true == needWait) {
         event.wait();
@@ -47,6 +53,55 @@ bool convertNCHWBufferToImage(const Tensor *input, Tensor *output, cl::Kernel &b
     return true;
 }
 
+
+
+bool convertNHWCHostToImage(const Tensor *input, const Tensor *output, OpenCLRuntime *runtime, bool needWait, bool isFloat16){
+    if(input->batch() == 1){
+        // MNN_PRINT("OpenCL::convertNHWCHostToImage\n");
+        const cl::array<size_t, 3> origin = {0, 0, 0};
+        const cl::array<size_t, 3> region = {(unsigned long)(UP_DIV(input->channel(), 4) * input->width()), 
+                                            (unsigned long)(input->batch() * input->height()), 1};
+        const int element_size = 4;
+        size_t row_pitch;
+        size_t slice_pitch;
+        cl_int error;
+        auto bufferPtr = runtime->commandQueue().enqueueMapImage(openCLImage(output), CL_TRUE, CL_MAP_WRITE, origin, region, 
+                                                                &row_pitch, nullptr, nullptr, nullptr, &error);
+        runtime->commandQueue().flush();
+        runtime->commandQueue().finish();
+        if(isFloat16){
+            OpenCL::nhwc_to_image<float16_t>(input->channel(), input->height(), input->width(), (const float*)input->buffer().host, (float16_t*)bufferPtr, row_pitch);
+        }else{
+            OpenCL::nhwc_to_image<float>(input->channel(), input->height(), input->width(), (float*)bufferPtr, (float*)bufferPtr, row_pitch);
+        }
+    }
+    return true;
+}
+
+
+bool convertImageToNHWCHost(const Tensor *input, const Tensor *output, OpenCLRuntime *runtime, bool needWait, bool isFloat16){
+    if(input->batch() == 1){
+        MNN_PRINT("OpenCL::convertImageToNHWCHost\n");
+        const cl::array<size_t, 3> origin = {0, 0, 0};
+        const cl::array<size_t, 3> region = {(unsigned long)(UP_DIV(input->channel(), 4) * input->width()), 
+                                            (unsigned long)(input->batch() * input->height()), 1};
+        const int element_size = 4;
+        size_t row_pitch;
+        size_t slice_pitch;
+        cl_int error;
+        auto bufferPtr = runtime->commandQueue().enqueueMapImage(openCLImage(input), CL_TRUE, CL_MAP_WRITE, origin, region, 
+                                                                &row_pitch, nullptr, nullptr, nullptr, &error);
+        runtime->commandQueue().flush();
+        runtime->commandQueue().finish();
+        if(isFloat16){
+            OpenCL::image_to_nhwc<float16_t>(input->channel(), input->height(), input->width(), (float16_t*)bufferPtr, (float*)output->buffer().host, row_pitch);
+        }else{
+            OpenCL::image_to_nhwc<float>(input->channel(), input->height(), input->width(), (float*)bufferPtr, (float*)output->buffer().host, row_pitch);
+        }
+    }
+    return true;
+}
+
 bool convertNHWCBufferToImage(const Tensor *input, Tensor *output, cl::Kernel &bufferToImageKernel,
                               OpenCLRuntime *runtime, bool needWait) {
     std::vector<int> outputShape = tensorShapeFormat(input);
@@ -84,6 +139,8 @@ bool convertNHWCBufferToImage(const Tensor *input, Tensor *output, cl::Kernel &b
     return true;
 }
 
+
+
 bool convertImageToNCHWBuffer(const Tensor *input, Tensor *output, cl::Kernel &imageToBufferKernel,
                               OpenCLRuntime *runtime, bool needWait) {
     std::vector<int> inputShape = tensorShapeFormat(input);
@@ -122,9 +179,69 @@ bool convertImageToNCHWBuffer(const Tensor *input, Tensor *output, cl::Kernel &i
     return true;
 }
 
+// When N == 1, convert NC4HW4 to OpenCL Image
+// src_i = ((ci * H) + hi) * W + wi
+// dst_i = ((hi * C) + ci) * W + wi
+// image_i = (hi * CW) + (ci * W + wi)
+
+
+bool convertImageToNC4HW4Host(const Tensor *input, const Tensor *output, OpenCLRuntime *runtime, bool needWait, bool isFloat16){
+    if(input->batch() == 1){
+        MNN_PRINT("OpenCL::convertImageToNC4HW4Host\n");
+        const cl::array<size_t, 3> origin = {0, 0, 0};
+        const cl::array<size_t, 3> region = {(unsigned long)(UP_DIV(input->channel(), 4) * input->width()), 
+                                            (unsigned long)(input->batch() * input->height()), 1};
+        const int element_size = 4;
+        size_t row_pitch;
+        size_t slice_pitch;
+        cl_int error;
+        auto bufferPtr = runtime->commandQueue().enqueueMapImage(openCLImage(input), CL_TRUE, CL_MAP_WRITE, origin, region, 
+                                                                &row_pitch, nullptr, nullptr, nullptr, &error);
+        runtime->commandQueue().flush();
+        runtime->commandQueue().finish();
+        
+        if(isFloat16){
+            OpenCL::image_to_nc4hw4<float16_t>(input->channel(), input->height(), input->width(), (float16_t*)bufferPtr, (float*)output->buffer().host, row_pitch);
+        }else{
+            OpenCL::image_to_nc4hw4<float>(input->channel(), input->height(), input->width(), (float*)bufferPtr, (float*)output->buffer().host, row_pitch);
+        }
+    }
+    return true;
+}
+// When N == 1, convert NC4HW4 to OpenCL Image
+// src_i = ((ci * H) + hi) * W + wi
+// dst_i = ((hi * C) + ci) * W + wi
+// image_i = (hi * CW) + (ci * W + wi)
+
+bool convertNC4HW4HostToImage(const Tensor *input, const Tensor *output, OpenCLRuntime *runtime, bool needWait, bool isFloat16){
+    if(input->batch() == 1){
+        auto needSize = input->size();
+
+        const cl::array<size_t, 3> origin = {0, 0, 0};
+        const cl::array<size_t, 3> region = {(unsigned long)(UP_DIV(input->channel(), 4) * input->width()), 
+                                            (unsigned long)(input->batch() * input->height()),
+                                            1};
+        const int element_size = 4;
+        size_t row_pitch;
+        size_t slice_pitch;
+        cl_int error;
+
+        auto bufferPtr = runtime->commandQueue().enqueueMapImage(openCLImage(output), CL_TRUE, CL_MAP_WRITE, origin, region, 
+        &row_pitch, nullptr, nullptr, nullptr, &error);
+        if(isFloat16){
+            OpenCL::c4hw4_to_image<float16_t>(input->channel(), input->height(), input->width(),
+                            (const float*)input->buffer().host, (const float16_t*)bufferPtr, row_pitch);
+        }else{
+            OpenCL::c4hw4_to_image<float>(input->channel(), input->height(), input->width(),
+                            (const float*)input->buffer().host, (const float*)bufferPtr, row_pitch);
+        }
+    }
+    return true;
+}
+
 bool convertNC4HW4BufferToImage(const Tensor *input, Tensor *output, cl::Kernel &bufferToImageKernel,
                                 OpenCLRuntime *runtime, bool needWait) {
-
+    
     uint32_t outputGlobalWorkSize[2] = {static_cast<uint32_t>(UP_DIV(input->channel(), 4) * input->width()),
                                         static_cast<uint32_t>(input->batch() * input->height())};
     if (bufferToImageKernel.get() == nullptr) {
@@ -133,6 +250,8 @@ bool convertNC4HW4BufferToImage(const Tensor *input, Tensor *output, cl::Kernel
     }
     uint32_t idx   = 0;
     int outputImageShape[2] = {input->height(), input->width()};
+    
+    // printf("In convertNC4HW4BufferToImage output Image size %d %d\n", outputImageShape[0], outputImageShape[1]);
     bufferToImageKernel.setArg(idx++, outputGlobalWorkSize[0]);
     bufferToImageKernel.setArg(idx++, outputGlobalWorkSize[1]);
     bufferToImageKernel.setArg(idx++, openCLBuffer(input));
@@ -148,9 +267,26 @@ bool convertNC4HW4BufferToImage(const Tensor *input, Tensor *output, cl::Kernel
     for (size_t i = 0; i < lws.size(); ++i) {
         roundUpGroupWorkSize[i] = ROUND_UP(outputGlobalWorkSize[i], lws[i]);
     }
+    
+    // printf("global work size(%d, %d)\nlocal work size(%d, %d)\n", 
+    //       roundUpGroupWorkSize[0], roundUpGroupWorkSize[1],
+    //       lws[0], lws[1]);
     error = runtime->commandQueue().enqueueNDRangeKernel(bufferToImageKernel, cl::NullRange,
                                                          cl::NDRange(roundUpGroupWorkSize[0], roundUpGroupWorkSize[1]),
-                                                         cl::NDRange(lws[0], lws[1]), nullptr, &event);
+                                                         cl::NDRange(lws[0], lws[1]),
+                                                        // cl::NDRange(roundUpGroupWorkSize[1]),
+                                                        // cl::NDRange(lws[0]),
+                                                         nullptr, &event);
+    runtime->commandQueue().finish();
+    const int element_size = 4;
+    const cl::array<size_t, 3> origin = {0, 0, 0};
+    const cl::array<size_t, 3> region = {(unsigned long)(UP_DIV(input->channel(), 4) * input->width()), 
+                                        (unsigned long)(input->batch() * input->height()), 1};
+    auto tmpPtr = malloc(input->size());
+    runtime->commandQueue().enqueueReadImage(openCLImage(output), 
+                                            CL_TRUE, origin, region, element_size * sizeof(float) * region[0], 0, (float*)tmpPtr);
+    
+    // print_tensor(input->channel(), input->height(), input->width(), (float*)tmpPtr);
     MNN_CHECK_CL_SUCCESS(error);
     if (true == needWait) {
         event.wait();
@@ -172,7 +308,7 @@ bool convertImageToNC4HW4Buffer(const Tensor *input, Tensor *output, cl::Kernel
     auto inputShape = tensorShapeFormat(input);
     uint32_t in_gws[2]          = {static_cast<uint32_t>(UP_DIV(inputShape.at(3), 4) * inputShape.at(2)),
                           static_cast<uint32_t>(inputShape.at(0) * inputShape.at(1))};
-
+    // MNN_PRINT("In ImageBufferConverter convertImageToNC4HW4Buffer\n");
     if (imageToBufferKernel.get() == nullptr) {
         std::set<std::string> buildOptions;
         imageToBufferKernel = runtime->buildKernel("buffer_to_image", "image_to_nc4hw4_buffer", buildOptions);
@@ -194,6 +330,9 @@ bool convertImageToNC4HW4Buffer(const Tensor *input, Tensor *output, cl::Kernel
     for (size_t i = 0; i < lws.size(); ++i) {
         roundUpGroupWorkSize[i] = ROUND_UP(in_gws[i], lws[i]);
     }
+    // printf("global work size(%d, %d)\nlocal work size(%d, %d)\n", 
+    //       roundUpGroupWorkSize[0], roundUpGroupWorkSize[1],
+    //       lws[0], lws[1]);
     error = runtime->commandQueue().enqueueNDRangeKernel(imageToBufferKernel, cl::NullRange,
                                                          cl::NDRange(roundUpGroupWorkSize[0], roundUpGroupWorkSize[1]),
                                                          cl::NDRange(lws[0], lws[1]), nullptr, &event);
diff --git a/source/backend/opencl/core/ImageBufferConvertor.hpp b/source/backend/opencl/core/ImageBufferConvertor.hpp
index 47de6a6..2516357 100644
--- a/source/backend/opencl/core/ImageBufferConvertor.hpp
+++ b/source/backend/opencl/core/ImageBufferConvertor.hpp
@@ -15,6 +15,14 @@
 
 namespace MNN {
 namespace OpenCL {
+
+bool convertNC4HW4HostToImage(const Tensor *input, const Tensor *output, OpenCLRuntime *runtime, bool needWait, bool isFloat16);
+
+bool convertImageToNC4HW4Host(const Tensor *input, const Tensor *output, OpenCLRuntime *runtime, bool needWait, bool isFloat16);
+
+bool convertNHWCHostToImage(const Tensor *input, const Tensor *output, OpenCLRuntime *runtime, bool needWait, bool isFloat16);
+
+bool convertImageToNHWCHost(const Tensor *input, const Tensor *output, OpenCLRuntime *runtime, bool needWait, bool isFloat16);
 /**
  * @brief convert nchw buffer to image.
  * @param input      input tensor.
diff --git a/source/backend/opencl/core/ImageHostConvertor.hpp b/source/backend/opencl/core/ImageHostConvertor.hpp
new file mode 100644
index 0000000..80707bb
--- /dev/null
+++ b/source/backend/opencl/core/ImageHostConvertor.hpp
@@ -0,0 +1,222 @@
+
+#ifndef ImageHostConvertor_hpp
+#define ImageHostConvertor_hpp
+
+#include <algorithm>
+#include <assert.h>
+#include <omp.h>
+
+namespace MNN {
+namespace OpenCL {
+
+const int maxThreadNumber = 1;
+
+template<typename T>
+void nhwc_to_image(const int c, const int h, const int w, 
+                        const float* input, T* output, const size_t row_pitch, const int n = 1){
+    // assert(c%4==0);
+    int shift_size = 4;
+    if(sizeof(T)==2){
+        shift_size = 3;
+    }
+    
+    const int c_4 = c / 4;
+    const int channel_remain = c % 4;
+    
+    const int threadNumber = std::max(std::min(maxThreadNumber, h), 1);
+    const int round_up_c = ROUND_UP(c, 4);
+    const int floor_down_c = FLOOR_DOWN(c, 4);
+    const int src_plane = w * c;
+    const int dst_plane = w * round_up_c;
+    
+    #pragma omp parallel for num_threads(threadNumber)
+    for(int h_idx = 0; h_idx < h; ++h_idx){
+        int src_h_idx_offset = h_idx * src_plane;
+        int dst_h_idx_offset = h_idx * row_pitch / sizeof(T);
+        for(int w_idx = 0; w_idx < w; ++w_idx){
+            for(int c_idx=0; c_idx < c_4 ;c_idx+=1){
+                int src_offset = (src_h_idx_offset + w_idx * c + c_idx * 4);
+                int dst_offset = dst_h_idx_offset + (c_idx * w + w_idx) * 4;
+                float* input_ptr = (float*)(input + src_offset);
+                auto output_ptr = output + dst_offset;
+                *output_ptr = (T)*(input_ptr);
+                *(output_ptr + 1) = (T)*(input_ptr+1);
+                *(output_ptr + 2) = (T)*(input_ptr+2);
+                *(output_ptr + 3) = (T)*(input_ptr+3);
+            }
+            if(channel_remain > 0){
+                int src_offset = (src_h_idx_offset + w_idx * c + floor_down_c);
+                int dst_offset = dst_h_idx_offset + floor_down_c * w + w_idx * 4;
+                float* input_ptr = (float*)(input + src_offset);
+                auto output_ptr = output + dst_offset;
+                if(channel_remain == 1){
+                    *output_ptr = (T)*(input_ptr);
+                    *(output_ptr + 1) = 0;
+                    *(output_ptr + 2) = 0;
+                    *(output_ptr + 3) = 0;
+                }else if(channel_remain == 2){
+                    *output_ptr = (T)*(input_ptr);
+                    *(output_ptr + 1) = (T)*(input_ptr+1);
+                    *(output_ptr + 2) = 0;
+                    *(output_ptr + 3) = 0;
+                }else if(channel_remain == 3){
+                    *output_ptr = (T)*(input_ptr);
+                    *(output_ptr + 1) = (T)*(input_ptr+1);
+                    *(output_ptr + 2) = (T)*(input_ptr+2);
+                    *(output_ptr + 3) = 0;
+                }
+            }
+        }
+    }
+}
+
+template<typename T>
+void image_to_nhwc(const int c, const int h, const int w, 
+                        const T* input, float* output, const size_t row_pitch, const int n = 1){
+    // assert(c%4==0);
+    int shift_size = 4;
+    if(sizeof(T)==2){
+        shift_size = 3;
+    }
+    
+    const int c_4 = c / 4;
+    const int channel_remain = c % 4;
+    
+    const int threadNumber = std::max(std::min(maxThreadNumber, h), 1);
+    const int round_up_c = ROUND_UP(c, 4);
+    const int floor_down_c = FLOOR_DOWN(c, 4);
+    assert(row_pitch % sizeof(T) == 0);
+    const int src_plane = row_pitch / sizeof(T);
+    const int dst_plane = w * c;
+    
+    #pragma omp parallel for num_threads(threadNumber)
+    for(int h_idx = 0; h_idx < h; ++h_idx){
+        int src_h_idx_offset = h_idx * src_plane;
+        int dst_h_idx_offset = h_idx * dst_plane;
+        for(int c_idx=0; c_idx < c_4 ;c_idx+=1){
+            for(int w_idx = 0; w_idx < w; ++w_idx) {
+                int src_offset = src_h_idx_offset + (c_idx * w + w_idx) * 4;
+                int dst_offset = (dst_h_idx_offset + w_idx * c + c_idx * 4);
+                auto* input_ptr = (T*)(input + src_offset);
+                float* output_ptr = output + dst_offset;
+                *output_ptr = (T)*(input_ptr);
+                *(output_ptr + 1) = (T)*(input_ptr+1);
+                *(output_ptr + 2) = (T)*(input_ptr+2);
+                *(output_ptr + 3) = (T)*(input_ptr+3);
+            }
+        }
+        if(channel_remain > 0){
+            for(int w_idx = 0; w_idx < w; ++w_idx){
+                int src_offset = (src_h_idx_offset + floor_down_c * w + w_idx * 4);
+                int dst_offset = dst_h_idx_offset + w_idx * c + floor_down_c;
+                auto input_ptr = (T*)(input + src_offset);
+                auto output_ptr = output + dst_offset;
+                if(channel_remain == 1){
+                    *output_ptr = (float)*(input_ptr);
+                }else if(channel_remain == 2){
+                    *output_ptr = (float)*(input_ptr);
+                    *(output_ptr + 1) = (float)*(input_ptr+1);
+                }else if(channel_remain == 3){
+                    *output_ptr = (float)*(input_ptr);
+                    *(output_ptr + 1) = (float)*(input_ptr+1);
+                    *(output_ptr + 2) = (float)*(input_ptr+2);
+                }
+            }
+        }
+    }
+}
+
+
+bool c4hw4_to_image_base(const int c, const int h, const int w, 
+                        const float* input, const float* output){
+    const int c_4 = UP_DIV(c, 4);
+    const int threadNumber = std::max(std::min(maxThreadNumber, c_4), 1);
+    #pragma omp parallel for num_threads(threadNumber)
+    for(int c_idx = 0; c_idx < c_4; ++c_idx){
+        int base_offset = c_idx * h * w;
+        for(int h_idx = 0; h_idx < h; ++h_idx){
+            for(int w_idx = 0; w_idx < w; w_idx += 1){
+                int offset_input = (base_offset + (h_idx * w + w_idx)) << 4;
+                float* input_ptr = (float*)((uintptr_t)input + offset_input);
+                int offset_output = (base_offset + (w_idx * h + h_idx)) << 4;
+                float* output_ptr = (float*)((uintptr_t)output + offset_output);
+                *output_ptr = *input_ptr;
+                *(output_ptr+1) = *(input_ptr+1);
+                *(output_ptr+2) = *(input_ptr+2);
+                *(output_ptr+3) = *(input_ptr+3);
+            }
+        }
+    }
+    return true;
+}
+template<typename T>
+bool c4hw4_to_image(const int c, const int h, const int w, 
+                        const float* input, const T* output, const size_t opencl_row_pitch){
+    const int c_4 = UP_DIV(c, 4);
+    const int opencl_row_size = opencl_row_pitch / sizeof(float);
+    int output_shift_size = 4; // sizeof(float) * 4 OpenCL elements(RGBA) = 2^4 = 16
+    if(sizeof(T) == 2){
+        output_shift_size = 3;
+    }
+
+    const int threadNumber = std::max(std::min(maxThreadNumber, c_4), 1);
+    #pragma omp parallel for num_threads(threadNumber)
+    for(int c_idx = 0; c_idx < c_4; ++c_idx){
+        int base_offset = c_idx * h * w;
+        int output_c_idx_offset = c_idx * w;
+        for(int h_idx = 0; h_idx < h; ++h_idx){
+            int output_h_idx_offset =  h_idx * opencl_row_pitch;
+            for(int w_idx = 0; w_idx < w; w_idx += 1){
+                int offset_input = (base_offset + (h_idx * w + w_idx)) << 4;
+                float* input_ptr = (float*)((uintptr_t)input + offset_input);
+                int offset_output = output_h_idx_offset + ((output_c_idx_offset + w_idx) << output_shift_size);
+                // int offset_output = ((h_idx * c + c_idx) * w + w_idx) << 4;
+                auto output_ptr = (T*)((uintptr_t)output + offset_output);
+                #ifdef __ANDROID__
+                *output_ptr = (T)*input_ptr;
+                *(output_ptr+1) = (T)*(input_ptr+1);
+                *(output_ptr+2) = (T)*(input_ptr+2);
+                *(output_ptr+3) = (T)*(input_ptr+3);
+                #endif
+            }
+        }
+    }
+    return true;
+}
+
+template<typename T>
+bool image_to_nc4hw4(const int c, const int h, const int w, 
+                        const T* input, const float* output, const size_t opencl_row_pitch){
+    const int c_4 = UP_DIV(c, 4);
+    int shift_size = 2 * 2; // sizeof(float) * 4 OpenCL elements(RGBA) = 2^4 = 16
+    if(sizeof(T) == 2){
+        shift_size = 3;
+    }
+    const int ori_image_size = h * w;
+    const int threadNumber = std::max(std::min(maxThreadNumber, c_4), 1);
+    #pragma omp parallel for num_threads(threadNumber)
+    for(int h_idx = 0; h_idx < h; ++h_idx){
+        uintptr_t input_h_idx_offset = h_idx * opencl_row_pitch;
+        uintptr_t output_h_idx_w = h_idx * w;
+        for(int c_idx = 0; c_idx < c_4; ++c_idx){
+            uintptr_t input_c_idx_offset = c_idx * w;
+            uintptr_t output_c_idx_offset = c_idx * ori_image_size;
+            for(int w_idx = 0; w_idx < w; w_idx += 1){
+                uintptr_t input_offset = input_h_idx_offset + ((input_c_idx_offset + w_idx) << shift_size);
+                uintptr_t output_offset = (output_c_idx_offset + output_h_idx_w + w_idx) << 4;
+                auto input_ptr = (T*)((uintptr_t)input + input_offset);
+                float* output_ptr = (float*)((uintptr_t)output + output_offset);
+                *output_ptr = (float)(*input_ptr);
+                *(output_ptr+1) = (float)(*(input_ptr+1));
+                *(output_ptr+2) = (float)(*(input_ptr+2));
+                *(output_ptr+3) = (float)(*(input_ptr+3));
+            }
+        }
+    }
+    return true;
+}
+
+}
+}
+
+#endif
\ No newline at end of file
diff --git a/source/backend/opencl/core/ImagePool.cpp b/source/backend/opencl/core/ImagePool.cpp
index 8a06027..978c4e8 100644
--- a/source/backend/opencl/core/ImagePool.cpp
+++ b/source/backend/opencl/core/ImagePool.cpp
@@ -33,7 +33,8 @@ cl::Image* ImagePool::alloc(int w, int h, bool seperate) {
     node->w = w;
     node->h = h;
     node->image.reset(
-        new cl::Image2D(mContext, CL_MEM_READ_WRITE, cl::ImageFormat(CL_RGBA, mType), w, h, 0, nullptr, nullptr));
+        new cl::Image2D(mContext, CL_MEM_READ_WRITE | CL_MEM_ALLOC_HOST_PTR, cl::ImageFormat(CL_RGBA, mType), w, h, 0, nullptr, nullptr));
+    // MNN_PRINT("%d %d new image\n", w, h);
     if (nullptr == node->image) {
         MNN_ERROR("All Image %d x %d error \n", w, h);
         return nullptr;
diff --git a/source/backend/opencl/core/OpenCLBackend.cpp b/source/backend/opencl/core/OpenCLBackend.cpp
index 9cc0c13..4b891b3 100644
--- a/source/backend/opencl/core/OpenCLBackend.cpp
+++ b/source/backend/opencl/core/OpenCLBackend.cpp
@@ -5,18 +5,23 @@
 //  Created by MNN on 2019/02/28.
 //  Copyright  2018, Alibaba Group Holding Limited
 //
+#include <map>
+#include <mutex>
+#include <thread>
+#include <sstream>
+#include <cstdlib>
+#include <cstring>
 
 #include "backend/opencl/core/OpenCLBackend.hpp"
 #include "MNN_generated.h"
-
+#include "core/MUtils.hpp"
 #include "core/TensorUtils.hpp"
 #include "core/SizeComputer.hpp"
-#include <map>
-#include <mutex>
-#include <thread>
 #include "core/Macro.h"
 
 namespace MNN {
+
+
 namespace OpenCL {
 
 std::map<OpType, OpenCLBackend::Creator*>* gCreator() {
@@ -55,6 +60,15 @@ OpenCLBackend::OpenCLBackend(BackendConfig::PrecisionMode precision, BackendConf
         mImageToNC4HW4BufferFloat = mOpenCLRuntime->buildKernel("buffer_to_image", "image_to_nc4hw4_buffer", buildOptions);
         mImageToNHWCBufferFloat = mOpenCLRuntime->buildKernel("buffer_to_image", "image_to_nhwc_buffer", buildOptions);
         mImageToNCHWBufferFloat = mOpenCLRuntime->buildKernel("buffer_to_image", "image_to_nchw_buffer", buildOptions);
+        mPrintBuffer = mOpenCLRuntime->buildKernel("buffer_to_image", "print_float_buffer", buildOptions);
+        mPrintImageKernel = mOpenCLRuntime->buildKernel("buffer_to_image", "print_image", buildOptions);
+    }
+
+    const char* p_env = std::getenv("MNN_PROFILE_DATA_TRANS");
+    if(p_env !=nullptr && strcmp("true", p_env) == 0){
+        mProfileDataTrans = true;
+    }else{
+        mProfileDataTrans = false;
     }
 }
 
@@ -68,6 +82,24 @@ OpenCLRuntime* OpenCLBackend::getOpenCLRuntime() {
     return mOpenCLRuntime.get();
 }
 
+bool printImage(const Tensor *input, cl::Kernel printImageKernel, OpenCLRuntime* runtime) {
+    const int imageWidth = UP_DIV(input->channel(), 4) * input->width();
+    const int imageHeight = input->batch() * input->height();
+    if(printImageKernel.get() == nullptr){
+        std::set<std::string> buildOptions;
+        printImageKernel = runtime->buildKernel("buffer_to_image", "print_image", buildOptions);
+    }
+    uint32_t idx = 0;
+    printImageKernel.setArg(idx++, imageWidth);
+    printImageKernel.setArg(idx++, imageHeight);
+    printImageKernel.setArg(idx++, openCLImage(input));
+    cl_int error = runtime->commandQueue().enqueueNDRangeKernel(printImageKernel, cl::NDRange(0, 0), 
+                                                cl::NDRange(imageWidth, imageHeight));
+    runtime->commandQueue().finish();
+    return error == CL_SUCCESS;
+}
+
+
 bool OpenCLBackend::onAcquireBuffer(const Tensor* nativeTensor, StorageType storageType) {
 #ifdef LOG_VERBOSE
     MNN_PRINT("Start OpenCLBackend::onAcquireBuffer !\n");
@@ -101,7 +133,7 @@ bool OpenCLBackend::onAcquireBuffer(const Tensor* nativeTensor, StorageType stor
 
     size_t imageWidth  = (size_t)UP_DIV(C, 4) * W;
     size_t imageHeight = (size_t)N * H;
-
+    
     const std::vector<size_t> requestShape{imageWidth, imageHeight};
 #ifdef LOG_VERBOSE
     MNN_PRINT("OpenCLBackend::onAcquireBuffer: [%d, %d, %d, %d], [%d, %d]\n", N, H, W, C, (int)imageWidth,
@@ -152,6 +184,7 @@ bool OpenCLBackend::onClearBuffer() {
     mBufferPoolInt8->clear();
     return true;
 }
+
 std::pair<float, bool> OpenCLBackend::onMeasure(const std::vector<Tensor*>& inputs, const std::vector<Tensor*>& outputs, const MNN::Op* op) {
     auto creators = gCreator();
     auto iter      = creators->find(op->type());
@@ -164,6 +197,14 @@ std::pair<float, bool> OpenCLBackend::onMeasure(const std::vector<Tensor*>& inpu
     auto computeFlops = mOpenCLRuntime->flops();
     return std::make_pair(defaultScheduleTime + flops / 1024.0f / computeFlops * 1000.0f, true);
 }
+
+bool OpenCLBackend::isSupportOpType(const MNN::Op* op){
+    auto map  = gCreator();
+    auto iter = map->find(op->type());
+    return (iter != map->end());
+}
+
+
 Execution* OpenCLBackend::onCreate(const std::vector<Tensor*>& inputs, const std::vector<Tensor*>& outputs,
                                    const MNN::Op* op) {
 #ifdef LOG_VERBOSE
@@ -197,7 +238,7 @@ Execution* OpenCLBackend::onCreate(const std::vector<Tensor*>& inputs, const std
     if (!valid) {
         return NULL;
     }
-
+    
     auto exe = iter->second->onCreate(inputs, outputs, op, this);
     if (NULL == exe) {
         MNN_PRINT("The Creator Don't support type %d, %s\n", op->type(), op->name()->c_str());
@@ -220,18 +261,63 @@ bool OpenCLBackend::onWaitFinish() {
     return rc == 0;
 }
 
+constexpr char OpenCLBackend::mLayoutTransferEnv[];
+
+bool OpenCLBackend::isCPUConverter() const{
+    const char* env_p = std::getenv(mLayoutTransferEnv);
+    if(env_p != nullptr && strcmp(env_p, "cpu") == 0){
+        // MNN_PRINT("*** Use cpu layout converter\n ***");
+        return true;
+    }return false;
+}
+
 bool OpenCLBackend::isCreateError() const {
     return mIsCreateError;
 }
 
-void OpenCLBackend::_allocHostBuffer(int length) const {
+void OpenCLBackend::printFloatBuffer(const Tensor* srcTensor) const {
+    const int needSize = srcTensor->size();
+    assert(needSize > 0);
+    uint32_t idx = 0;
+    
+    (*const_cast<cl::Kernel*>(&mPrintBuffer)).setArg(idx++, srcTensor->size());
+    (*const_cast<cl::Kernel*>(&mPrintBuffer)).setArg(idx++,  openCLBuffer(srcTensor));
+    cl_int error;
+    error = this->mOpenCLRuntime->commandQueue().enqueueNDRangeKernel((*const_cast<cl::Kernel*>(&mPrintBuffer)), cl::NullRange, 
+                                                          cl::NDRange(needSize), cl::NullRange);
+    MNN_CHECK_CL_SUCCESS(error);
+    this->mOpenCLRuntime->commandQueue().flush();
+    this->mOpenCLRuntime->commandQueue().finish();
+}
+
+// void OpenCLBackend::_allocHostBuffer(int length) const {
+//     MNN_ASSERT(length > 0);
+//     if (nullptr != mHostBuffer.second && length <= mHostBuffer.first) {
+//         return;
+//     }
+//     mHostBuffer.first = length;
+//     mHostBuffer.second.reset(
+//         new cl::Buffer(mOpenCLRuntime->context(), CL_MEM_READ, length));
+// }
+
+
+void OpenCLBackend::_allocHostBuffer(int length, void* hostPtr) const {
     MNN_ASSERT(length > 0);
     if (nullptr != mHostBuffer.second && length <= mHostBuffer.first) {
         return;
     }
     mHostBuffer.first = length;
-    mHostBuffer.second.reset(
-        new cl::Buffer(mOpenCLRuntime->context(), CL_MEM_READ_WRITE | CL_MEM_ALLOC_HOST_PTR, length));
+    if(hostPtr != NULL){
+        mHostBuffer.second.reset(
+            new cl::Buffer(mOpenCLRuntime->context(), CL_MEM_READ_WRITE | CL_MEM_USE_HOST_PTR, length, hostPtr, 0,0,0,0));
+    }else{
+        mHostBuffer.second.reset(
+            new cl::Buffer(mOpenCLRuntime->context(), CL_MEM_READ_WRITE, length));
+    }
+}
+
+void OpenCLBackend::_allocCPUBuffer(int length) {
+    
 }
 
 void OpenCLBackend::copyFromDeviceInt8(const Tensor* srcTensor, const Tensor* dstTensor) const{
@@ -251,6 +337,20 @@ void OpenCLBackend::copyToDeviceInt8(const Tensor* srcTensor, const Tensor* dstT
 }
 
 void OpenCLBackend::copyFromDevice(const Tensor* srcTensor, const Tensor* dstTensor) const{
+    if(isCPUConverter()){
+        MNN_DATA_FORMAT data_format = TensorUtils::getDescribe(srcTensor)->dimensionFormat;
+        if (MNN_DATA_FORMAT_NHWC == data_format) {
+            OpenCL::convertImageToNHWCHost(srcTensor, dstTensor, mOpenCLRuntime.get(), true, mPrecision == BackendConfig::Precision_Low);
+            return;
+        }
+        if (MNN_DATA_FORMAT_NCHW == data_format) {
+            
+        }
+        if (MNN_DATA_FORMAT_NC4HW4 == data_format) {
+            OpenCL::convertImageToNC4HW4Host(srcTensor, dstTensor, mOpenCLRuntime.get(), true, mPrecision == BackendConfig::Precision_Low);
+            return;
+        }
+    }else{
     std::vector<int> bufferShape = MNN::OpenCL::tensorShapeFormat(srcTensor);
     MNN::Tensor interBuffer(0, Tensor::TENSORFLOW);
     interBuffer.buffer().dimensions = bufferShape.size();
@@ -258,66 +358,137 @@ void OpenCLBackend::copyFromDevice(const Tensor* srcTensor, const Tensor* dstTen
         interBuffer.buffer().dim[i].extent = bufferShape.at(i);
     }
     auto needSize = dstTensor->size();
+    auto hostPtr = dstTensor->host<float>();
+
+    // 1. Allocate with hostPtr
+    // _allocHostBuffer(needSize, hostPtr);
     _allocHostBuffer(needSize);
     interBuffer.buffer().device = (uint64_t)mHostBuffer.second.get();
+    // 2. 
+    // mOpenCLRuntime->commandQueue().enqueueMapBuffer(*mHostBuffer.second, CL_TRUE, 
+    //                                                     CL_MEM_READ_WRITE | CL_MEM_USE_HOST_PTR , 0, needSize);
 
     MNN_DATA_FORMAT data_format = TensorUtils::getDescribe(dstTensor)->dimensionFormat;
     switch (data_format) {
         case MNN_DATA_FORMAT_NHWC:
+            //MNN_PRINT("OpenCL::convertImageToNHWCBuffer\n");
             OpenCL::convertImageToNHWCBuffer(srcTensor, &interBuffer,
                                              *const_cast<cl::Kernel*>(&mImageToNHWCBufferFloat), mOpenCLRuntime.get());
             break;
         case MNN_DATA_FORMAT_NCHW:
+            //MNN_PRINT("OpenCL::convertImageToNCHWBuffer\n");
             OpenCL::convertImageToNCHWBuffer(srcTensor, &interBuffer,
                                              *const_cast<cl::Kernel*>(&mImageToNCHWBufferFloat), mOpenCLRuntime.get());
             break;
         case MNN_DATA_FORMAT_NC4HW4:
+            //MNN_PRINT("OpenCL::convertImageToNC4HW4Buffer\n");
             OpenCL::convertImageToNC4HW4Buffer(
                 srcTensor, &interBuffer, *const_cast<cl::Kernel*>(&mImageToNC4HW4BufferFloat), mOpenCLRuntime.get());
+            // printFloatBuffer(interBuffer);
             break;
         default:
             break;
     }
-    auto hostPtr = dstTensor->host<float>();
+    
     cl_int error                = CL_SUCCESS;
-
+    // 3. Unmap
+    // mOpenCLRuntime->commandQueue().enqueueUnmapMemObject(*mHostBuffer.second, hostPtr);
+    // mOpenCLRuntime->commandQueue().finish();
     mOpenCLRuntime->commandQueue().enqueueReadBuffer(*mHostBuffer.second, CL_TRUE, 0, needSize, hostPtr);
+    }
 }
+
+
+
 void OpenCLBackend::copyToDevice(const Tensor* srcTensor, const Tensor* dstTensor) const{
-    std::vector<int> bufferShape = MNN::OpenCL::tensorShapeFormat(srcTensor);
-    MNN::Tensor interBuffer(0, Tensor::TENSORFLOW);
-    interBuffer.buffer().dimensions = bufferShape.size();
-    for (int i = 0; i < bufferShape.size(); i++) {
-        interBuffer.buffer().dim[i].extent = bufferShape.at(i);
-    }
-    auto needSize = srcTensor->size();
-    _allocHostBuffer(needSize);
-    interBuffer.buffer().device = (uint64_t)mHostBuffer.second.get();
-    auto hostPtr                = srcTensor->host<float>();
-    cl_int error                = CL_SUCCESS;
-    mOpenCLRuntime->commandQueue().enqueueWriteBuffer(*mHostBuffer.second, CL_TRUE, 0, needSize, hostPtr);
-    // Host -> OpenCL
-    MNN_DATA_FORMAT data_format = TensorUtils::getDescribe(srcTensor)->dimensionFormat;
-    if (MNN_DATA_FORMAT_NHWC == data_format) {
-        OpenCL::convertNHWCBufferToImage(&interBuffer, const_cast<Tensor*>(dstTensor),
-                                         *const_cast<cl::Kernel*>(&mNHWCBufferToImageFloat), mOpenCLRuntime.get());
-        return;
-    }
-    if (MNN_DATA_FORMAT_NCHW == data_format) {
-        OpenCL::convertNCHWBufferToImage(&interBuffer, const_cast<Tensor*>(dstTensor),
-                                         *const_cast<cl::Kernel*>(&mNCHWBufferToImageFloat), mOpenCLRuntime.get());
-        return;
+    if(isCPUConverter()){
+        MNN_DATA_FORMAT data_format = TensorUtils::getDescribe(srcTensor)->dimensionFormat;
+        if (MNN_DATA_FORMAT_NHWC == data_format) {
+            OpenCL::convertNHWCHostToImage(srcTensor, dstTensor, mOpenCLRuntime.get(), true, mPrecision==BackendConfig::Precision_Low);
+            // printImage(dstTensor, mPrintImageKernel, mOpenCLRuntime.get());
+            return;
+        }
+        if (MNN_DATA_FORMAT_NCHW == data_format) {
+            
+        }
+        if (MNN_DATA_FORMAT_NC4HW4 == data_format) {
+            OpenCL::convertNC4HW4HostToImage(srcTensor, dstTensor, mOpenCLRuntime.get(), true, mPrecision==BackendConfig::Precision_Low);
+            // printImage(dstTensor, mPrintImageKernel, mOpenCLRuntime.get());
+            return;
+        }
     }
-    if (MNN_DATA_FORMAT_NC4HW4 == data_format) {
-        OpenCL::convertNC4HW4BufferToImage(&interBuffer, const_cast<Tensor*>(dstTensor),
-                                           *const_cast<cl::Kernel*>(&mNC4HW4BufferToImageFloat),
-                                           mOpenCLRuntime.get());
-        return;
+    else{
+        std::vector<int> bufferShape = MNN::OpenCL::tensorShapeFormat(srcTensor);
+        MNN::Tensor interBuffer(0, Tensor::TENSORFLOW);
+        interBuffer.buffer().dimensions = bufferShape.size();
+        for (int i = 0; i < bufferShape.size(); i++) {
+            interBuffer.buffer().dim[i].extent = bufferShape.at(i);
+        }
+
+        long startCopy = 0, endCopy = 0, startConvert = 0, endConvert = 0;
+        // startCopy = NowMicros();
+        auto needSize = srcTensor->size();
+        auto hostPtr                = srcTensor->host<float>();
+        
+        // printf("In copyToDevice %lx shape:[%d %d %d %d]\n", dstTensor->deviceId(), 
+        //         srcTensor->batch(), srcTensor->channel(), srcTensor->height(), srcTensor->width());
+        // MNN::OpenCL::print_tensor(srcTensor->channel(), srcTensor->height(), srcTensor->width(), (float*)(srcTensor->buffer().host));
+        // 1. Allocate
+        _allocHostBuffer(needSize);
+        // _allocHostBuffer(needSize, hostPtr);
+        // MNN_PRINT("In OpenCLBackend _allocHostBuffer\n");
+        // mHostBuffer is instance of cl::Buffer
+        interBuffer.buffer().device = (uint64_t)mHostBuffer.second.get();
+        
+        cl_int error                = CL_SUCCESS;
+        // 2. Map or enqueue
+        mOpenCLRuntime->commandQueue().enqueueWriteBuffer(*mHostBuffer.second, CL_TRUE, 0, needSize, hostPtr);
+        // mOpenCLRuntime->commandQueue().enqueueMapBuffer(*mHostBuffer.second, CL_TRUE, 
+        //                                                    CL_MEM_READ_WRITE | CL_MEM_USE_HOST_PTR , 0, needSize);
+        // MNN_PRINT("In OpenCLBackend enqueueMapBuffer\n");
+        // mOpenCLRuntime->commandQueue().finish();
+        // endCopy = NowMicros();
+        // MNN_PRINT("copy %lu ", (endCopy-startCopy));
+        // Host -> OpenCL
+        MNN_DATA_FORMAT data_format = TensorUtils::getDescribe(srcTensor)->dimensionFormat;
+        if (MNN_DATA_FORMAT_NHWC == data_format) {
+            // MNN_PRINT("OpenCL::convertNHWCBufferToImage\n");
+            OpenCL::convertNHWCBufferToImage(&interBuffer, const_cast<Tensor*>(dstTensor),
+                                            *const_cast<cl::Kernel*>(&mNHWCBufferToImageFloat), mOpenCLRuntime.get());
+            mOpenCLRuntime->commandQueue().finish();
+            return;
+        }
+        if (MNN_DATA_FORMAT_NCHW == data_format) {
+            // MNN_PRINT("OpenCL::convertNCHWBufferToImage\n");
+            OpenCL::convertNCHWBufferToImage(&interBuffer, const_cast<Tensor*>(dstTensor),
+                                            *const_cast<cl::Kernel*>(&mNCHWBufferToImageFloat), mOpenCLRuntime.get());
+            mOpenCLRuntime->commandQueue().finish();
+            return;
+        }
+        if (MNN_DATA_FORMAT_NC4HW4 == data_format) {
+            // MNN_PRINT("OpenCL::convertNC4HW4BufferToImage\n");
+            OpenCL::convertNC4HW4BufferToImage(&interBuffer, const_cast<Tensor*>(dstTensor),
+                                            *const_cast<cl::Kernel*>(&mNC4HW4BufferToImageFloat),
+                                            mOpenCLRuntime.get());
+            mOpenCLRuntime->commandQueue().finish();
+            // mOpenCLRuntime->commandQueue().finish();
+            // printImage(dstTensor, mPrintImageKernel, mOpenCLRuntime.get());
+            // 3. Unmap
+            // mOpenCLRuntime->commandQueue().enqueueUnmapMemObject(*mHostBuffer.second, hostPtr);
+            // mOpenCLRuntime->commandQueue().finish();
+            // MNN_PRINT("In OpenCLBackend enqueueUnmapMemObject\n");
+            
+
+            // printFloatBuffer(srcTensor);
+            return;
+        }
     }
     MNN_ASSERT(false);
     return;
 }
 
+
+
 void OpenCLBackend::onCopyBuffer(const Tensor* srcTensor, const Tensor* dstTensor) const {
 #ifdef LOG_VERBOSE
     MNN_PRINT("Start onCopyBuffer !\n");
@@ -333,9 +504,42 @@ void OpenCLBackend::onCopyBuffer(const Tensor* srcTensor, const Tensor* dstTenso
         }
     }else{
         if (srcTensor->deviceId() == 0 && dstTensor->deviceId() != 0) {
+            auto start = MNN::NowMicros();
+            // srcTensor->print();
             copyToDevice(srcTensor, dstTensor);
-        }else if(srcTensor->deviceId() != 0 && dstTensor->deviceId() == 0){
+            mOpenCLRuntime->commandQueue().finish();
+            // printImage(dstTensor, mPrintImageKernel, mOpenCLRuntime.get());
+            if(mProfileDataTrans){
+                auto end = MNN::NowMicros();
+                if(srcTensor->shape().size() > 0){
+                    std::stringstream strStream;
+                    strStream << srcTensor->shape()[0];
+                    for(int i=1;i<srcTensor->shape().size(); ++i) {
+                        strStream << "," << srcTensor->shape()[i];
+                    }
+                    strStream << " copyToDevice " << end-start << " TensorConvert\n";
+                    MNN_PRINT("%s", strStream.str().c_str());
+                }
+            }
+            
+        } else if(srcTensor->deviceId() != 0 && dstTensor->deviceId() == 0) {
+            auto start = MNN::NowMicros();
+            // printImage(srcTensor, mPrintImageKernel, mOpenCLRuntime.get());
             copyFromDevice(srcTensor, dstTensor);
+            // dstTensor->print();
+            // mOpenCLRuntime->commandQueue().finish();
+            if(mProfileDataTrans){
+                auto end = MNN::NowMicros();
+                if(srcTensor->shape().size() > 0){
+                    std::stringstream strStream;
+                    strStream << srcTensor->shape()[0];
+                    for(int i=1;i<srcTensor->shape().size(); ++i) {
+                        strStream << "," << srcTensor->shape()[i];
+                    }
+                    strStream << " copyFromDevice " << end-start << " TensorConvert\n";
+                    MNN_PRINT("%s", strStream.str().c_str());
+                }
+            }
         }else{
             MNN_PRINT("onCopyBuffer float error !!! \n");
         }
diff --git a/source/backend/opencl/core/OpenCLBackend.hpp b/source/backend/opencl/core/OpenCLBackend.hpp
index 044acd6..52d663d 100644
--- a/source/backend/opencl/core/OpenCLBackend.hpp
+++ b/source/backend/opencl/core/OpenCLBackend.hpp
@@ -20,6 +20,7 @@
 #include "core/Macro.h"
 #include "backend/opencl/core/ImageBufferConvertor.hpp"
 #include "backend/opencl/core/OpenCLRunningUtils.hpp"
+#include "backend/opencl/core/TransformProfiler.hpp"
 
 namespace MNN {
 namespace OpenCL {
@@ -71,7 +72,7 @@ public:
     virtual bool onReleaseBuffer(const Tensor *nativeTensor, StorageType storageType) override;
     virtual bool onAllocateBuffer() override;
     virtual bool onClearBuffer() override;
-
+    virtual bool isSupportOpType(const MNN::Op* op) override;
     virtual Execution *onCreate(const std::vector<Tensor *> &inputs, const std::vector<Tensor *> &outputs,
                                 const MNN::Op *op) override;
 
@@ -101,13 +102,18 @@ public:
 
     bool isCreateError() const;
 
+    bool isCPUConverter() const;
+
 private:
     void copyFromDevice(const Tensor* srcTensor, const Tensor* dstTensor) const;
     void copyToDevice(const Tensor* srcTensor, const Tensor* dstTensor) const;
+    void mapToDevice(const Tensor* srcTensor, const Tensor* dstTensor) const;
     void copyFromDeviceInt8(const Tensor* srcTensor, const Tensor* dstTensor) const;
     void copyToDeviceInt8(const Tensor* srcTensor, const Tensor* dstTensor) const;
+    void printFloatBuffer(const Tensor* srcTensor) const;
 
-    void _allocHostBuffer(int length) const;
+    void _allocHostBuffer(int length, void* hostPtr=NULL) const;
+    void _allocCPUBuffer(int length);
     cl::Kernel mImageToNCHWBufferFloat;
     cl::Kernel mImageToNC4HW4BufferFloat;
     cl::Kernel mImageToNHWCBufferFloat;
@@ -115,6 +121,10 @@ private:
     cl::Kernel mNCHWBufferToImageFloat;
     cl::Kernel mNHWCBufferToImageFloat;
     cl::Kernel mNHWCBufferToImageInt8;
+    cl::Kernel mPrintBuffer;
+    cl::Kernel mPrintImageKernel;
+    bool mProfileDataTrans;
+    
     std::shared_ptr<ImagePool> mImagePool;
     std::shared_ptr<ImagePool> mStaticImagePool;
     std::shared_ptr<BufferPool> mBufferPool;
@@ -125,6 +135,7 @@ private:
     mutable std::pair<int, std::shared_ptr<SharedBuffer>> mSharedBuffer;
     BackendConfig::PrecisionMode mPrecision;
     bool mIsCreateError{false};
+    static constexpr char mLayoutTransferEnv[] = "MNN_LAYOUT_CONVERTER";
 };
 
 template <class T>
diff --git a/source/backend/opencl/core/OpenCLRunningUtils.cpp b/source/backend/opencl/core/OpenCLRunningUtils.cpp
index 5fb6ffc..a732016 100644
--- a/source/backend/opencl/core/OpenCLRunningUtils.cpp
+++ b/source/backend/opencl/core/OpenCLRunningUtils.cpp
@@ -268,5 +268,19 @@ void copyBufferToImage(OpenCLRuntime *runtime, const cl::Buffer &buffer, const c
     comandQueue.enqueueNDRangeKernel(kernel, cl::NullRange, cl::NDRange(w, h, 1));
 }
 
+void print_tensor(const int c, const int h, const int w, 
+                        const float* input){
+    const int c_4 = UP_DIV(c, 4);
+    for(int c_idx = 0; c_idx < c_4; ++c_idx){
+        int base_offset = c_idx * h * w;
+        for(int h_idx = 0; h_idx < h; ++h_idx){
+            for(int w_idx = 0; w_idx < w; w_idx += 1){
+                int offset_input = (base_offset + (h_idx * w + w_idx)) << 4;
+                float* input_ptr = (float*)((uintptr_t)input + offset_input);
+                printf("%3.0f %3.0f %3.0f %3.0f ", *(input_ptr), *(input_ptr+1), *(input_ptr+2), *(input_ptr+3) );
+            }printf("\n");
+        }printf("-----------------\n");
+    }
+}
 } // namespace OpenCL
 } // namespace MNN
diff --git a/source/backend/opencl/core/OpenCLRunningUtils.hpp b/source/backend/opencl/core/OpenCLRunningUtils.hpp
index dfb984e..b5a114c 100644
--- a/source/backend/opencl/core/OpenCLRunningUtils.hpp
+++ b/source/backend/opencl/core/OpenCLRunningUtils.hpp
@@ -102,7 +102,7 @@ void runTurnKernelLWS2D(const ::cl::Kernel &kernel, const std::vector<uint32_t>
 std::vector<uint32_t> localWS3DDefault(const std::vector<uint32_t> &gws, const uint32_t maxWorkGroupSize,
                                        OpenCLRuntime *runtime);
 void copyBufferToImage(OpenCLRuntime *runtime, const cl::Buffer &buffer, const cl::Image &image, int w, int h);
-
+void print_tensor(const int c, const int h, const int w, const float* input);
 } // namespace OpenCL
 } // namespace MNN
 #endif  /* OpenCLRunningUtils_hpp */
diff --git a/source/backend/opencl/core/TransformProfiler.hpp b/source/backend/opencl/core/TransformProfiler.hpp
new file mode 100644
index 0000000..4bb7b48
--- /dev/null
+++ b/source/backend/opencl/core/TransformProfiler.hpp
@@ -0,0 +1,14 @@
+#ifndef TRANSFORM_PROFILER
+#include <MNN/Tensor.hpp>
+
+struct TensorTransformProfiler
+{
+    /* data */
+    MNN::Tensor * srcTensor;
+    long latency;
+    unsigned long startTimeStamp;
+    unsigned long endTimeStamp;
+    int type;
+};
+
+#endif
\ No newline at end of file
diff --git a/source/backend/opencl/core/runtime/OpenCLRuntime.cpp b/source/backend/opencl/core/runtime/OpenCLRuntime.cpp
index 2d575cf..a7aa1e6 100644
--- a/source/backend/opencl/core/runtime/OpenCLRuntime.cpp
+++ b/source/backend/opencl/core/runtime/OpenCLRuntime.cpp
@@ -213,7 +213,7 @@ bool OpenCLRuntime::buildProgram(const std::string &buildOptionsStr, cl::Program
     if (ret != CL_SUCCESS) {
         if (program->getBuildInfo<CL_PROGRAM_BUILD_STATUS>(*mFirstGPUDevicePtr) == CL_BUILD_ERROR) {
             std::string buildLog = program->getBuildInfo<CL_PROGRAM_BUILD_LOG>(*mFirstGPUDevicePtr);
-            MNN_PRINT("Program build log: %s \n", buildLog.c_str());
+            // MNN_PRINT("Program build log: %s \n", buildLog.c_str());
         }
         MNN_PRINT("Build program failed ! \n");
         return false;
@@ -251,6 +251,8 @@ cl::Kernel OpenCLRuntime::buildKernel(const std::string &programName, const std:
     cl_int err;
     cl::Kernel kernel = cl::Kernel(program, kernelName.c_str(), &err);
     MNN_CHECK_CL_SUCCESS(err);
+    //MNN_PRINT("build program %s, kernel %s\n", programName.c_str(), kernelName.c_str());
+    
     return kernel;
 }
 
diff --git a/source/backend/opencl/core/runtime/OpenCLWrapper.cpp b/source/backend/opencl/core/runtime/OpenCLWrapper.cpp
index 9646069..6ce7770 100644
--- a/source/backend/opencl/core/runtime/OpenCLWrapper.cpp
+++ b/source/backend/opencl/core/runtime/OpenCLWrapper.cpp
@@ -140,6 +140,13 @@ bool OpenCLSymbols::LoadLibraryFromPath(const std::string &library_path) {
 std::shared_ptr<OpenCLSymbols> OpenCLSymbolsOperator::gOpenclSymbols;
 
 OpenCLSymbols *OpenCLSymbolsOperator::getOpenclSymbolsPtr() {
+    if (gOpenclSymbols.get() == nullptr) {
+        gOpenclSymbols.reset(new OpenCLSymbols());
+        if (false == gOpenclSymbols->LoadOpenCLLibrary()) {
+            gOpenclSymbols.reset();
+        }
+        printf("In OpenCLWapper.cpp gOpenclSymbols is null\n");
+    }
     return gOpenclSymbols.get();
 }
 
diff --git a/source/backend/opencl/execution/ConvWinograd.cpp b/source/backend/opencl/execution/ConvWinograd.cpp
index 3cfef7a..97cfa52 100644
--- a/source/backend/opencl/execution/ConvWinograd.cpp
+++ b/source/backend/opencl/execution/ConvWinograd.cpp
@@ -326,7 +326,7 @@ ErrorCode ConvWinograd::onExecute(const std::vector<Tensor*>& inputs, const std:
             }
         }
     }
-
+    this->backend()->onWaitFinish();
     return NO_ERROR;
 }
 
diff --git a/source/backend/opencl/execution/DeconvExecution.cpp b/source/backend/opencl/execution/DeconvExecution.cpp
index f0d71ba..8a389cc 100644
--- a/source/backend/opencl/execution/DeconvExecution.cpp
+++ b/source/backend/opencl/execution/DeconvExecution.cpp
@@ -212,7 +212,7 @@ ErrorCode DeconvExecution::onExecute(const std::vector<Tensor *> &inputs, const
     MNN_PRINT("Start DeconvExecution onExecute... \n");
 #endif
     run3DKernelDefault(mKernel, mGWS, mLWS, mOpenCLBackend->getOpenCLRuntime());
-
+    mOpenCLBackend->onWaitFinish();
 #ifdef LOG_VERBOSE
     MNN_PRINT("End DeconvExecution onExecute... \n");
 #endif
diff --git a/source/backend/opencl/execution/DepthwiseConvExecution.cpp b/source/backend/opencl/execution/DepthwiseConvExecution.cpp
index 2e1af33..0ed9c5d 100644
--- a/source/backend/opencl/execution/DepthwiseConvExecution.cpp
+++ b/source/backend/opencl/execution/DepthwiseConvExecution.cpp
@@ -186,7 +186,7 @@ ErrorCode DepthwiseConvExecution::onExecute(const std::vector<Tensor *> &inputs,
 #endif
 
     runKernel2D(mKernel, mGlobalWorkSize, mLocalWorkSize, mOpenCLBackend->getOpenCLRuntime());
-
+    this->backend()->onWaitFinish();
 #ifdef LOG_VERBOSE
     MNN_PRINT("end DepthwiseConvExecution onExecute !\n");
 #endif
diff --git a/source/backend/opencl/execution/PoolExecution.cpp b/source/backend/opencl/execution/PoolExecution.cpp
index 80439cb..3f42bb0 100644
--- a/source/backend/opencl/execution/PoolExecution.cpp
+++ b/source/backend/opencl/execution/PoolExecution.cpp
@@ -176,7 +176,6 @@ ErrorCode PoolExecution::onExecute(const std::vector<Tensor *> &inputs, const st
     MNN_PRINT("start PoolExecution onExecute !\n");
 #endif
     run3DKernelDefault(mKernel, mGlobalWorkSize, mLocalWorkSize, mOpenCLBackend->getOpenCLRuntime());
-
 #ifdef LOG_VERBOSE
     MNN_PRINT("end PoolExecution onExecute !\n");
 #endif
diff --git a/source/backend/opencl/execution/ReshapeExecution.cpp b/source/backend/opencl/execution/ReshapeExecution.cpp
index 1fe6476..b7a8c16 100644
--- a/source/backend/opencl/execution/ReshapeExecution.cpp
+++ b/source/backend/opencl/execution/ReshapeExecution.cpp
@@ -47,6 +47,7 @@ ErrorCode ReshapeExecution::onResize(const std::vector<Tensor *> &inputs, const
         }else{
             mImageToBufferKernelname = formatMap[inputFormat];
         }
+        MNN_PRINT("mImageToBufferKernelname %s\n", mImageToBufferKernelname.c_str());
     }
 
     {
@@ -60,6 +61,7 @@ ErrorCode ReshapeExecution::onResize(const std::vector<Tensor *> &inputs, const
         }else{
             mBufferToImageKernelname = formatMap[outputFormat];
         }
+        MNN_PRINT("mBufferToImageKernelname %s\n", mBufferToImageKernelname.c_str());
     }
 
     if (mImageToBufferKernel.get() == nullptr) {
@@ -99,6 +101,8 @@ ErrorCode ReshapeExecution::onResize(const std::vector<Tensor *> &inputs, const
         for (size_t i = 0; i < mLocalWorkSize.size(); ++i) {
             mImageToBufferRoundUpGWS[i] = ROUND_UP(inputGlobalWorkSize[i], std::max((uint32_t)1, mLocalWorkSize[i]));
         }
+        // mLocalWorkSize is fixed to {16, maxWorkGroupSize / 16}
+        // Set globalWorksize to the inputGlobalWorkSize round up of mLocalWorkSize
     }
 
     // buffer->image
@@ -124,19 +128,25 @@ ErrorCode ReshapeExecution::onExecute(const std::vector<Tensor *> &inputs, const
     MNN_PRINT("start ReshapeExecution onExecute !\n");
 #endif
 
-    auto runtime = mOpenCLBackend->getOpenCLRuntime();
-
-    cl_int error;
-    error = runtime->commandQueue().enqueueNDRangeKernel(
-        mImageToBufferKernel, cl::NullRange, cl::NDRange(mImageToBufferRoundUpGWS[0], mImageToBufferRoundUpGWS[1]),
-        cl::NDRange(mLocalWorkSize[0], mLocalWorkSize[1]), nullptr, nullptr);
-    MNN_CHECK_CL_SUCCESS(error);
-
-    error = runtime->commandQueue().enqueueNDRangeKernel(
-        mBufferToImageKernel, cl::NullRange, cl::NDRange(mBufferToImageRoundUpGWS[0], mBufferToImageRoundUpGWS[1]),
-        cl::NDRange(mLocalWorkSize[0], mLocalWorkSize[1]), nullptr, nullptr);
-    MNN_CHECK_CL_SUCCESS(error);
-
+    // auto runtime = mOpenCLBackend->getOpenCLRuntime();
+    // runtime->commandQueue().flush();
+    // runtime->commandQueue().finish();
+    // MNN_PRINT("In ReshapeExecution ReshapeExecution before run commandQueue finish!\n");
+
+    // cl_int error;
+    // error = runtime->commandQueue().enqueueNDRangeKernel(
+    //     mImageToBufferKernel, cl::NullRange, cl::NDRange(mImageToBufferRoundUpGWS[0], mImageToBufferRoundUpGWS[1]),
+    //     cl::NDRange(mLocalWorkSize[0], mLocalWorkSize[1]), nullptr, nullptr);
+    // MNN_CHECK_CL_SUCCESS(error);
+
+    // error = runtime->commandQueue().enqueueNDRangeKernel(
+    //     mBufferToImageKernel, cl::NullRange, cl::NDRange(mBufferToImageRoundUpGWS[0], mBufferToImageRoundUpGWS[1]),
+    //     cl::NDRange(mLocalWorkSize[0], mLocalWorkSize[1]), nullptr, nullptr);
+    // runtime->commandQueue().flush();
+    // runtime->commandQueue().finish();
+    // MNN_CHECK_CL_SUCCESS(error);
+    
+    // MNN_PRINT("end ReshapeExecution onExecute !\n");
 #ifdef LOG_VERBOSE
     MNN_PRINT("end ReshapeExecution onExecute !\n");
 #endif
diff --git a/source/backend/opencl/execution/cl/buffer_to_image.cl b/source/backend/opencl/execution/cl/buffer_to_image.cl
index c84f1ae..cc78740 100644
--- a/source/backend/opencl/execution/cl/buffer_to_image.cl
+++ b/source/backend/opencl/execution/cl/buffer_to_image.cl
@@ -5,28 +5,113 @@
     }
 __constant sampler_t SAMPLER = CLK_NORMALIZED_COORDS_FALSE | CLK_ADDRESS_CLAMP | CLK_FILTER_NEAREST;
 
+__kernel void print_float_buffer(__private const int global_size, __global const float *input_ptr){
+    int idx = get_global_id(0);
+    printf("%f \n", input_ptr[idx]);
+}
+
+__kernel void print_image(GLOBAL_SIZE_2_DIMS __read_only image2d_t input){
+    int image_width_idx  = get_global_id(0); // UP_DIV(input->channel(), 4) * input->width()
+    int image_height_idx = get_global_id(1); // input->batch() * input->height()
+    DEAL_NON_UNIFORM_DIM2(image_width_idx, image_height_idx);
+    int2 coord = (int2)(image_width_idx, image_height_idx);
+    float4 values = read_imagef(input, coord);
+    printf("(%d, %d: %3.0f %3.0f %3.0f %3.0f)", image_width_idx, image_height_idx, values.x, values.y, values.z, values.w);
+    if(image_width_idx == global_size_dim0 - 1){
+        printf("\n");
+    }
+}
 
 __kernel void nc4hw4_buffer_to_image(GLOBAL_SIZE_2_DIMS __global const float *input_ptr, __private const int2 output_shape,
                                      __private const int channel_4, __write_only image2d_t output) {
 
-    int image_width_idx  = get_global_id(0);
-    int image_height_idx = get_global_id(1);
-
+    int image_width_idx  = get_global_id(0); // UP_DIV(input->channel(), 4) * input->width()
+    int image_height_idx = get_global_id(1); // input->batch() * input->height()
+    
     DEAL_NON_UNIFORM_DIM2(image_width_idx, image_height_idx);
-
+    //printf("(%d, %d: %f)\n", image_width_idx, image_height_idx, *(input_ptr + buffer_offset));
     const int batch_idx         = image_height_idx / output_shape.x;
     const int height_idx        = image_height_idx % output_shape.x;
     const int width_idx         = image_width_idx % output_shape.y;
     const int channel_block_idx = image_width_idx / output_shape.y;
     int buffer_offset =
         (((batch_idx * channel_4 + channel_block_idx) * output_shape.x + height_idx) * output_shape.y + width_idx) * 4;
+    // printf("%f\n", (input_ptr + buffer_offset)[0]);
+    float4 values = vload4(0, input_ptr + buffer_offset);
+    //printf("(%d, %d: %f)\n", image_width_idx, image_height_idx, *(input_ptr + buffer_offset));
+    // printf("(%d, %d: %3.0f %3.0f %3.0f %3.0f)", image_width_idx, image_height_idx, values.x, values.y, values.z, values.w);
+    int2 coord = (int2)(image_width_idx, image_height_idx);
+    write_imagef(output, coord, values);
+}
 
+__kernel void nc4hw4_buffer_to_image_v2(GLOBAL_SIZE_2_DIMS __global const float *input_ptr, __private const int2 output_shape,
+                                     __private const int channel_4, __write_only image2d_t output) {
+
+    int image_width_idx  = get_global_id(0);
+    int image_height_idx = get_global_id(1);
+    
+    DEAL_NON_UNIFORM_DIM2(image_width_idx, image_height_idx);
+    // printf("(%d, %d)\n", image_width_idx, image_height_idx);
+    const int batch_idx         = 0;
+    const int height_idx        = image_height_idx;
+    const int width_idx         = image_width_idx % output_shape.y;
+    const int channel_block_idx = image_width_idx / output_shape.y;
+    int buffer_offset =
+        (((channel_4 + channel_block_idx) * output_shape.x + height_idx) * output_shape.y + width_idx) * 4;
+    // printf("%f\n", (input_ptr + buffer_offset)[0]);
     float4 values = vload4(0, input_ptr + buffer_offset);
 
     int2 coord = (int2)(image_width_idx, image_height_idx);
     write_imagef(output, coord, values);
 }
 
+
+__kernel void nc4hw4_buffer_to_image_v1(GLOBAL_SIZE_2_DIMS __global const float *input_ptr, __private const int2 output_shape,
+                                     __private const int channel_4, __write_only image2d_t output) {
+
+    int image_height_idx = get_global_id(0);
+    // int local_id = get_local_id(0);
+    // printf("Global id: %d, local id: %d \n", image_height_idx, local_id);
+    if(image_height_idx >= global_size_dim1){
+        return;
+    }
+    const int W = output_shape.y / (channel_4);
+    // const int batch_idx         = image_height_idx / output_shape.x;
+    const int batch_idx         = 0;
+    const int height_idx        = image_height_idx % output_shape.x;
+
+    for(int channel_block_idx = 0; channel_block_idx < channel_4; ++channel_block_idx){
+        int image_width_idx_tmp = channel_block_idx * output_shape.y;
+        int buffer_offset_tmp =
+            (((channel_block_idx) * output_shape.x + height_idx) * output_shape.y);
+        // (((batch_idx * channel_4 + channel_block_idx) * output_shape.x + height_idx) * output_shape.y);
+        for(int width_idx = 0; width_idx < W - 1; width_idx += 2){
+            int image_width_idx = image_width_idx_tmp + width_idx;
+            int buffer_offset = (buffer_offset_tmp + width_idx) * 4;
+            float4 values = vload4(0, input_ptr + buffer_offset);
+            int2 coord = (int2)(image_width_idx, image_height_idx);
+            write_imagef(output, coord, values);
+            // float* tmp = input_ptr + buffer_offset;
+            // printf("v1 (%d, %d, %d, %d)->(%d, %d)->[%f, %f, %f, %f]\n",
+            // batch_idx, height_idx, width_idx, channel_block_idx,
+            // image_width_idx, image_height_idx, 
+            // tmp[0], tmp[1], tmp[2], tmp[3]);
+
+            image_width_idx += 1;
+            buffer_offset += 4;
+            values = vload4(0, input_ptr + buffer_offset);
+            coord = (int2)(image_width_idx, image_height_idx);
+            write_imagef(output, coord, values);
+            // tmp = input_ptr + buffer_offset;
+            // printf("v1 (%d, %d, %d, %d)->(%d, %d)->[%f, %f, %f, %f]\n",
+            // batch_idx, height_idx, width_idx+1, channel_block_idx,
+            // image_width_idx, image_height_idx, 
+            // tmp[0], tmp[1], tmp[2], tmp[3]);
+        }
+    }
+}
+
+
 __kernel void image_to_nc4hw4_buffer(GLOBAL_SIZE_2_DIMS __global float *output, /* nchw */
                                      __private const int2 output_shape,
                                      __private const int channel_4,
@@ -293,7 +378,7 @@ __kernel void nchw_buffer_to_image(GLOBAL_SIZE_2_DIMS __global const float *inpu
     int image_height_idx = get_global_id(1);
 
     DEAL_NON_UNIFORM_DIM2(image_width_idx, image_height_idx);
-
+    // printf("nchw_buffer_to_image (%d, %d)\n", image_width_idx, image_height_idx);
     const int batch_idx     = image_height_idx / height;
     const int height_idx    = image_height_idx % height;
     const int width_idx     = image_width_idx % width;
@@ -342,7 +427,7 @@ __kernel void image_to_nhwc_buffer(GLOBAL_SIZE_2_DIMS __global float *output, /*
     int image_height_idx = get_global_id(1);
 
     DEAL_NON_UNIFORM_DIM2(image_width_idx, image_height_idx);
-
+    // printf("image_to_nhwc_buffer (%d, %d)\n", image_width_idx, image_height_idx);
     const int batch_idx     = image_height_idx / height;
     const int height_idx    = image_height_idx % height;
     const int width_idx     = image_width_idx % width;
@@ -370,6 +455,7 @@ __kernel void image_to_nhwc_buffer(GLOBAL_SIZE_2_DIMS __global float *output, /*
         int offset     = buffer_offset;
         output[offset] = values.x;
     }
+    // printf("image_to_nhwc_buffer (%d, %d) finish\n", image_width_idx, image_height_idx);
 }
 
 // only for debug
diff --git a/source/backend/opengl/GLRelu.cpp b/source/backend/opengl/GLRelu.cpp
index e2c36ef..03f2661 100644
--- a/source/backend/opengl/GLRelu.cpp
+++ b/source/backend/opengl/GLRelu.cpp
@@ -41,6 +41,7 @@ ErrorCode GLRelu::onResize(const std::vector<Tensor *> &inputs, const std::vecto
         int count = ALIGN_UP4(mOp->main_as_PRelu()->slope()->size());
         mSlopeBuffer = std::shared_ptr<GLSSBOBuffer>(new GLSSBOBuffer(sizeof(float) * count));
         float* slope = (float*)(mSlopeBuffer->map(GL_MAP_WRITE_BIT | GL_MAP_INVALIDATE_BUFFER_BIT));
+        // Memory copy happens here
         if(slope != nullptr){
             ::memset(slope, 0, count * sizeof(float));
             ::memcpy(slope, mOp->main_as_PRelu()->slope()->data(), mOp->main_as_PRelu()->slope()->size() * sizeof(float));
@@ -67,9 +68,9 @@ ErrorCode GLRelu::onExecute(const std::vector<Tensor *> &inputs, const std::vect
         glBindImageTexture(0, output->deviceId(), 0, GL_TRUE, 0, GL_WRITE_ONLY, ((GLBackend *)backend())->getTextrueFormat());
         {
             int texId = 0;
-            glActiveTexture(GL_TEXTURE0 + texId);
-            glUniform1i(1, texId);
-            glBindTexture(GL_TEXTURE_3D, input->deviceId());
+            glActiveTexture(GL_TEXTURE0 + texId); // select active texture unit
+            glUniform1i(1, texId); // Specify the value of a uniform variable for the current program object
+            glBindTexture(GL_TEXTURE_3D, input->deviceId()); // bind a named texture to a texturing target
             OPENGL_CHECK_ERROR;
         }
         glBindBufferBase(GL_SHADER_STORAGE_BUFFER, 2, mSlopeBuffer->getId());
diff --git a/source/backend/vulkan/backend/VulkanBackend.cpp b/source/backend/vulkan/backend/VulkanBackend.cpp
index ff5a3ac..19be789 100644
--- a/source/backend/vulkan/backend/VulkanBackend.cpp
+++ b/source/backend/vulkan/backend/VulkanBackend.cpp
@@ -254,6 +254,14 @@ bool VulkanBackend::onClearBuffer() {
     mAllBuffers.clear();
     return true;
 }
+
+
+bool isSupportOpType(const MNN::Op* op){
+    auto creator = getCreatorMap();
+    auto iter    = creator->find(op->type());
+    return (iter == creator->end());
+}
+
 Execution* VulkanBackend::onCreate(const std::vector<Tensor*>& inputs, const std::vector<Tensor*>& outputs,
                                    const MNN::Op* op) {
     auto creator = getCreatorMap();
diff --git a/source/backend/vulkan/backend/VulkanBackend.hpp b/source/backend/vulkan/backend/VulkanBackend.hpp
index 5ef6658..12e4ca9 100644
--- a/source/backend/vulkan/backend/VulkanBackend.hpp
+++ b/source/backend/vulkan/backend/VulkanBackend.hpp
@@ -54,6 +54,7 @@ public:
     virtual bool onAcquireBuffer(const Tensor* tensor, StorageType storageType) override;
     virtual bool onReleaseBuffer(const Tensor* tensor, StorageType storageType) override;
     virtual bool onClearBuffer() override;
+    virtual bool isSupportOpType(const MNN::Op* op) override;
     virtual Execution* onCreate(const std::vector<Tensor*>& inputs, const std::vector<Tensor*>& outputs,
                                 const MNN::Op* op) override;
     virtual std::pair<float, bool> onMeasure(const std::vector<Tensor*>& inputs, const std::vector<Tensor*>& outputs,
diff --git a/source/core/Backend.hpp b/source/core/Backend.hpp
index fc330c0..f5b6f4c 100644
--- a/source/core/Backend.hpp
+++ b/source/core/Backend.hpp
@@ -24,6 +24,7 @@ struct Op;
 struct GpuLibrary;
 class Execution;
 
+
 /** abstract backend */
 class Backend : public NonCopyable {
 public:
@@ -31,8 +32,11 @@ public:
     struct Info {
         /** forward type. */
         MNNForwardType type = MNN_FORWARD_CPU;
+        /** Execution type */
+        MNNExecutionType executionType = MNNExecutionType::MNN_EXECUTION_DEFAULT;
         /** for CPU only. number of threads. */
         int numThread = 4;
+        int CPULittleNumThread = 4;
         /** user data. */
         BackendConfig* user = NULL;
         enum Mode {
@@ -97,6 +101,8 @@ public:
         return std::make_pair(0.0f, false);
     }
 
+    virtual bool isSupportOpType(const MNN::Op* op);
+
     /**
      * @brief create execution for op with input and output tensors.
      * @param inputs    input tensors.
diff --git a/source/core/CPUScheduler.hpp b/source/core/CPUScheduler.hpp
new file mode 100644
index 0000000..9092079
--- /dev/null
+++ b/source/core/CPUScheduler.hpp
@@ -0,0 +1,138 @@
+#ifdef __ANDROID__
+#include <stdio.h>
+#include <string.h>
+#include <vector>
+#include <algorithm>
+#include <stdint.h>
+#include <sys/syscall.h>
+#include <unistd.h>
+#include <algorithm>
+#include <pthread.h>
+#include <sched.h>
+#ifdef _OPENMP
+#include <omp.h>
+#endif // _OPENMP
+#include "Macro.h"
+#include "core/Concurrency.h"
+
+static int getNumberOfCPU() {
+    FILE* fp = fopen("/proc/cpuinfo", "rb");
+    if (!fp) {
+        return 1;
+    }
+    int number = 0;
+    char buffer[1024];
+    while (!feof(fp)) {
+        char* str = fgets(buffer, 1024, fp);
+        if (!str) {
+            break;
+        }
+        if (memcmp(buffer, "processor", 9) == 0) {
+            number++;
+        }
+    }
+    fclose(fp);
+    if (number < 1) {
+        number = 1;
+    }
+    return number;
+}
+
+static int getCPUMaxFreqKHz(int cpuID) {
+    char path[256];
+    sprintf(path, "/sys/devices/system/cpu/cpufreq/stats/cpu%d/time_in_state", cpuID);
+    FILE* fp = fopen(path, "rb");
+    if (!fp) {
+        sprintf(path, "/sys/devices/system/cpu/cpu%d/cpufreq/stats/time_in_state", cpuID);
+        fp = fopen(path, "rb");
+        if (!fp) {
+            sprintf(path, "/sys/devices/system/cpu/cpu%d/cpufreq/cpuinfo_max_freq", cpuID);
+            fp = fopen(path, "rb");
+            if (!fp) {
+                return -1;
+            }
+            int maxfrequency = -1;
+            fscanf(fp, "%d", &maxfrequency);
+            fclose(fp);
+            return maxfrequency;
+        }
+    }
+    int maxfrequency = 0;
+    while (!feof(fp)) {
+        int frequency = 0;
+        int history   = fscanf(fp, "%d %*d", &frequency);
+        if (history != 1) {
+            break;
+        }
+        if (frequency > maxfrequency) {
+            maxfrequency = frequency;
+        }
+    }
+    fclose(fp);
+    return maxfrequency;
+}
+
+static int getBigCoreIndex(){
+    const int cpuNumbers = getNumberOfCPU();
+    std::vector<std::pair<int, int>> coreIdxFreq;
+    int freq = 0;
+    std::vector<int> clusterStartCorIdx;
+    for (int i = 0; i < cpuNumbers; ++i) {
+        int maxfrequency = getCPUMaxFreqKHz(i);
+        if(maxfrequency > freq){
+            freq = maxfrequency;
+            clusterStartCorIdx.push_back(i);
+        }
+        coreIdxFreq.push_back(std::make_pair(i, maxfrequency));
+    }
+    for (int i = 0; i < cpuNumbers; ++i) {
+        printf("CPU cluster <core_id: %d, max_freq: %d>\n", coreIdxFreq[i].first, coreIdxFreq[i].second);
+    }
+    // Support arm CPU 3 clusters
+    if(clusterStartCorIdx.size() == 1){
+        return 0;
+    }else if(clusterStartCorIdx.size() == 2){
+        return clusterStartCorIdx[1];
+    }else if(clusterStartCorIdx.size() == 3){
+        return clusterStartCorIdx[1];
+    }else{
+        return 0;
+    }
+    for (int i = 0; i < cpuNumbers; ++i) {
+        printf("<%d, %d>\n", coreIdxFreq[i].first, coreIdxFreq[i].second);
+        if(coreIdxFreq[i].second == freq){
+            return i;
+        }
+    }
+    return 0;
+    // auto freqCmp = [](std::pair<int, int> p1, std::pair<int, int> p2) {
+    //     return p1.second < p2.second;
+    // };
+    // sort(coreIdxFreq.begin(), coreIdxFreq.end(), freqCmp);
+    // for (int i = 0; i < cpuNumbers; ++i) {
+    //     printf("%d %d\n", coreIdxFreq[i].first, coreIdxFreq[i].second);
+    // }
+    // return coreIdxFreq[0].first;
+}
+
+static bool setCPUAffinity(std::vector<int>* CPUCoreList){
+    std::string forwardTypeStr = "CPU";
+    const int threadNumber = CPUCoreList->size();
+    omp_set_num_threads(threadNumber);
+    int rc = 1;
+    MNN_CONCURRENCY_BEGIN(tId, threadNumber) {
+    cpu_set_t cpusetOfCPU;
+    CPU_ZERO(&cpusetOfCPU);
+    CPU_SET(CPUCoreList->at(tId), &cpusetOfCPU);
+    pid_t pid = pthread_gettid_np(pthread_self());
+    
+    int attemptCount = 1000;
+    while(rc != 0 && attemptCount--) {
+        rc = sched_setaffinity(pid, sizeof(cpusetOfCPU), &cpusetOfCPU);
+    }
+    MNN_PRINT("CPU->RunUnits set tId %d pid %d set-on-core %d real-on-core %d ret %d\n", tId, pid, CPUCoreList->at(tId), sched_getcpu(), rc);
+    }
+    MNN_CONCURRENCY_END();
+    return rc == 0;
+}
+#endif
\ No newline at end of file
diff --git a/source/core/Execution.hpp b/source/core/Execution.hpp
index c4f65a7..7577c9a 100644
--- a/source/core/Execution.hpp
+++ b/source/core/Execution.hpp
@@ -15,6 +15,7 @@
 #include <MNN/MNNForwardType.h>
 #include "NonCopyable.hpp"
 #include <MNN/Tensor.hpp>
+#include "Pipeline.hpp"
 
 namespace MNN {
 class Backend;
@@ -118,12 +119,34 @@ public:
     Backend *backend() const {
         return mBackEnd;
     }
+    
+    /**
+     * For WrapExecution, use this two field to record the actual operation computing time
+    */
+    uint64_t mStartTimeStamp = 0;
+    uint64_t mEndTimeStamp = 0;
+    /**
+     * @brief get unit that create this unit.
+     * @return unit.
+    */
+    // MNN::Pipeline::Unit* unit() const {
+    //     return mUnit;
+    // }
+    
+    // bool setUnit(MNN::Pipeline::Unit *u) {
+    //     if(u == nullptr){
+    //         return false;
+    //     }
+    //     this->mUnit = u;
+    //     return true;
+    // }
 
 protected:
     bool mValid = true;
 
 private:
     Backend *mBackEnd;
+    // MNN::Pipeline::Unit *mUnit;
 };
 
 } // namespace MNN
diff --git a/source/core/Interpreter.cpp b/source/core/Interpreter.cpp
index 01006e4..d3c52ab 100644
--- a/source/core/Interpreter.cpp
+++ b/source/core/Interpreter.cpp
@@ -108,6 +108,12 @@ Session* Interpreter::createMultiPathSession(const std::vector<ScheduleConfig>&
         return nullptr;
     }
     auto result = newSession.get();
+    // Set op to device map file
+    auto config = configs[0];
+    result->executionType = config.executionType;
+    result->deviceMapFilePath = config.deviceMapFilePath;
+    // Call pipeline prepare here
+    // pipeline call Unit prepare
     if (info.validForResize) {
         result->resize();
     }
@@ -142,6 +148,10 @@ ErrorCode Interpreter::runSession(Session* session) const {
     return session->run();
 }
 
+ErrorCode Interpreter::runSession(Session* session, std::vector<RunMetadata> *net_stats) {
+    return session->run(net_stats);
+}
+
 Tensor* Interpreter::getSessionInput(const Session* session, const char* name) {
     MNN_ASSERT(nullptr != session);
     if (session == nullptr) {
diff --git a/source/core/MUtils.hpp b/source/core/MUtils.hpp
new file mode 100644
index 0000000..d29714f
--- /dev/null
+++ b/source/core/MUtils.hpp
@@ -0,0 +1,121 @@
+
+#ifndef MUtils_hpp
+#define MUtils_hpp
+
+#include <sys/time.h>
+#include <string>
+#include <vector>
+#include <map>
+#include <sstream> 
+
+#include <MNN/MNNDefine.h>
+#include <MNN/MNNForwardType.h>
+#include "core/Macro.h"
+
+namespace MNN {
+
+static inline uint64_t getTimeInUs() {
+  uint64_t time;
+#if defined(_MSC_VER)
+  LARGE_INTEGER now, freq;
+  QueryPerformanceCounter(&now);
+  QueryPerformanceFrequency(&freq);
+  uint64_t sec = now.QuadPart / freq.QuadPart;
+  uint64_t usec = (now.QuadPart % freq.QuadPart) * 1000000 / freq.QuadPart;
+  time = sec * 1000000 + usec;
+#else
+  struct timeval tv;
+  gettimeofday(&tv, nullptr);
+  time = static_cast<uint64_t>(tv.tv_sec) * 1000000 + tv.tv_usec;
+#endif
+  return time;
+}
+
+
+inline uint64_t NowMicros() {
+  
+  struct timeval tv;
+  gettimeofday(&tv, nullptr);
+  return static_cast<int64_t>(tv.tv_sec) * 1000000 + tv.tv_usec;
+}
+
+struct CallStats {
+  uint64_t start_micros;
+  uint64_t end_micros;
+};
+
+struct ConvPoolArgs {
+  std::vector<int> strides;
+  int padding_type;
+  std::vector<int> paddings;
+  std::vector<int> dilations;
+  std::vector<int64_t> kernels;
+};
+
+struct OperatorStats {
+  std::string operator_name;
+  std::string type;
+  MNNForwardType forwardType;
+  std::vector<std::vector<int>> output_shape;
+  //ConvPoolArgs args;
+  CallStats stats;
+};
+
+class RunMetadata {
+ public:
+  std::vector<OperatorStats> op_stats;
+};
+
+class Record{
+  public:
+  std::string operator_name;
+  std::string type;
+  std::vector<std::vector<int>> output_shape;
+  std::vector<int64_t> start_micros;
+  std::vector<int64_t> end_micros;
+  
+  Record(){
+
+  }
+
+  Record(std::string name, std::string type){
+    this->operator_name = name;
+    this->type = type;
+  }
+
+  int64_t getAverageTime(){
+    if(start_micros.size()!=end_micros.size()){
+      return -1;
+    }
+    int64_t sum = 0;
+    const int64_t size = start_micros.size();
+    for(int i=0; i<size; ++i){
+      sum += (end_micros[i] - start_micros[i]);
+    }
+    if(size==0){
+      return 0;
+    }else{
+      return sum / size;
+    }
+  }
+
+  void addStartEndMicros(int64_t start_micro, int64_t end_micro){
+    start_micros.push_back(start_micro);
+    end_micros.push_back(end_micro);
+  }
+
+  std::string getOneStartEndMicros(){
+    std::stringstream ss;
+    if(start_micros.size() > 0){
+      ss<<start_micros[0]<<"\t"<<end_micros[0];
+      return ss.str();
+    }
+    return "";
+  }
+};
+
+//void statOpMetadata(std::vector<MNN::RunMetadata> *net_stats)
+
+}
+
+#endif
\ No newline at end of file
diff --git a/source/core/Macro.h b/source/core/Macro.h
index fba6a47..3603d76 100644
--- a/source/core/Macro.h
+++ b/source/core/Macro.h
@@ -13,6 +13,7 @@
 #define ALIMIN(x, y) ((x) < (y) ? (x) : (y))
 #define ALIMAX(x, y) ((x) > (y) ? (x) : (y))
 
+#define FLOOR_DOWN(x, y) (x / y * y)
 #define UP_DIV(x, y) (((x) + (y) - (1)) / (y))
 #define ROUND_UP(x, y) (((x) + (y) - (1)) / (y) * (y))
 #define ALIGN_UP4(x) ROUND_UP((x), 4)
diff --git a/source/core/Pipeline.cpp b/source/core/Pipeline.cpp
index ba81c9b..83f62cd 100644
--- a/source/core/Pipeline.cpp
+++ b/source/core/Pipeline.cpp
@@ -6,350 +6,1180 @@
 //  Copyright  2018, Alibaba Group Holding Limited
 //
 
-#include "core/Pipeline.hpp"
-#include "core/Backend.hpp"
-#include "core/Macro.h"
-#include "core/SizeComputer.hpp"
-#include "core/TensorUtils.hpp"
-#include "core/WrapExecution.hpp"
+#include "Pipeline.hpp"
+
+#include <queue>
+#include <algorithm>
+#include <assert.h>
+
+#ifdef _OPENMP
+#include <omp.h>
+#endif // _OPENMP
+#ifdef __ANDROID__
+#include "CPUScheduler.hpp"
+#endif
+#include "core/Concurrency.h"
+#include "Macro.h"
+#include "SizeComputer.hpp"
+#include "TensorUtils.hpp"
+#include "WrapExecution.hpp"
+#include "backend/cpu/CPUConcat.hpp"
 //#define MNN_OPEN_TIME_TRACE
-#include <MNN/AutoTime.hpp>
+#include "MNN/AutoTime.hpp"
 //#define MNN_DEBUG_TENSOR_SIZE
 namespace MNN {
 OperatorInfo::OperatorInfo() {
-    mContent = new Info;
-    MNN_ASSERT(nullptr != mContent);
+  mContent = new Info;
+  MNN_ASSERT(nullptr != mContent);
 }
-OperatorInfo::~OperatorInfo() {
-    delete mContent;
+OperatorInfo::~OperatorInfo() { delete mContent; }
+
+const std::string& OperatorInfo::name() const { return mContent->name; }
+
+const std::string& OperatorInfo::type() const { return mContent->type; }
+
+float OperatorInfo::flops() const { return mContent->flops; }
+
+static Backend::StorageType _getTensorStorageType(const Tensor* tensor) {
+  auto des = TensorUtils::getDescribe(tensor);
+  if (des->isConst || des->isInput) {
+    return Backend::DYNAMIC_SEPERATE;
+  }
+  if (des->handleType != Tensor::HANDLE_NONE) {
+    return Backend::DYNAMIC_SEPERATE;
+  }
+  return Backend::DYNAMIC;
 }
 
-const std::string& OperatorInfo::name() const {
-    return mContent->name;
+static Backend::StorageType _getTensorReleaseStorageType(const Tensor* tensor) {
+  auto des = TensorUtils::getDescribe(tensor);
+  if (des->handleType != Tensor::HANDLE_NONE) {
+    return Backend::DYNAMIC_SEPERATE;
+  }
+  if (des->isConst) {
+    return Backend::DYNAMIC_SEPERATE;
+  }
+  return Backend::DYNAMIC;
 }
 
-const std::string& OperatorInfo::type() const {
-    return mContent->type;
+bool Pipeline::Unit::_allocTensors(Backend* bn,
+                                   const std::vector<Tensor*>& tensors) {
+  for (auto t : tensors) {
+    auto des = TensorUtils::getDescribe(t);
+    if (nullptr != des->backend) {
+      continue;
+    }
+    des->backend = bn;
+    TensorUtils::setLinearLayout(t);
+    auto success = bn->onAcquireBuffer(t, _getTensorStorageType(t));
+    if (!success) {
+      return false;
+    }
+  }
+  return true;
 }
 
-float OperatorInfo::flops() const {
-    return mContent->flops;
+Pipeline::Unit::Unit(const Op* op, const std::vector<Tensor*>& inputs,
+                     const std::vector<Tensor*>& outputs) {
+  MNN_ASSERT(nullptr != op);
+  mOriginOp = op;
+  mType = op->type();
+  mInputs = inputs;
+  mOutputs = outputs;
+  // MNN_PRINT("In Unit::Unit op name %s, type %d\n", op->name()->str().c_str(),
+  // op->type());
+  if (nullptr != op->name()) {
+    mContent->name = op->name()->str();
+  }
+  auto typeStr = EnumNameOpType(mType);
+  if (nullptr != typeStr) {
+    mContent->type = typeStr;
+  }
+  mComputer = SizeComputerSuite::get()->search(mType);
+  std::atomic_init(&(this->mDispatched), false);
+  std::atomic_init(&(this->mExecuted), false);
+  std::atomic_init(&(this->mAddedToWaitList), false);
+  this->mForwardType = MNNForwardType::MNN_FORWARD_CPU;
 }
 
-static Backend::StorageType _getTensorStorageType(const Tensor* tensor) {
-    auto des = TensorUtils::getDescribe(tensor);
-    if (des->isConst || des->isInput || des->isTrainableParameter) {
-        return Backend::DYNAMIC_SEPERATE;
+bool Pipeline::Unit::_createExecution(Backend* bn, Backend* cpuBn) {
+  // Create Execution specified by user
+  mExecution.reset(bn->onCreate(mInputs, mOutputs, mOriginOp));
+  // MNN_PRINT("Creating: %s %d %p\n", mOriginOp->name()->c_str(), bn->type(),
+  // mExecution.get());
+  if (nullptr ==
+      mExecution) {  // If not support, using default CPUBackend Execution
+    mExecution.reset(cpuBn->onCreate(mInputs, mOutputs, mOriginOp));
+  }
+  if (nullptr == mExecution) {
+    return false;
+  }
+  bool needWrap = false;
+
+  auto executionBackend = mExecution->backend();
+  for (int i = 0; i < mInputs.size(); ++i) {
+    auto t = mInputs[i];
+    auto des = TensorUtils::getDescribe(t);
+    if (des->backend != executionBackend &&
+        SizeComputer::opNeedContent(mOriginOp->type(), i)) {
+      needWrap = true;
     }
-    if (des->handleType != Tensor::HANDLE_NONE) {
-        return Backend::DYNAMIC_SEPERATE;
+  }
+  if (pipelineExecutionType==MNNExecutionType::MNN_EXECUTION_BIG_LITTLE){
+    needWrap = false;
+  }
+  if (needWrap) {
+    // FUNC_PRINT_ALL(mOriginOp->name()->c_str(), s);
+    auto tempExecution = mExecution;
+    mExecution.reset(new WrapExecution(cpuBn, tempExecution));
+    this->isWrapExecution = true;
+    // MNN_PRINT("%s need to wrap*****\n", this->name().c_str());
+  }
+
+  return mExecution->valid();
+}
+
+ErrorCode Pipeline::Unit::execute() {
+  if (nullptr == mExecution) {
+    return NO_EXECUTION;
+  }
+  if (mConst) {
+    return NO_ERROR;
+  }
+
+  // auto start = (MNN::NowMicros());
+  // mExecution->setNetStartTime(this->netStartTime);
+  // MNN_PRINT("mExecution->onExecute %s Start \n", this->type().c_str());
+  MNN::ErrorCode code = mExecution->onExecute(mInputs, mOutputs);
+  // auto end = (MNN::NowMicros());
+  // if (this->type() == "Concat") {
+  //   MNN_PRINT("Concat execute:%lu\n", end - start);
+  // }
+
+  if (NO_ERROR != code) {
+    MNN_ERROR("Execute Error for %s, code=%d\n", mContent->name.c_str(), code);
+  }
+  return code;
+}
+
+ErrorCode Pipeline::Unit::executeCallBack(const TensorCallBackWithInfo& before,
+                                          const TensorCallBackWithInfo& after) {
+  if (nullptr == mExecution) {
+    return NO_EXECUTION;
+  }
+  if (mConst) {
+    return NO_ERROR;
+  }
+  auto run = before(mInputs, this);
+  if (run) {
+    auto code = mExecution->onExecute(mInputs, mOutputs);
+    if (NO_ERROR != code) {
+      MNN_ERROR("Execute Error for %s, code=%d\n", mContent->name.c_str(),
+                code);
+      return code;
     }
-    return Backend::DYNAMIC;
+  }
+  auto runOthers = after(mOutputs, this);
+  if (!runOthers) {
+    return CALL_BACK_STOP;
+  }
+  return NO_ERROR;
 }
 
-static Backend::StorageType _getTensorReleaseStorageType(const Tensor* tensor) {
-    auto des = TensorUtils::getDescribe(tensor);
-    if (des->handleType != Tensor::HANDLE_NONE) {
-        return Backend::DYNAMIC_SEPERATE;
+ErrorCode Pipeline::Unit::prepare(Backend* bn, Backend* cpuBn) {
+  std::string opName = this->name();
+  // MNN_PRINT("In Pipeline::Unit::prepare %s backend
+  // %d\n",this->name().c_str(), bn->type());
+  for (auto t : mInputs) {
+    bool valid = true;
+    for (int i = 0; i < t->dimensions(); ++i) {
+      if (t->length(i) <= 0) {
+        valid = false;
+        break;
+      }
     }
-    if (des->isConst || des->isTrainableParameter) {
-        return Backend::DYNAMIC_SEPERATE;
+    if (!valid) {
+      MNN_ERROR("The %s's input is not ready\n", mContent->name.c_str());
+      return COMPUTE_SIZE_ERROR;
     }
-    return Backend::DYNAMIC;
-}
+  }
+  {
+    auto success = _allocTensors(bn, mInputs);
+    if (!success) {
+      return OUT_OF_MEMORY;
+    }
+  }
+  bool ready = SizeComputer::computeOutputSize(mOriginOp, mInputs, mOutputs);
+  for (auto o : mOutputs) {
+    if (o->size() <= 0) {
+      ready = false;
+    }
+  }
+  if (nullptr != mComputer) {
+    mContent->flops = mComputer->onComputeFlops(mOriginOp, mInputs, mOutputs);
+  } else {
+    // Default set the same as output size, unit is M
+    mContent->flops = (float)mOutputs[0]->elementSize() / 1024.0f / 1024.0f;
+  }
 
-bool Pipeline::Unit::_allocTensors(Backend* bn, const std::vector<Tensor*>& tensors) {
-    for (auto t : tensors) {
-        auto des = TensorUtils::getDescribe(t);
-        if (nullptr != des->backend) {
-            continue;
-        }
-        des->backend = bn;
-        TensorUtils::setLinearLayout(t);
-        auto success = bn->onAcquireBuffer(t, _getTensorStorageType(t));
-        if (!success) {
-            return false;
-        }
+#ifdef MNN_DEBUG_TENSOR_SIZE
+  MNN_PRINT("\n===> compute shape: %s, [%d]\n", mOriginOp->name()->c_str(),
+            mOriginOp->type());
+  if (mInputs.size()) {
+    MNN_PRINT("Inputs:\n");
+    for (auto o : mInputs) {
+      if (o->dimensions() == 0) {
+        MNN_PRINT("\t*Scalar*");
+      }
+      for (int i = 0; i < o->dimensions(); ++i) {
+        MNN_PRINT("%d, ", o->length(i));
+      }
+      MNN_PRINT("\n");
     }
-    return true;
-}
+  }
+  MNN_PRINT("Outputs:\n");
+  for (auto o : mOutputs) {
+    if (o->dimensions() == 0) {
+      MNN_PRINT("\t*Scalar*");
+    }
+    for (int i = 0; i < o->dimensions(); ++i) {
+      MNN_PRINT("%d, ", o->length(i));
+    }
+    MNN_PRINT("\n");
+  }
+#endif
+  if (!ready) {
+    return COMPUTE_SIZE_ERROR;
+  }
 
-Pipeline::Unit::Unit(const Op* op, const std::vector<Tensor*>& inputs, const std::vector<Tensor*>& outputs) {
-    MNN_ASSERT(nullptr != op);
-    mOriginOp = op;
-    mType     = op->type();
-    mInputs   = inputs;
-    mOutputs  = outputs;
-    if (nullptr != op->name()) {
-        mContent->name = op->name()->str();
+  // Check const
+  mConst = true;
+  for (int i = 0; i < mInputs.size(); ++i) {
+    if (SizeComputer::opNeedContent(mOriginOp->type(), i) &&
+        (!TensorUtils::getDescribe(mInputs[i])->isConst)) {
+      mConst = false;
+      break;
+    }
+  }
+
+  if (mConst) {
+    for (auto t : mOutputs) {
+      TensorUtils::getDescribe(t)->isConst = true;
+    }
+    bn = cpuBn;
+  }
+  // Important! create Execution Here
+  // Create or Resize execution
+  if (nullptr == mExecution) {
+    auto sucess = _createExecution(bn, cpuBn);
+    if (!sucess || mExecution == nullptr) {
+      return NOT_SUPPORT;
+    }
+  }
+  bn = mExecution->backend();
+  {
+    auto success = _allocTensors(bn, mOutputs);
+    if (!success) {
+      return OUT_OF_MEMORY;
     }
-    auto typeStr = EnumNameOpType(mType);
-    if (nullptr != typeStr) {
-        mContent->type = typeStr;
+  }
+  auto code = mExecution->onResize(mInputs, mOutputs);
+  if (TENSOR_NOT_SUPPORT == code || TENSOR_NEED_DIVIDE == code) {
+    // TODO
+    mExecution.reset();
+    for (auto t : mOutputs) {
+      auto des = TensorUtils::getDescribe(t);
+      des->backend->onReleaseBuffer(t, _getTensorReleaseStorageType(t));
+      des->backend = nullptr;
     }
+    auto sucess =
+        _createExecution(cpuBn, cpuBn);  // Create operation's execution
+    MNN_ASSERT(NO_ERROR == sucess);
+    auto success = _allocTensors(mExecution->backend(), mOutputs);
+    if (!success) {
+      return OUT_OF_MEMORY;
+    }
+    code = mExecution->onResize(mInputs, mOutputs);
+  }
+  if (NO_ERROR != code) {
+    mExecution.reset();
+    return code;
+  }
+  if (mConst) {
+    code = mExecution->onExecute(mInputs, mOutputs);
+  }
+
+  for (auto t : mInputs) {
+    auto des = TensorUtils::getDescribe(t);
+    des->useCount -= 1;
+    if (0 == des->useCount) {
+      des->backend->onReleaseBuffer(t, _getTensorReleaseStorageType(t));
+    }
+  }
+  return code;
 }
 
+Pipeline::Pipeline(const std::vector<Schedule::PipelineInfo>& infos,
+                   Backend* backend, Backend* cpuBackend) {
+  MNN_ASSERT(nullptr != backend);
+  MNN_ASSERT(nullptr != cpuBackend);
+  mBackupBackend = cpuBackend;
+  mBackend = backend;
 
+  // Create varies Unit based on op from flatbuff.
+  for (auto& info : infos) {
+    std::shared_ptr<Unit> unit(new Unit(info.op, info.inputs, info.outputs));
+    mUnits.emplace_back(unit);
+  }
+}
 
-bool Pipeline::Unit::_createExecution(Backend* bn, Backend* cpuBn) {
-    mExecution.reset(bn->onCreate(mInputs, mOutputs, mOriginOp));
-    if (nullptr == mExecution) {
-        mExecution.reset(cpuBn->onCreate(mInputs, mOutputs, mOriginOp));
-    }
-    if (nullptr == mExecution) {
-        return false;
+ErrorCode Pipeline::getDeviceMap(const char* file_path) {
+  // Load device map from txt file
+  // "/data/local/tmp/device_map.txt"
+  if (file_path == nullptr) {
+    MNN_PRINT("DeviceMapFilePath is null.\n");
+    return NO_ERROR;
+  }
+  if (!fileExists(file_path)) {
+    MNN_PRINT("Can not find device map file\n");
+    return NO_ERROR;
+  }
+  std::ifstream infile(file_path, std::ifstream::in);
+  std::string node;
+  int64_t device;
+
+  while (infile >> node >> device) {
+    mDevice_map.insert(std::pair<std::string, int64_t>(node, device));
+    mOp_order.push_back(node);
+  }
+  return NO_ERROR;
+}
+
+std::string printInputsOutputs(std::shared_ptr<MNN::Pipeline::Unit> unit) {
+  std::stringstream ss;
+  ss << "Input:";
+  for (auto input : unit->mInputs) {
+    ss << " " << input;
+  }
+  ss << " Output:";
+  for (auto output : unit->mOutputs) {
+    ss << " " << output;
+  }
+  return ss.str();
+}
+
+std::string printRelationship(std::shared_ptr<MNN::Pipeline::Unit> unit) {
+  std::stringstream ss;
+  ss << "Parents:";
+  for (auto p : unit->parents) {
+    ss << " " << p;
+  }
+  ss << " Childs:";
+  for (auto c : unit->childs) {
+    ss << " " << c;
+  }
+  return ss.str();
+}
+
+ErrorCode Pipeline::setupRelationshipForUnits() {
+  const int64_t netSize = mUnits.size();
+  for (int64_t i = 1; i < netSize; ++i) {
+    auto unit = mUnits[i];
+    for (int64_t j = 0; j < i; ++j) {
+      auto unitBefore = mUnits[j];
+      for (auto input : unit->mInputs) {
+        for (auto output : unitBefore->mOutputs) {
+          if (input == output) {
+            unit->parents.insert(j);
+            unitBefore->childs.insert(i);
+          }
+        }
+      }
     }
-    bool needWrap = false;
+  }
+  return NO_ERROR;
+}
 
-    auto executionBackend = mExecution->backend();
-    for (int i = 0; i < mInputs.size(); ++i) {
-        auto t   = mInputs[i];
-        auto des = TensorUtils::getDescribe(t);
-        if (des->backend != executionBackend && SizeComputer::opNeedContent(mOriginOp->type(), i)) {
-            needWrap = true;
+ErrorCode Pipeline::setUnitsOrder(){
+  if(mDevice_map.size() != mOp_order.size()){
+    MNN_PRINT("Op device map file does not contain all the operations\nUse device map only\n");
+    return ErrorCode::NO_ERROR;
+  }
+  std::vector<std::shared_ptr<Unit>> mTmpUnits(mDevice_map.size());
+  for(int i=0; i<mOp_order.size(); i++){
+    std::string name = mOp_order[i];
+    for(auto u: mUnits){
+        if(u->name() == name){
+          mTmpUnits[i] = u;
+          break;
         }
     }
-    if (needWrap) {
-        // FUNC_PRINT_ALL(mOriginOp->name()->c_str(), s);
-        auto tempExecution = mExecution;
-        mExecution.reset(new WrapExecution(cpuBn, tempExecution));
+  }
+  mUnits = mTmpUnits;
+  MNN_PRINT("Set Unit order done");
+  return ErrorCode::NO_ERROR;
+}
+
+ErrorCode Pipeline::printOpRelationship() {
+  // Print Units name, input tensor shapes, output tensor shapes, parent idx,
+  // children idx
+  int i = 0;
+  MNN_PRINT("OpName Inputs Outputs ParentsIdx ChildIdx\n");
+  
+  for (auto& u : mUnits) {
+    MNN_PRINT("%s ", u->name().c_str());// OpName
+    if (u->mInputs.size() > 0) { // Inputs
+      for (auto tensor : u->mInputs) {
+        MNN_PRINT("%d,%d,%d,%d@%p;", tensor->batch(), tensor->channel(),
+                  tensor->height(), tensor->width(), tensor);
+      }
+    } else {
+      MNN_PRINT("none");
+    }
+    MNN_PRINT(" "); // Outputs
+    if (u->mOutputs.size() > 0) {
+      for (auto tensor : u->mOutputs) {
+        MNN_PRINT("%d,%d,%d,%d@%p;", tensor->batch(), tensor->channel(),
+                  tensor->height(), tensor->width(), tensor);
+      }
+    } else {
+      MNN_PRINT("none");
+    }
+    MNN_PRINT(" "); // Parents
+    if (u->parents.size() > 0) {
+      for (auto p : u->parents) {
+        MNN_PRINT("%ld;", p);
+      }
+    } else {
+      MNN_PRINT("none");
     }
-    return mExecution->valid();
+    MNN_PRINT(" "); // Children
+    if (u->childs.size() > 0) {
+      for (auto p : u->childs) {
+        MNN_PRINT("%ld;", p);
+      }
+    } else {
+      MNN_PRINT("none");
+    }
+    
+    MNN_PRINT("\n");
+    i++;
+  }
+  return MNN::ErrorCode::NO_ERROR;
 }
 
-ErrorCode Pipeline::Unit::execute() {
-    if (nullptr == mExecution) {
-        return NO_EXECUTION;
+ErrorCode Pipeline::prepare() {
+  /* random assign device to op */
+  // srand (time(NULL));
+  // for(auto opDevice: mDevice_map) {
+  // MNN_PRINT("%s\t%lu\n", opDevice.first.c_str(), opDevice.second);
+  //}
+  // getDeviceMap("/data/local/tmp/mDevice_map-2-thread.txt");
+  if (this->executionType ==
+                   MNNExecutionType::MNN_EXECUTION_PARALLEL ||
+               this->executionType ==
+                   MNNExecutionType::MNN_EXECUTION_SERIAL_HYBRID ||
+                this->executionType == MNNExecutionType::MNN_EXECUTION_BIG_LITTLE) {
+    MNN_PRINT("In Pipeline.cpp Pipeline::prepare deviceMapFilePath: %s\n",
+            deviceMapFilePath);
+    getDeviceMap(this->deviceMapFilePath);
+  }
+
+  mBackend->onResizeBegin();
+  mBackupBackend->onResizeBegin();
+  int opDeviceCount = 0;
+  for (auto& u : mUnits) {
+    int deviceType = -1;
+    if (this->executionType != MNNExecutionType::MNN_EXECUTION_DEFAULT) {
+      if (this->mDevice_map.find(u->name()) != mDevice_map.end()) {
+        deviceType = mDevice_map[u->name()];
+        opDeviceCount++;
+        // MNN_PRINT("Get %s %d form devicemap\n", u->name().c_str(), deviceType);
+      }else{
+        // MNN_PRINT("Can not find device for %s form devicemap\n", u->name().c_str());
+      }
     }
-    if (mConst) {
-        return NO_ERROR;
+    ErrorCode code;
+    if (this->executionType == MNNExecutionType::MNN_EXECUTION_DEFAULT) {
+      code = u->prepare(mBackend,
+                        mBackupBackend);  // Pass pipeline backend to each unit
+    } else if (this->executionType ==
+                   MNNExecutionType::MNN_EXECUTION_PARALLEL ||
+               this->executionType ==
+                   MNNExecutionType::MNN_EXECUTION_SERIAL_HYBRID) {
+      if ((deviceType == -1 || deviceType == 3) &&
+          mBackend->isSupportOpType(
+              u->mOriginOp)) {  // If deviceType not set or set to GPU, Set to
+                                // GPU if Backend support
+        code = u->prepare(
+            mBackend, mBackupBackend);  // Pass pipeline backend to each unit
+      } else {                          // Set to CPU by default
+        code =
+            u->prepare(mBackupBackend,
+                       mBackupBackend);  // Pass pipeline backend to each unit
+      }
+    } else if (this->executionType == MNNExecutionType::MNN_EXECUTION_BIG_LITTLE) {
+      /**
+       * big cluster use mBackend
+       * little cluster use mBackupBackend
+      */
+      printf("deviceType %d\n", deviceType);
+      u->pipelineExecutionType=MNNExecutionType::MNN_EXECUTION_BIG_LITTLE;
+      if(deviceType == MNNCpuClusterType::MNN_CPU_BIG) {
+        
+        code = u->prepare(mBackend, mBackupBackend);
+        u->CPUClusterType = MNNCpuClusterType::MNN_CPU_BIG;
+      } else if (deviceType == MNNCpuClusterType::MNN_CPU_LITTLE) {
+        code = u->prepare(mBackupBackend, mBackupBackend);
+        u->CPUClusterType = MNNCpuClusterType::MNN_CPU_LITTLE;
+      }
     }
-    auto code = mExecution->onExecute(mInputs, mOutputs);
+    // MNN_PRINT("%s %s\n", u->name().c_str(), printInputsOutputs(u).c_str());
     if (NO_ERROR != code) {
-        MNN_ERROR("Execute Error for %s, code=%d\n", mContent->name.c_str(), code);
+      if (nullptr != u->mOriginOp->name()) {
+        MNN_ERROR("Resize error for %s, code=%d\n",
+                  u->mOriginOp->name()->c_str(), code);
+      }
+      return code;
     }
-    return code;
+  }
+  MNN_PRINT("Total %d ops in net, %d ops have device map\n", mUnits.size(), opDeviceCount);
+  printf("mBackupBackend numThread %d\n", ((CPUBackend*)mBackupBackend)->threadNumber());
+  // if (isp)
+  // reorderPnasnetUnits();
+  if (this->executionType == MNNExecutionType::MNN_EXECUTION_PARALLEL) {
+      setUnitsOrder();
+  }
+  setupRelationshipForUnits();
+  // printOpRelationship();
+  
+  mBackend->onResizeEnd();
+  mBackupBackend->onExecuteEnd();
+  return NO_ERROR;
 }
-ErrorCode Pipeline::Unit::executeCallBack(const TensorCallBackWithInfo& before, const TensorCallBackWithInfo& after) {
-    if (nullptr == mExecution) {
-        return NO_EXECUTION;
-    }
-    if (mConst) {
-        return NO_ERROR;
-    }
-    auto run = before(mInputs, this);
-    if (run) {
-        auto code = mExecution->onExecute(mInputs, mOutputs);
-        if (NO_ERROR != code) {
-            MNN_ERROR("Execute Error for %s, code=%d\n", mContent->name.c_str(), code);
-            return code;
-        }
-    }
-    auto runOthers = after(mOutputs, this);
-    if (!runOthers) {
-        return CALL_BACK_STOP;
+
+ErrorCode Pipeline::execute() {
+  mBackend->onExecuteBegin();
+  MNN_PRINT("In Pipline::execute mUnits size: %lu\n", mUnits.size());
+  for (auto& u : mUnits) {
+    auto code = u->execute();
+    if (code != NO_ERROR) {
+      mBackend->onExecuteEnd();
+      return code;
     }
-    return NO_ERROR;
+  }
+  mBackend->onExecuteEnd();
+  return NO_ERROR;
 }
+int bigCoreIndex = getBigCoreIndex();
 
-ErrorCode Pipeline::Unit::prepare(Backend* bn, Backend* cpuBn) {
-    for (auto t : mInputs) {
-        bool valid = true;
-        for (int i = 0; i < t->dimensions(); ++i) {
-            if (t->length(i) <= 0) {
-                valid = false;
-                break;
-            }
-        }
-        if (!valid) {
-            MNN_ERROR("The %s's input is not ready\n", mContent->name.c_str());
-            return COMPUTE_SIZE_ERROR;
-        }
+ErrorCode Pipeline::execute(RunMetadata* runMetadata) {
+  if (runMetadata == nullptr) {
+    return ErrorCode::NO_EXECUTION;
+  }
+  CallStats callStats;
+  mBackend->onExecuteBegin();
+  mBackupBackend->onExecuteBegin();
+  ((CPUBackend*)mBackupBackend)->clearDataTransBuff();
+  MNN_PRINT("In Pipline::execute mUnits size: %lu\n", mUnits.size());
+
+  if (mUnits.size() <= 0) {
+    return NO_EXECUTION;
+  }
+  long net_start_time = NowMicros();
+  assert(mUnits.size() > 0);
+
+  //const int threadNumber = ((CPUBackend*)mBackend)->threadNumber();
+  const int threadNumber = ((CPUBackend*)mBackupBackend)->threadNumber();
+  
+  const char* p_env = std::getenv("MNN_LITTLE_CORES");
+    if(p_env !=nullptr && strcmp("true", p_env) == 0){
+        bigCoreIndex = 0;
     }
-    {
-        auto success = _allocTensors(bn, mInputs);
-        if (!success) {
-            return OUT_OF_MEMORY;
-        }
+  const int numOfCPU = getNumberOfCPU();
+  auto CPUCoreListPtr = std::make_shared<std::vector<int>>();
+  for(int i=bigCoreIndex; i<std::min(numOfCPU, bigCoreIndex+threadNumber); ++i){
+    CPUCoreListPtr->push_back(i);
+  }
+  MNN_PRINT("threadNumber %d\n", threadNumber);
+  setCPUAffinity(CPUCoreListPtr.get());
+  int count = 0;
+  for (auto& u : mUnits) {
+    count++;
+    #ifdef LOG_VERBOSE
+    MNN_PRINT("count %d\n", count);
+    MNN_PRINT("opName %s\n", u->name().c_str());
+    #endif
+    auto forwardType = u->mExecution->backend()->type();
+    
+    #ifdef LOG_VERBOSE
+    MNN_PRINT("Tensor data format:");
+    for(auto tensor: u->mInputs){
+        MNN_PRINT("%d ", TensorUtils::getDescribe(tensor)->dimensionFormat ==
+        MNN_DATA_FORMAT_NC4HW4);
+    }MNN_PRINT("\n");
+    #endif
+    auto start = NowMicros();
+    std::string opName = u->name();
+    #ifdef LOG_VERBOSE
+    MNN_PRINT("opName %s\n", opName.c_str());
+    MNN_PRINT("Start execute %s type:%s on %s %lu\n", opName.c_str(),
+    u->type().c_str(),
+        (forwardType == MNN_FORWARD_CPU ? "CPU": "GPU"),
+        (start-net_start_time));
+    #endif
+    callStats.start_micros = (start - net_start_time);
+    MNN::ErrorCode code;
+    code = u->execute();
+    u->mExecution->backend()->onWaitFinish();
+    // To measure the OpenCL time correctly,
+    // we need to force the Opencl command queue to finish
+    // This will greatly increase the overall inference latency.
+    // if(forwardType != MNN_FORWARD_CPU){
+    //     if(u->mType == OpType_Convolution
+    //         || u->mType == OpType_ConvolutionDepthwise
+    //         || u->mType == OpType_Concat){
+    //             MNN_PRINT("OpenCL onWaitFinish\n");
+    //             mBackend->onWaitFinish();
+    //         }
+    // }
+    // Execute pooling multi times on GPU OpType_Pooling
+    // if(forwardType != MNN_FORWARD_CPU && u->mType == OpType::OpType_Pooling
+    //     // && u->name().find("Branch_3") != std::string::npos
+    //     && u->name().find("AvgPool") != std::string::npos) {
+    //     int repeat = 20;
+    //     MNN_PRINT("Start execute %s %d times\n", u->name().c_str(), repeat);
+    //     for(int j=0;j<repeat;j++) {
+    //         code = u->execute();
+    //     }
+    // }
+    // // Execute convolution multi times on CPU
+    // if(forwardType == MNN_FORWARD_CPU && u->mType ==
+    // OpType::OpType_Convolution
+    //     && u->name().find("Branch_2")  != std::string::npos
+    //     && u->name().find("3x3") != std::string::npos) {
+    //     int repeat = 10;
+    //     MNN_PRINT("Start execute %s %d times\n", u->name().c_str(), repeat);
+    //     for(int j=0;j<repeat;j++) {
+    //         code = u->execute();
+    //     }
+    // }
+    // For now ignore the errors.
+    // if (code != NO_ERROR) {
+    //     mBackend->onExecuteEnd();
+    //     return code;
+    // }
+    auto end = NowMicros();
+    callStats.end_micros = (end - net_start_time);
+    #ifdef LOG_VERBOSE
+    MNN_PRINT("End execute %s on %s %lu %lu %lu\n", opName.c_str(),
+        (forwardType == MNN_FORWARD_CPU ? "CPU": "GPU"),
+        (end-start), (start-net_start_time), (end-net_start_time));
+    #endif
+
+    std::vector<std::vector<int>> output_shapes;
+    for (auto outputTensor : u->mOutputs) {
+      output_shapes.push_back(outputTensor->shape());
     }
-    bool ready = SizeComputer::computeOutputSize(mOriginOp, mInputs, mOutputs);
-    for (auto o : mOutputs) {
-        if (o->size() <= 0) {
-            ready = false;
-        }
-        if (o->dimensions() < 4 && TensorUtils::getDescribe(o)->dimensionFormat == MNN_DATA_FORMAT_NC4HW4) {
-            for (auto index = o->dimensions(); index < 4; ++index) {
-                o->setLength(index, 1);
-            }
-        }
+    OperatorStats op_stats = {opName, u->type(), forwardType, output_shapes,
+                              //{-1, -1, -1, -1, -1},
+                              callStats};
+    runMetadata->op_stats.push_back(op_stats);
+    if(count == mUnits.size()){
+      break;
     }
-    mContent->flops = SizeComputer::computeFlops(mOriginOp, mInputs, mOutputs);
+  }
+  mBackend->onExecuteEnd();
+  return NO_ERROR;
+}
 
-#ifdef MNN_DEBUG_TENSOR_SIZE
-    MNN_PRINT("\n===> compute shape: %s, [%d]\n", mOriginOp->name()->c_str(), mOriginOp->type());
-    if (mInputs.size()) {
-        MNN_PRINT("Inputs:\n");
-        for (auto o : mInputs) {
-            if (o->dimensions() == 0) {
-                MNN_PRINT("\t*Scalar*");
-            }
-            for (int i = 0; i < o->dimensions(); ++i) {
-                MNN_PRINT("%d, ", o->length(i));
-            }
-            MNN_PRINT("\n");
-        }
+/**
+ * Check whether the unit is ready to execute
+ */
+inline bool readyToExecute(std::vector<std::shared_ptr<MNN::Pipeline::Unit>>* units,
+                    std::shared_ptr<MNN::Pipeline::Unit> unit) {
+  for (uint64_t p : unit->parents) {
+    if (p >= units->size()) {
+      MNN_PRINT(
+          "Error: In Pipeline.cpp readyToExecute, Unit parent index %lu "
+          "exceeds the pipeline mUnits size %lu\n",
+          p, units->size());
+      return false;
     }
-    MNN_PRINT("Outputs:\n");
-    for (auto o : mOutputs) {
-        if (o->dimensions() == 0) {
-            MNN_PRINT("\t*Scalar*");
-        }
-        for (int i = 0; i < o->dimensions(); ++i) {
-            MNN_PRINT("%d, ", o->length(i));
-        }
-        MNN_PRINT("\n");
-    }
-#endif
-    if (!ready) {
-        return COMPUTE_SIZE_ERROR;
+
+    auto unitParent = units->at(p);
+    if (!unitParent->mExecuted) {
+      if (unit->name().compare("InceptionV3/InceptionV3/Mixed_6a/concat") ==
+          0) {
+        printf("concatparents %s not executed\n", unitParent->name().c_str());
+      }
+      return false;
     }
+  }
+  return true;
+}
 
-    // Check const
-    mConst = true;
-    for (int i = 0; i < mInputs.size(); ++i) {
-        if (SizeComputer::opNeedContent(mOriginOp->type(), i) && (!TensorUtils::getDescribe(mInputs[i])->isConst)) {
-            mConst = false;
-            break;
-        }
+struct RunNetStructure {
+  RunMetadata* run_metadata;
+  std::vector<std::shared_ptr<MNN::Pipeline::Unit>>* deviceUnits;
+  std::vector<std::shared_ptr<MNN::Pipeline::Unit>>* units;
+  long* netStartTime;
+  std::queue<std::shared_ptr<MNN::Pipeline::Unit>>* CPUWaitList;
+  std::queue<std::shared_ptr<MNN::Pipeline::Unit>>* GPUWaitList;
+  std::vector<int>* CPUCoreList;
+};
+
+pthread_mutex_t fastmutex = PTHREAD_MUTEX_INITIALIZER;
+pthread_cond_t fastcond = PTHREAD_COND_INITIALIZER;
+
+void* runUnits(void* rns) {
+  // Get args
+  RunNetStructure* runNetStructure = (RunNetStructure*)rns;
+  MNN::RunMetadata* run_metadata = runNetStructure->run_metadata;
+  std::vector<std::shared_ptr<MNN::Pipeline::Unit>>* deviceUnits =
+      runNetStructure->deviceUnits;
+  std::vector<std::shared_ptr<MNN::Pipeline::Unit>>* mUnits =
+      runNetStructure->units;
+  long* netStartTime = runNetStructure->netStartTime;
+  std::queue<std::shared_ptr<MNN::Pipeline::Unit>>* CPUWaitList =
+      runNetStructure->CPUWaitList;
+  std::queue<std::shared_ptr<MNN::Pipeline::Unit>>* GPUWaitList =
+      runNetStructure->GPUWaitList;
+  std::vector<int> *CPUCoreList = runNetStructure->CPUCoreList;
+  
+  if (deviceUnits->size() <= 0 || run_metadata == nullptr) {
+    return (void*)ErrorCode::NO_EXECUTION;
+  }
+
+  const int numOfUnits = deviceUnits->size();
+  // Get forward type
+  // auto firstUnit = deviceUnits->at(0);
+  auto deviceForwardType = deviceUnits->at(0)->mExecution->backend()->type();
+  auto backend = deviceUnits->at(0)->mExecution->backend();
+  
+  bool isCPU = true;
+  std::string forwardTypeStr;
+  if (deviceForwardType == MNN_FORWARD_OPENCL) {
+    // backend->netStartTime = *netStartTime;
+    forwardTypeStr = "OpenCL";
+    isCPU = false;
+    assert(CPUCoreList->size()>0);
+    cpu_set_t cpusetOfCPU;
+    CPU_ZERO(&cpusetOfCPU);
+    CPU_SET(CPUCoreList->at(0), &cpusetOfCPU);
+    pid_t pid = pthread_gettid_np(pthread_self());
+    int rc = 1;
+    int attemptCount = 100;
+    while(rc != 0 && attemptCount--) {
+        rc = sched_setaffinity(pid, sizeof(cpusetOfCPU), &cpusetOfCPU);
     }
-    if (mType == OpType_TrainableParam) {
-        for (auto t : mOutputs) {
-            TensorUtils::getDescribe(t)->isTrainableParameter = true;
-        }
-        mConst = false;
+    MNN_PRINT("GPU->RunUnits set tId %d pid %d set-on-core %d real-on-core %d ret %d\n", CPUCoreList->at(0), pid, CPUCoreList->at(0), sched_getcpu(), rc);
+  } else {
+    forwardTypeStr = "CPU";
+    const int threadNumber = CPUCoreList->size();
+    omp_set_num_threads(threadNumber);
+    MNN_CONCURRENCY_BEGIN(tId, threadNumber) {
+    cpu_set_t cpusetOfCPU;
+    CPU_ZERO(&cpusetOfCPU);
+    CPU_SET(CPUCoreList->at(tId), &cpusetOfCPU);
+    pid_t pid = pthread_gettid_np(pthread_self());
+    int rc = 1;
+    int attemptCount = 100;
+    while(rc != 0 && attemptCount--) {
+        rc = sched_setaffinity(pid, sizeof(cpusetOfCPU), &cpusetOfCPU);
     }
-
-    if (mConst) {
-        for (auto t : mOutputs) {
-            TensorUtils::getDescribe(t)->isConst = true;
-        }
-        bn = cpuBn;
+    MNN_PRINT("CPU->RunUnits set tId %d pid %d set-on-core %d real-on-core %d ret %d\n", tId, pid, CPUCoreList->at(tId), sched_getcpu(), rc);
     }
+    MNN_CONCURRENCY_END();
+  }
+  MNN_PRINT("Start execute on %s units size %d coreId %d\n",
+            forwardTypeStr.c_str(), numOfUnits, sched_getcpu());
+  MNN_PRINT("OPM_NUM_THREADS: %d\n", omp_get_num_threads());
+  // Debug for Concat
+  // if(forwardType != MNN_FORWARD_CPU){
+  //     firstGPUUnit = firstUnit;
+  // }
+  int executedUnitsCount = numOfUnits;
+  std::vector<bool> executed(numOfUnits);
+  for(int i=0; i<numOfUnits; ++i){
+    executed[i] = false;
+  }
+  int countClFinish = 0;
+  // Main execution loop
+  while (executedUnitsCount--) {
+    
+    std::shared_ptr<MNN::Pipeline::Unit> unit;
+    // Try to find one unit to execute, if not, wait
+    // bool findReadyUnit = false;
+    // auto fetchUnitFromWaitList =
+    //     [&findReadyUnit, mUnits,
+    //      &unit](std::queue<std::shared_ptr<MNN::Pipeline::Unit>>* waitList) {
+    //       if (waitList != nullptr && !waitList->empty()) {
+    //         auto tmpUnit = waitList->front();
+    //         if (!tmpUnit->mDispatched && readyToExecute(mUnits, tmpUnit)) {
+    //           waitList->pop();
+    //           findReadyUnit = true;
+    //           unit = tmpUnit;
+    //           unit->mDispatched = true;
+    //         }
+    //       }
+    //     };
+    // while (true) {
+    //   // Check wait list
+    //   if (deviceForwardType == MNN_FORWARD_CPU) {
+    //     fetchUnitFromWaitList(GPUWaitList);
+    //   } else {
+    //     fetchUnitFromWaitList(CPUWaitList);
+    //   }
+    //   if (findReadyUnit) {
+    //     break;
+    //   }
+    //   // Try to fetch one ready unit
+    //   if (!findReadyUnit) {
+    //     for (auto tmpUnit : *deviceUnits) {
+    //       if (!tmpUnit->mDispatched && !tmpUnit->mExecuted) {
+    //         if (readyToExecute(mUnits, tmpUnit)) {
+    //           findReadyUnit = true;
+    //           unit = tmpUnit;
+    //           unit->mDispatched = true;
+    //           break;
+    //         } else {
+    //           // To reduce the runtime scheduling overhead,
+    //           // if waitList has more than 3 units to be execute, do not add new unit into it
+    //           const int waitListThershold = 3;
+    //           if(deviceForwardType == MNN_FORWARD_CPU && CPUWaitList->size() > waitListThershold){
+    //             continue;
+    //           }
+    //           if(deviceForwardType == MNN_FORWARD_OPENCL && GPUWaitList->size() > waitListThershold){
+    //             continue;
+    //           }
+    //           for (auto parentIndex : tmpUnit->parents) {
+    //             assert(parentIndex < mUnits->size());
+    //             auto parentUnit = mUnits->at(parentIndex);
+    //             // Wait parent to be dispatched that are scheduled on other
+    //             // device
+    //             if (!parentUnit->mDispatched && !parentUnit->mAddedToWaitList &&
+    //                 parentUnit->mForwardType != deviceForwardType) {
+    //               if (deviceForwardType == MNN_FORWARD_CPU) {
+    //                 parentUnit->mAddedToWaitList = true;
+    //                 CPUWaitList->push(parentUnit);
+    //               } else {
+    //                 parentUnit->mAddedToWaitList = true;
+    //                 GPUWaitList->push(parentUnit);
+    //               }
+    //             }
+    //           }
+    //         }
+    //       }
+    //     }
+    //   }
+    //   if (!findReadyUnit) {
+    //     // Before sleep, notify other thread to check whether there exists ops
+    //     // than are ready to execute
+    //     pthread_cond_broadcast(&fastcond);
+    //     // MNN_PRINT("%s wait %lu\n", forwardTypeStr.c_str(), NowMicros()-*netStartTime);
+    //     pthread_mutex_lock(&fastmutex);
+    //     pthread_cond_wait(&fastcond, &fastmutex);
+    //     pthread_mutex_unlock(&fastmutex);
+    //     // MNN_PRINT("%s notified %lu\n", forwardTypeStr.c_str(), NowMicros()-*netStartTime);
+    //   } else {
+    //     break;
+    //   }
+    // }
 
-    // Create or Resize execution
-    if (nullptr == mExecution) {
-        auto sucess = _createExecution(bn, cpuBn);
-        if (!sucess || mExecution == nullptr) {
-            return NOT_SUPPORT;
+    while(true){
+      bool findReadyUnit = false;
+      std::string opName;
+      int count = 0;
+      for(int i =0;i<deviceUnits->size(); i++){
+        if(executed[i]){
+          continue;
         }
-    }
-    bn = mExecution->backend();
-    {
-        auto success = _allocTensors(bn, mOutputs);
-        if (!success) {
-            return OUT_OF_MEMORY;
+        count++;
+        if(count>20){
+          break;
         }
-    }
-    auto code = mExecution->onResize(mInputs, mOutputs);
-    if (TENSOR_NOT_SUPPORT == code || TENSOR_NEED_DIVIDE == code) {
-        // TODO
-        mExecution.reset();
-        for (auto t : mOutputs) {
-            auto des = TensorUtils::getDescribe(t);
-            des->backend->onReleaseBuffer(t, _getTensorReleaseStorageType(t));
-            des->backend = nullptr;
+        auto u = (*deviceUnits)[i];
+        opName = u->name();
+        if(readyToExecute(mUnits, u)){
+          findReadyUnit = true;
+          executed[i] = true;
+          unit = u;
+          break;
         }
-        auto sucess = _createExecution(cpuBn, cpuBn);
-        MNN_ASSERT(NO_ERROR == sucess);
-        auto success = _allocTensors(mExecution->backend(), mOutputs);
-        if (!success) {
-            return OUT_OF_MEMORY;
+      }
+      if(findReadyUnit){
+        break;
+      } else {
+        // Before sleep, notify other thread to check whether there exists ops
+        // than are ready to execute
+        pthread_cond_broadcast(&fastcond);
+        pthread_mutex_lock(&fastmutex);
+        //MNN_PRINT("%s on %s wait %lu\n",opName.c_str(), forwardTypeStr.c_str(), NowMicros()-*netStartTime);
+        pthread_cond_wait(&fastcond, &fastmutex);
+        pthread_mutex_unlock(&fastmutex);
+        // MNN_PRINT("%s notified %lu\n", forwardTypeStr.c_str(), NowMicros()-*netStartTime);
+      }
+    }
+    
+    // Execute unit
+    deviceForwardType = unit->mExecution->backend()->type();
+    if(unit->CPUClusterType == MNNCpuClusterType::MNN_CPU_LITTLE){
+      deviceForwardType = MNNForwardType::MNN_FORWARD_OPENGL;
+    }
+    MNN::ErrorCode code = MNN::NO_ERROR;
+    auto start = (MNN::NowMicros());
+    
+    // MNN_PRINT("%s unit pid %d execute on core %d\n",forwardTypeStr.c_str(), pthread_gettid_np(pthread_self()), sched_getcpu());
+    unit->netStartTime = *netStartTime;
+    // unit->mExecution->setNetStartTime(*netStartTime);
+    // MNN_PRINT("Start execute -> %s on %s %lu type %s\n", unit->name().c_str(),
+    //   forwardTypeStr.c_str(), (start-*netStartTime), unit->type().c_str());
+    code = unit->execute();
+    // unit->mExecution->backend()->onWaitFinish();
+    // Do not call clFinish util one of it's successor is placed on CPU
+    if (deviceForwardType == MNN_FORWARD_OPENCL) {
+      bool isSuccessorOnCPU = false;
+      for(auto childIdx: unit->childs){
+        if(((*mUnits)[childIdx])->mExecution->backend()->type()==MNN_FORWARD_CPU){
+          isSuccessorOnCPU = true;
+          break;
         }
-        code = mExecution->onResize(mInputs, mOutputs);
+        // MNN_PRINT("%d ", childIdx);
+      }
+      if(isSuccessorOnCPU){
+        //MNN_PRINT("%s has child on CPU, call clFinish\n", unit->name().c_str());
+        unit->mExecution->backend()->onWaitFinish();
+        countClFinish++;
+      }
+      //MNN_PRINT("\n");
     }
-    if (NO_ERROR != code) {
-        mExecution.reset();
-        return code;
+    // int rc = unit->mExecution->backend()->onWaitFinish();
+    
+    unit->mExecuted = true;
+    auto end = (MNN::NowMicros());
+    auto opName = unit->name();
+    while(opName.empty()) {
+      MNN_PRINT("opName empty\n");
+      opName = unit->name();
     }
-    if (mConst) {
-        code = mExecution->onExecute(mInputs, mOutputs);
+    
+    // Deal with WrapExecution profiling
+    if(unit->isWrapExecution) {
+      // Record data transformation here here
+      MNN::CallStats callStats = {
+        (start - *netStartTime),
+        (unit->mExecution->mStartTimeStamp - *netStartTime)};
+      std::vector<std::vector<int>> output_shapes;
+      for (auto outputTensor : unit->mInputs) {
+        output_shapes.push_back(outputTensor->shape());
+      }
+      MNN::OperatorStats op_stats = {opName+"/Convert", unit->type(),
+                                    MNNForwardType::MNN_FORWARD_CONVERT, output_shapes, 
+                                    {(start - *netStartTime),
+                                    (unit->mExecution->mStartTimeStamp - *netStartTime)}};
+      pthread_mutex_lock(&fastmutex);
+      run_metadata->op_stats.push_back(op_stats);
+      pthread_mutex_unlock(&fastmutex);
+      // MNN_PRINT("End executeConvert %s on %s %lu %lu %lu %s\n", opName.c_str(), forwardTypeStr.c_str(),
+      //   (unit->mExecution->mStartTimeStamp - start),
+      //   (start - *netStartTime),
+      //   (unit->mExecution->mStartTimeStamp - *netStartTime),
+      //   unit->type().c_str());
+        // Modify real execution latency
+        start = unit->mExecution->mStartTimeStamp;
     }
+    
+    // MNN_PRINT("End execute %s on %s %lu %lu %lu %s rc %d type %s\n", opName.c_str(), forwardTypeStr.c_str(),
+    //     (end-start), (start-*netStartTime), (end-*netStartTime),
+    //     unit->type().c_str(), rc, unit->type().c_str());
 
-    for (auto t : mInputs) {
-        auto des = TensorUtils::getDescribe(t);
-        des->useCount -= 1;
-        if (0 == des->useCount) {
-            des->backend->onReleaseBuffer(t, _getTensorReleaseStorageType(t));
-        }
+    // Notify another thread
+    pthread_cond_broadcast(&fastcond);
+    // MNN_PRINT("%s notify other done\n", unit->name().c_str());
+    if (code != MNN::NO_ERROR) {
+      printf("Execute %s error, code: %d", unit->name().c_str(), code);
+      // return (void*)code;
     }
-    return code;
-}
-
-Pipeline::Pipeline(const std::vector<Schedule::PipelineInfo>& infos, Backend* backend, Backend* cpuBackend) {
-    SizeComputerSuite::init();
-    MNN_ASSERT(nullptr != backend);
-    MNN_ASSERT(nullptr != cpuBackend);
-    mBackupBackend = cpuBackend;
-    mBackend       = backend;
 
-    for (auto& info : infos) {
-        std::shared_ptr<Unit> unit(new Unit(info.op, info.inputs, info.outputs));
-        mUnits.emplace_back(unit);
+    // statistics unit info
+    MNN::CallStats callStats = {(start - *netStartTime),
+        (end - *netStartTime)};
+    std::vector<std::vector<int>> output_shapes;
+    for (auto outputTensor : unit->mOutputs) {
+      output_shapes.push_back(outputTensor->shape());
     }
+    MNN::OperatorStats op_stats = {unit->name(), unit->type(),
+                                   deviceForwardType, output_shapes, 
+                                   {(start - *netStartTime),
+                                   (end - *netStartTime)}};
+    pthread_mutex_lock(&fastmutex);
+    run_metadata->op_stats.push_back(op_stats);
+    pthread_mutex_unlock(&fastmutex);
+  } // Main execution loop end
+  if(!isCPU){
+    MNN_PRINT("On GPU call clFinish() %d times, saved %d times\n", countClFinish, deviceUnits->size() - countClFinish);
+  }
+  
+
+  return (void*)MNN::NO_ERROR;
 }
 
-ErrorCode Pipeline::prepare() {
-    mBackend->onResizeBegin();
-    for (auto& u : mUnits) {
-        auto code = u->prepare(mBackend, mBackupBackend);
-        if (NO_ERROR != code) {
-            if (nullptr != u->mOriginOp->name()) {
-                MNN_ERROR("Resize error for %s, code=%d\n", u->mOriginOp->name()->c_str(), code);
-            }
-            return code;
-        }
+MNN::ErrorCode MNN::Pipeline::executeInParallel(RunMetadata* runMetadata) {
+  if (runMetadata == nullptr) {
+    return ErrorCode::NO_EXECUTION;
+  }
+  
+  std::vector<std::shared_ptr<MNN::Pipeline::Unit>> CPUUnits, GPUUnits;
+  for (auto u : mUnits) {
+    // Mark the unit as not executed
+    u->mExecuted = false;
+    u->mDispatched = false;
+    auto forwardType = u->mExecution->backend()->type();
+    if (forwardType == MNN_FORWARD_CPU) {
+      // MNN_PRINT("%s cluster type %d\n", u->name().c_str(), u->CPUClusterType);
+      u->mForwardType = MNN_FORWARD_CPU;
+      if (u->CPUClusterType == MNNCpuClusterType::MNN_CPU_BIG){
+        CPUUnits.push_back(u);
+      } else if (u->CPUClusterType == MNNCpuClusterType::MNN_CPU_LITTLE) {
+        GPUUnits.push_back(u);
+      }
+    } else if (forwardType == MNN_FORWARD_OPENCL ||
+               forwardType == MNN_FORWARD_OPENGL ||
+               forwardType == MNN_FORWARD_VULKAN) {
+      u->mForwardType = MNN_FORWARD_OPENCL;
+      GPUUnits.push_back(u);
+    } 
+    else {
+      return MNN::ErrorCode::NO_EXECUTION;
     }
-    mBackend->onResizeEnd();
-    return NO_ERROR;
-}
+  } // End for
 
-ErrorCode Pipeline::execute() {
-    mBackend->onExecuteBegin();
-    for (int i=0; i<mUnits.size(); ++i) {
-        auto& u = mUnits[i];
-        auto code = u->execute();
-        if (code != NO_ERROR) {
-            mBackend->onExecuteEnd();
-            return code;
-        }
+  std::queue<std::shared_ptr<MNN::Pipeline::Unit>> CPUWaitList;
+  std::queue<std::shared_ptr<MNN::Pipeline::Unit>> GPUWaitList;
+  MNN_PRINT(
+      "In Pipline::executeInParallel mUnits size: %lu, CPU units: %lu, GPU "
+      "units %lu\n",
+      mUnits.size(), CPUUnits.size(), GPUUnits.size());
+  mBackend->onExecuteBegin();
+  mBackupBackend->onExecuteBegin();
+  
+  // for(auto u: CPUUnits){
+  //   MNN_PRINT("CPU %s\n", u->name().c_str());
+  // }
+  // for(auto u: GPUUnits){
+  //   MNN_PRINT("GPU %s\n", u->name().c_str());
+  // }
+  // Set Core Index for CPU and GPU
+  std::vector<int> CPUCoreList, GPUCoreList;
+  const int numOfCPUCores = getNumberOfCPU();
+  
+  MNN_PRINT("numberOfCPU %d bigCoreIndex %d\n", numOfCPUCores, bigCoreIndex);
+  if (this->executionType == MNNExecutionType::MNN_EXECUTION_BIG_LITTLE){
+    // In symmetric CPUs, set bigCoreIndex start at #Cores/2
+    if(bigCoreIndex == 0){
+      bigCoreIndex = numOfCPUCores / 2;
     }
-    mBackend->onExecuteEnd();
-    return NO_ERROR;
+    // Set big core affinity
+    const int backendThreadNum = ((CPUBackend*)mBackend)->threadNumber();
+    if(bigCoreIndex + backendThreadNum > numOfCPUCores){
+      MNN_ERROR("Their are not enough big cores\n");
+    }
+    for(int coreIdx = bigCoreIndex; coreIdx<std::min(numOfCPUCores, bigCoreIndex + backendThreadNum); coreIdx++){
+        CPUCoreList.push_back(coreIdx);
+    }
+    // Set little core affinity
+    const int backupBackendThreadNum = ((CPUBackend*)mBackupBackend)->threadNumber();
+    if(backendThreadNum > bigCoreIndex){
+      MNN_ERROR("Their are not enough little cores\n");
+    }
+    // Little cores always start from 0
+    for(int coreIdx = 0; coreIdx < std::min(bigCoreIndex, backupBackendThreadNum); coreIdx++){
+      GPUCoreList.push_back(coreIdx);
+    }
+  } else if(this->executionType == MNNExecutionType::MNN_EXECUTION_PARALLEL) {
+    const int backupBackendThreadNum = ((CPUBackend*)mBackupBackend)->threadNumber();
+    for(int coreIdx = bigCoreIndex; coreIdx<std::min(numOfCPUCores, bigCoreIndex + backupBackendThreadNum); coreIdx++){
+        CPUCoreList.push_back(coreIdx);
+    }
+    // Attempt to assign the GPU thread to big cores
+    // GPUCoreList.push_back(bigCoreIndex);
+
+    if(bigCoreIndex + backupBackendThreadNum < numOfCPUCores){
+        GPUCoreList.push_back((bigCoreIndex + backupBackendThreadNum+1) % numOfCPUCores);
+        // GPUCoreList.push_back(bigCoreIndex);
+    }else if(bigCoreIndex > 0){
+        GPUCoreList.push_back(bigCoreIndex-1);
+    }else{
+      GPUCoreList.push_back((bigCoreIndex) % numOfCPUCores);
+    }
+  }
+  // Print Core list
+  for(auto c: CPUCoreList){
+    MNN_PRINT("%d ", c);
+  }MNN_PRINT("\n");
+  for(auto c: GPUCoreList){
+    MNN_PRINT("%d ", c);
+  }MNN_PRINT("\n");
+  pthread_t CPUThread, GPUThread;
+  long netStartTime = MNN::NowMicros();
+  RunNetStructure CPURunStructure{runMetadata, &CPUUnits, &mUnits,
+                                  &netStartTime, &CPUWaitList, &GPUWaitList, &CPUCoreList};
+  RunNetStructure GPURunStructure{runMetadata, &GPUUnits, &mUnits,
+                                  &netStartTime, &CPUWaitList, &GPUWaitList, &GPUCoreList};
+  pthread_create(&CPUThread, NULL, runUnits, (void*)&CPURunStructure);
+  pthread_create(&GPUThread, NULL, runUnits, (void*)&GPURunStructure);
+  
+  // Use the largest # of threadNumber() cores for GPU thread
+  // {
+  //   cpu_set_t cpuset;
+  //   CPU_ZERO(&cpuset);
+  //   CPU_SET((bigCoreIndex + ((CPUBackend*)mBackupBackend)->threadNumber()) % numOfCPUCores,
+  //           &cpuset);
+  //   pid_t pid = pthread_gettid_np(pthread_self());
+  //   int rc = -1;
+  //   int attemptCount = 100;
+  //   while(rc != 0 && attemptCount--) {
+  //     rc = sched_setaffinity(pid, sizeof(cpuset), &cpuset);
+  //   }
+  //   MNN_PRINT("GPU affinity %x pid is %d set-on-core %d real-on-core %d ret %d\n", cpuset, pid, 
+  //     (bigCoreIndex + ((CPUBackend*)mBackupBackend)->threadNumber()), sched_getcpu(), rc);
+  // }
+
+  pthread_join(CPUThread, NULL);
+  pthread_join(GPUThread, NULL);
+
+  mBackend->onExecuteEnd();
+  mBackupBackend->onExecuteEnd();
+  ((CPUBackend*)mBackupBackend)->clearDataTransBuff();
+  return NO_ERROR;
 }
 
-ErrorCode Pipeline::executeCallBack(const TensorCallBackWithInfo& before, const TensorCallBackWithInfo& after) {
-    mBackend->onExecuteBegin();
-    std::shared_ptr<char> __defer(nullptr, [this](void*) { mBackend->onExecuteEnd(); });
-    for (auto& u : mUnits) {
-        auto code = u->executeCallBack(before, after);
-        if (code != NO_ERROR) {
-            return code;
-        }
+ErrorCode Pipeline::executeCallBack(const TensorCallBackWithInfo& before,
+                                    const TensorCallBackWithInfo& after) {
+  mBackend->onExecuteBegin();
+  std::shared_ptr<char> __defer(nullptr,
+                                [this](void*) { mBackend->onExecuteEnd(); });
+  for (auto& u : mUnits) {
+    auto code = u->executeCallBack(before, after);
+    if (code != NO_ERROR) {
+      return code;
     }
-    return NO_ERROR;
+  }
+  return NO_ERROR;
 }
 
 ErrorCode Pipeline::releaseCache() {
-    for (auto& u : mUnits) {
-        if (nullptr != u->mExecution) {
-            auto code = u->mExecution->onReleaseCache();
-            if (NO_ERROR != code) {
-                MNN_ERROR("Error for release cache for %s\n", u->name().c_str());
-                return code;
-            }
-        }
+  for (auto& u : mUnits) {
+    if (nullptr != u->mExecution) {
+      auto code = u->mExecution->onReleaseCache();
+      if (NO_ERROR != code) {
+        MNN_ERROR("Error for release cache for %s\n", u->name().c_str());
+        return code;
+      }
     }
-    return NO_ERROR;
+  }
+  return NO_ERROR;
 }
 
-} // namespace MNN
+}  // namespace MNN
diff --git a/source/core/Pipeline.hpp b/source/core/Pipeline.hpp
index f4045fe..8ad6afc 100644
--- a/source/core/Pipeline.hpp
+++ b/source/core/Pipeline.hpp
@@ -8,12 +8,37 @@
 
 #ifndef Pipeline_hpp
 #define Pipeline_hpp
+#include <atomic>
+#include <fstream>
+#include <stdlib.h>     /* srand, rand */
+#include <time.h>       /* time */
+#include <sys/stat.h>
+#include <pthread.h>
+#include <sched.h>
+#include <unistd.h>
+#include <set>
 
-#include "core/Execution.hpp"
+#include "Execution.hpp"
 #include "Schedule.hpp"
+#include "MUtils.hpp"
+#include "core/Backend.hpp"
+#include "backend/cpu/CPUBackend.hpp"
 #include "MNN_generated.h"
+//#include "core/OpenCLBackend.hpp"
+
 
 namespace MNN {
+
+static bool fileExists(const char* filename)
+{
+    struct stat buf;
+    if (stat(filename, &buf) != -1)
+    {
+        return true;
+    }
+    return false;
+}
+
 struct OperatorInfo::Info {
     std::string name;
     std::string type;
@@ -42,6 +67,11 @@ public:
      * @return result code.
      */
     ErrorCode execute();
+    /**
+     * @brief execute all units.
+     * @return result code.
+     */
+    ErrorCode execute(RunMetadata *runMetadata);
     /**
      * @brief execute all units with callbacks.
      * @param before    callback before execute each op.
@@ -49,12 +79,41 @@ public:
      * @return result code.
      */
     ErrorCode executeCallBack(const TensorCallBackWithInfo& before, const TensorCallBackWithInfo& after);
+
+    /**
+     * Execute the pipeline in parallel
+    */
+    ErrorCode executeInParallel(RunMetadata *runMetadata);
+
+    /**
+     * Execute the units in pipeline with different backend
+     * 
+    */
+    // ErrorCode ExecuteInHybrid(RunMetadata *runMetadata);
+
     /**
      * @brief the Pipline need not prepare any more, release all cache used for resize.
      * @return errorcode
      */
     ErrorCode releaseCache();
 
+
+    ErrorCode getDeviceMap(const char *file_path);
+
+    ErrorCode setUnitsOrder();
+    
+    ErrorCode printOpRelationship();
+    /**
+    * Set up parents and childs for each op
+    */
+    ErrorCode setupRelationshipForUnits();
+    
+    /**
+     * @brief reorder the module execution order for pnasnet-mobile
+     * @return ErrorCode 
+     */
+    ErrorCode reorderPnasnetUnits();
+
     /** op unit in pipeline */
     class Unit : public NonCopyable, public OperatorInfo {
     public:
@@ -87,6 +146,7 @@ public:
     public:
         /** op execution */
         std::shared_ptr<Execution> mExecution;
+        Execution* mExecutionBak;
         /** op type*/
         OpType mType;
         /** input tensors */
@@ -95,6 +155,24 @@ public:
         std::vector<Tensor*> mOutputs;
         /** op */
         const Op* mOriginOp;
+        /** Parents and childs*/
+        std::set<int64_t> parents;
+        std::set<int64_t> childs;
+        /** Whether the unit has been executed*/
+        std::atomic_bool mExecuted;
+        /** Whether the unit has been scheduled*/
+        std::atomic_bool mDispatched;
+        /** Whether the unit has been put in the waitList queue*/
+        std::atomic_bool mAddedToWaitList;
+        /** Which device the unit are dispatched */
+        MNNForwardType mForwardType;
+        /** Used to help profile the data format transformation in WrapExecution */
+        long netStartTime = 0;
+        /** Used to find whether its execution is WrapExecution */
+        bool isWrapExecution = false;
+        // 0 for big, 1 for medium, 2 for little
+        MNNCpuClusterType CPUClusterType = MNNCpuClusterType::MNN_CPU_BIG;
+        MNNExecutionType pipelineExecutionType = MNNExecutionType::MNN_EXECUTION_DEFAULT;
 
     private:
         bool _createExecution(Backend* bn, Backend* cpuBn);
@@ -102,6 +180,7 @@ public:
 
     private:
         bool mConst                   = false;
+        const SizeComputer* mComputer = nullptr;
     };
 
 protected:
@@ -114,6 +193,12 @@ private:
     Backend* mBackend;
     Backend* mBackupBackend;
     std::vector<std::shared_ptr<Unit>> mUnits;
+    std::map<std::string, int64_t>mDevice_map;
+    std::vector<std::string>mOp_order;
+public:
+    MNNExecutionType executionType = MNNExecutionType::MNN_EXECUTION_DEFAULT;
+    char* deviceMapFilePath = nullptr;
+    bool isProfiling = false;
 };
 } // namespace MNN
 
diff --git a/source/core/Schedule.cpp b/source/core/Schedule.cpp
index 0eee037..9878b3c 100644
--- a/source/core/Schedule.cpp
+++ b/source/core/Schedule.cpp
@@ -333,7 +333,9 @@ Schedule::ScheduleInfo Schedule::schedule(const Net* net, const std::vector<Sche
     for (auto& config : configs) {
         Backend::Info compute;
         compute.type      = _getApprociateType(config, net, allTensors, valid);
+        compute.executionType = config.executionType;
         compute.numThread = config.numThread;
+        compute.CPULittleNumThread = config.CPULittleNumThread;
         compute.user      = config.backendConfig;
         auto oplists      = _scheduleUnit(net, config, allTensors);
         result.emplace_back(std::make_pair(compute, std::move(oplists)));
diff --git a/source/core/Schedule.hpp b/source/core/Schedule.hpp
index 794c5fb..bfd5be3 100644
--- a/source/core/Schedule.hpp
+++ b/source/core/Schedule.hpp
@@ -46,6 +46,8 @@ public:
         std::vector<std::pair<int, std::shared_ptr<Tensor>>> allTensors;
         /** input valid for resize*/
         bool validForResize;
+        /** Execution Type */
+        MNNExecutionType executionType;
     };
 
     /**
diff --git a/source/core/Session.cpp b/source/core/Session.cpp
index 67cd8ef..1a17f14 100644
--- a/source/core/Session.cpp
+++ b/source/core/Session.cpp
@@ -6,27 +6,35 @@
 //  Copyright  2018, Alibaba Group Holding Limited
 //
 
-#include "core/Session.hpp"
+#include "Session.hpp"
 #include <string.h>
 #include <map>
 #include <set>
-#include "core/AutoStorage.h"
-#include <MNN/AutoTime.hpp>
+#include <fstream>
+
+#include "AutoStorage.h"
+#include "MNN/AutoTime.hpp"
 #include "core/BackendFactory.hpp"
+#include "core/Backend.hpp"
+#include "backend/cpu/CPUBackend.hpp"
+#include "backend/cpu/compute/CommonOptFunction.h"
 #include "MNN_generated.h"
-#include "core/TensorUtils.hpp"
-#include "core/WrapExecution.hpp"
+#include "TensorUtils.hpp"
+#include "WrapExecution.hpp"
 
 using namespace std;
 
 namespace MNN {
 
-Backend* Session::_getDefaultBackend() {
+Backend* Session::_getDefaultBackend(int numThread) {
     auto defaultType = MNN_FORWARD_CPU;
     if (mBackends.find(defaultType) == mBackends.end()) {
         Backend::Info info;
         info.type      = defaultType;
-        info.numThread = 1;
+        info.numThread = numThread;
+        info.user = new MNN::BackendConfig;
+        info.user->memory = MNN::BackendConfig::Memory_High;
+        info.user->power = MNN::BackendConfig::Power_High;
         mBackends[info.type].reset(BackendFactory::create(info));
     }
     auto cpuBackend = mBackends.find(defaultType)->second.get();
@@ -47,10 +55,30 @@ Session::Session(const Schedule::ScheduleInfo& info) {
                 mValid = false;
                 return;
             }
+            // if (newBn->type() != MNN_FORWARD_CPU) {
+            //     newBn->onLoadLibrary(info.library);
+            // }
             mBackends[iter.first.type].reset(newBn);
         }
         auto backend    = mBackends.find(iter.first.type)->second.get();
-        auto cpuBackend = _getDefaultBackend();
+        int cpuBackendNumThread = iter.first.numThread;
+        this->executionType = iter.first.executionType;
+        
+        auto cpuBackend = _getDefaultBackend(cpuBackendNumThread);
+        if(this->executionType == MNNExecutionType::MNN_EXECUTION_BIG_LITTLE) {
+           MNN::Backend::Info backendInfo;
+           backendInfo.type = iter.first.type;
+           backendInfo.executionType = iter.first.executionType;
+           backendInfo.numThread = iter.first.CPULittleNumThread;
+           backendInfo.user = iter.first.user;
+           backendInfo.mode = iter.first.mode;
+           cpuBackend = BackendFactory::create(backendInfo);
+        }
+        MNN_PRINT("In Session::Session backup cpuBackend thread number is %d\n", iter.first.numThread);
+        MNN_PRINT("In Session::Session backup cpuBackupBackend thread number is %d\n", cpuBackendNumThread);
+        MNN_PRINT("In Session::Session backup cpuBackupBackend real thread number is %d\n", ((CPUBackend*)cpuBackend)->threadNumber());
+        
+        // Also news the Units
         std::unique_ptr<Pipeline> newPipeline(new Pipeline(iter.second, backend, cpuBackend));
         mPipelines.emplace_back(std::move(newPipeline));
     }
@@ -59,6 +87,9 @@ Session::Session(const Schedule::ScheduleInfo& info) {
     for (auto& iter : mInputs) {
         TensorUtils::getDescribe(iter.second)->isInput = true;
     }
+    const std::string profilePath = "/data/local/tmp/profile.txt";
+    mBenchmarkFile = new std::ofstream(profilePath, ios::out);
+    MNN_PRINT("Open Profile file %s\n", profilePath.c_str());
 }
 
 Session::~Session() {
@@ -68,18 +99,126 @@ Session::~Session() {
     mPipelines.clear();
     mBackends.clear();
     mTensors.clear();
+    mBenchmarkFile->flush();
+    mBenchmarkFile->close();
+    delete mBenchmarkFile;
+}
+
+static inline std::string forwardType(MNNForwardType type) {
+    switch (type) {
+        case MNN_FORWARD_CPU:
+            return "CPU";
+        case MNN_FORWARD_VULKAN:
+            return "Vulkan";
+        case MNN_FORWARD_OPENCL:
+            return "OpenCL";
+        case MNN_FORWARD_METAL:
+            return "Metal";
+        case MNN_FORWARD_CONVERT:
+            return "Convert";
+        default:
+            break;
+    }
+    return "N/A";
 }
 
 ErrorCode Session::run() const {
+    if (mNeedResize) {
+        MNN_ERROR("Can't run session because not resized");
+        return COMPUTE_SIZE_ERROR;
+    }
+    //MNN_PRINT("In Seesion::Run mPipelines size: %lu\n", mPipelines.size());
+    for (auto& iter : mPipelines) {
+        #ifdef MNN_BENCHMARK
+        RunMetadata run_metadata;
+        RunMetadata *run_metadata_ptr = nullptr;
+        if (1) {
+        run_metadata_ptr = &run_metadata;
+        }
+        ErrorCode error;
+        switch(this->executionType){
+            case MNNExecutionType::MNN_EXECUTION_DEFAULT:
+                MNN_PRINT("Start run pipeline in default.\n");
+                error = iter->execute(run_metadata_ptr);
+                break;
+            case MNNExecutionType::MNN_EXECUTION_SERIAL_HYBRID:
+                MNN_PRINT("Start run pipeline in serial hybrid.\n");
+                error = iter->execute(run_metadata_ptr);
+                break;
+            case MNNExecutionType::MNN_EXECUTION_PARALLEL:
+                MNN_PRINT("Start run pipeline in parallel.\n");
+                error = iter->executeInParallel(run_metadata_ptr);
+                break;
+            case MNNExecutionType::MNN_EXECUTION_BIG_LITTLE:
+                MNN_PRINT("Start run pipeline in parallel.\n");
+                error = iter->executeInParallel(run_metadata_ptr);
+                break;
+            default:
+                MNN_PRINT("Execution type not support yet\n");
+        }
+        if (run_metadata_ptr->op_stats.empty()) {
+            MNN_ERROR("Op metadata should not be empty\n");
+        }
+        MNN_PRINT("Start processing op_stats\n");
+        int count = 0;
+        for(auto op_stat: run_metadata_ptr->op_stats){
+            long time_in_us = op_stat.stats.end_micros - op_stat.stats.start_micros;
+            char buf[1024];
+            sprintf(buf, "Iter %d: %s\t%s\t%s\t%lu\t%lu\t%lu\n", count, op_stat.operator_name.c_str(), op_stat.type.c_str(), 
+              forwardType(op_stat.forwardType).c_str(), time_in_us, op_stat.stats.start_micros, op_stat.stats.end_micros);
+            mBenchmarkFile->write(buf, strlen(buf));
+            // MNN_PRINT("Iter %d: %s\t%s\t%s\t%lu\t%lu\t%lu\t", count, op_stat.operator_name.c_str(), op_stat.type.c_str(), 
+            //   forwardType(op_stat.forwardType).c_str(), time_in_us, op_stat.stats.start_micros, op_stat.stats.end_micros);
+            // for(auto v: op_stat.output_shape){
+            //     for(auto s: v){
+            //         MNN_PRINT("%d,",s);
+            //     }MNN_PRINT("\t");
+            // }
+            // MNN_PRINT("\n");
+            count++;
+        }
+        MNN_PRINT("End processing op_stats\n");
+        #else
+        auto error = iter->execute();
+        #endif
+        if (NO_ERROR != error) {
+            return error;
+        }
+        
+    }
+    return NO_ERROR;
+}
+
+ErrorCode Session::run(std::vector<RunMetadata> *net_stats) const {
     if (mNeedResize) {
         MNN_ERROR("Can't run session because not resized");
         return COMPUTE_SIZE_ERROR;
     }
     for (auto& iter : mPipelines) {
+        #ifdef MNN_BENCHMARK
+        RunMetadata run_metadata;
+        RunMetadata *run_metadata_ptr = nullptr;
+        if (1) {
+        run_metadata_ptr = &run_metadata;
+        }
+        auto error = iter->execute(run_metadata_ptr);
+        if (run_metadata_ptr->op_stats.empty()) {
+            MNN_ERROR("Op metadata should not be empty");
+        }
+        net_stats->push_back(run_metadata);
+        /*
+        for(auto op_stat: run_metadata_ptr->op_stats){
+            long time_in_us = op_stat.stats.end_micros - op_stat.stats.start_micros;
+            MNN_PRINT("%s\t%s\t%lu\n",op_stat.operator_name.c_str(), op_stat.type.c_str(), time_in_us);
+        }
+        */
+        #else
         auto error = iter->execute();
+        #endif
         if (NO_ERROR != error) {
             return error;
         }
+        
     }
     return NO_ERROR;
 }
@@ -120,6 +259,9 @@ ErrorCode Session::resize() {
     }
 
     for (auto& iter : mPipelines) {
+        // Set is parallel for pipeline before prepare
+        iter->executionType = this->executionType;
+        iter->deviceMapFilePath = this->deviceMapFilePath;
         auto error = iter->prepare();
         if (NO_ERROR != error) {
             return error;
@@ -181,14 +323,12 @@ ErrorCode Session::releaseCache() {
     }
     return NO_ERROR;
 }
+
 ErrorCode Session::updateToModel(Net* net) const {
     int opSize = net->oplists()->size();
     for (int i = 0; i < opSize; ++i) {
         auto op = net->oplists()->GetAs<Op>(i);
-        if (net->usage() == Usage_INFERENCE && op->type() != OpType_Const) {
-            continue;
-        }
-        if (net->usage() == Usage_TRAIN && op->type() != OpType_TrainableParam) {
+        if (op->type() != OpType_Const) {
             continue;
         }
         if (!op->outputIndexes() || op->outputIndexes()->size() != 1) {
@@ -199,15 +339,8 @@ ErrorCode Session::updateToModel(Net* net) const {
         if (blob->dataType() != DataType_DT_FLOAT) {
             continue;
         }
-        std::shared_ptr<Tensor> tensor = mTensors[index].second;
-        if (tensor->host<void>() == nullptr && tensor->deviceId() != 0) {
-            tensor.reset(Tensor::createHostTensorFromDevice(tensor.get(), true));
-            if (tensor.get() == nullptr) {
-                MNN_ERROR("failed to copy trained param from device to host\n");
-                return INVALID_VALUE;
-            }
-        }
-        ::memcpy((void*)blob->float32s()->data(), tensor->host<float>(), tensor->size());
+        ::memcpy((void*)blob->float32s()->data(), mTensors[index].second->host<float>(),
+                 mTensors[index].second->size());
     }
 
     return NO_ERROR;
diff --git a/source/core/Session.hpp b/source/core/Session.hpp
index 65211dc..276300a 100644
--- a/source/core/Session.hpp
+++ b/source/core/Session.hpp
@@ -12,17 +12,19 @@
 #include <map>
 #include <memory>
 #include <vector>
+#include <fstream>
 #include "core/Backend.hpp"
 #include "core/Macro.h"
-#include "Pipeline.hpp"
-#include "Schedule.hpp"
-#include "SizeComputer.hpp"
-#include <MNN/Tensor.hpp>
+#include "core/Pipeline.hpp"
+#include "core/Schedule.hpp"
+#include "core/SizeComputer.hpp"
+#include "MNN/Tensor.hpp"
+
 
 namespace MNN {
 struct Net;
 /** infer unit. multiple sessions could share one net. */
-class MNN_PUBLIC Session {
+class Session {
 public:
     /**
      * @breif initializ with schedule info.
@@ -37,6 +39,13 @@ public:
      * @return result code.
      */
     ErrorCode run() const;
+
+    /**
+     * @brief infer.
+     * @return result code.
+     */
+    ErrorCode run(std::vector<RunMetadata> *net_stats) const;
+
     /**
      * @brief infer with callbacks and sync option.
      * @param enterCallback callback before each op.
@@ -122,20 +131,24 @@ public:
      * @return errorcode
      */
     ErrorCode updateToModel(Net* net) const;
+    public:
+    MNNExecutionType executionType = MNNExecutionType::MNN_EXECUTION_DEFAULT;
+    char* deviceMapFilePath = nullptr;
+    std::ofstream* mBenchmarkFile;
 
 protected:
-    const std::vector<std::shared_ptr<Pipeline>>& getPipelines() const {
+    const std::vector<std::unique_ptr<Pipeline>>& getPipelines() const {
         return this->mPipelines;
     }
 
 private:
     void _clearCache();
     void _setUpTensorInfo(const Schedule::ScheduleInfo& info);
-    Backend* _getDefaultBackend();
+    Backend* _getDefaultBackend(int numThread = 2);
 
 private:
-    std::map<MNNForwardType, std::shared_ptr<Backend>> mBackends;
-    std::vector<std::shared_ptr<Pipeline>> mPipelines;
+    std::map<MNNForwardType, std::unique_ptr<Backend>> mBackends;
+    std::vector<std::unique_ptr<Pipeline>> mPipelines;
     std::vector<std::pair<int, std::shared_ptr<Tensor>>> mTensors;
     std::map<std::string, Tensor*> mInputs;
     std::map<std::string, Tensor*> mOutputs;
diff --git a/source/core/Tensor.cpp b/source/core/Tensor.cpp
index 2910f92..1875709 100644
--- a/source/core/Tensor.cpp
+++ b/source/core/Tensor.cpp
@@ -163,6 +163,7 @@ bool Tensor::copyFromHostTensor(const Tensor* hostTensor) {
 }
 
 bool Tensor::copyToHostTensor(Tensor* hostTensor) const {
+    MNN_PRINT("Tensor::copyToHostTensor\n");
     auto bn = mDescribe->backend;
     if (nullptr == bn) {
         return false;
diff --git a/source/core/WrapExecution.cpp b/source/core/WrapExecution.cpp
index 0d7ad1d..058c451 100644
--- a/source/core/WrapExecution.cpp
+++ b/source/core/WrapExecution.cpp
@@ -6,21 +6,33 @@
 //  Copyright  2018, Alibaba Group Holding Limited
 //
 
+#include <cstdlib>
+#include <cstring>
+
 #include "core/WrapExecution.hpp"
 #include "core/TensorUtils.hpp"
+#include "core/MUtils.hpp"
 
 namespace MNN {
 
 WrapExecution::WrapExecution(Backend* CPUBackend, std::shared_ptr<Execution> execution)
     : Execution(execution->backend()), mCPUBackend(CPUBackend), mExecution(execution) {
     mValid = execution->valid();
+    mUsedCachedTensor = true;
 }
 
 ErrorCode WrapExecution::onResize(const std::vector<Tensor*>& inputs, const std::vector<Tensor*>& outputs) {
     mWrapInputTensors.clear();
     mInputMaps.clear();
+    const char* p_env = std::getenv(mUseCachedEnv);
+    if(p_env !=nullptr && strcmp("true", p_env) == 0){
+        mUsedCachedTensor = true;
+    }else{
+        mUsedCachedTensor = false;
+    }
 
     auto dstBackend = mExecution->backend();
+    // MNN_PRINT("WrapExecution inputssize %lu\n", inputs.size());
     for (int i = 0; i < inputs.size(); ++i) {
         auto inputTensor = inputs[i];
         auto srcBackend  = TensorUtils::getDescribe(inputTensor)->backend;
@@ -31,6 +43,7 @@ ErrorCode WrapExecution::onResize(const std::vector<Tensor*>& inputs, const std:
         }
         // CPU -> XPU
         else if (srcBackend == mCPUBackend) {
+            // MNN_PRINT("CPU->GPU pointer %x\n", (void*) inputTensor);
             std::shared_ptr<Tensor> wrapTensor(new Tensor);
             TensorUtils::copyShape(inputTensor, wrapTensor.get(), true);
             wrapTensor->buffer().type = inputTensor->buffer().type;
@@ -39,6 +52,7 @@ ErrorCode WrapExecution::onResize(const std::vector<Tensor*>& inputs, const std:
         }
         // XPU -> CPU
         else if (dstBackend == mCPUBackend) {
+            // MNN_PRINT("GPU->CPU pointer %x\n", (void*) inputTensor);
             std::shared_ptr<Tensor> wrapTensor(new Tensor);
             TensorUtils::copyShape(inputTensor, wrapTensor.get(), true);
             wrapTensor->buffer().type = inputTensor->buffer().type;
@@ -103,17 +117,33 @@ ErrorCode WrapExecution::onResize(const std::vector<Tensor*>& inputs, const std:
 
 ErrorCode WrapExecution::onExecute(const std::vector<Tensor*>& inputs, const std::vector<Tensor*>& outputs) {
     MNN_ASSERT(mWrapInputTensors.size() == inputs.size());
-
+    // MNN_PRINT("mDataTransBuff size %lu\n", ((CPUBackend*)mCPUBackend)->mDataTransBuff.size());
     // copy variant tensors
     for (auto& iter : mInputMaps) {
+        
         auto converter = std::get<1>(iter);
         auto src       = std::get<2>(iter);
         auto dst       = std::get<3>(iter).get();
         if (!TensorUtils::getDescribe(src)->isConst) {
-            converter->onCopyBuffer(src, dst);
+            if(((CPUBackend*)mCPUBackend)->mDataTransBuff.find(src) == ((CPUBackend*)mCPUBackend)->mDataTransBuff.end() || !mUsedCachedTensor){
+                converter->onCopyBuffer(src, dst);
+                ((CPUBackend*)mCPUBackend)->mDataTransBuff[src] = dst;
+            }else {
+                // Using spinlock to avoid data tr
+                while (lock.test_and_set(std::memory_order_acquire))  // acquire lock
+                    ; // spin
+                // Replace the tensor in mWrapInputTensors
+                std::replace(mWrapInputTensors.begin(), mWrapInputTensors.end(), dst, ((CPUBackend*)mCPUBackend)->mDataTransBuff[src]);
+                MNN_PRINT("[NCHW: %d %d %d %d] Use cached\n", src->batch(), src->channel(), src->height(), src->width());
+                lock.clear(std::memory_order_release);
+            }
         }
     }
+    // Profile the real execution time
+    this->mStartTimeStamp = NowMicros();
     mExecution->onExecute(mWrapInputTensors, outputs);
+    mExecution->backend()->onWaitFinish();
+    this->mEndTimeStamp = NowMicros();
     return NO_ERROR;
 }
 
diff --git a/source/core/WrapExecution.hpp b/source/core/WrapExecution.hpp
index 778f682..e0c7a4f 100644
--- a/source/core/WrapExecution.hpp
+++ b/source/core/WrapExecution.hpp
@@ -11,11 +11,16 @@
 
 #include <stdio.h>
 #include <memory>
+#include <map>
+#include <atomic>
+
 #include "core/Backend.hpp"
 #include "core/Execution.hpp"
 #include "core/Macro.h"
 
 namespace MNN {
+static constexpr char mUseCachedEnv[] = "MNN_USE_CACHED";
+static std::atomic_flag lock = ATOMIC_FLAG_INIT;
 
 /** execution wrapper. hiding cross-backend tensor converting. */
 class WrapExecution : public Execution {
@@ -32,12 +37,15 @@ public:
     virtual ~WrapExecution() = default;
     virtual ErrorCode onResize(const std::vector<Tensor *> &inputs, const std::vector<Tensor *> &outputs) override;
     virtual ErrorCode onExecute(const std::vector<Tensor *> &inputs, const std::vector<Tensor *> &outputs) override;
-
+    bool isUseCachedTensor() const;
+    
 private:
     Backend *mCPUBackend;
     std::shared_ptr<Execution> mExecution;
     std::vector<Tensor *> mWrapInputTensors;
     std::vector<std::tuple<Backend *, Backend *, Tensor *, std::shared_ptr<Tensor>>> mInputMaps;
+    bool mUsedCachedTensor;
+    
 };
 } // namespace MNN
 
diff --git a/test/TestUtils.h b/test/TestUtils.h
index f3514ed..cb30e62 100644
--- a/test/TestUtils.h
+++ b/test/TestUtils.h
@@ -9,6 +9,7 @@
 #ifndef TestUtils_h
 #define TestUtils_h
 
+#include <sys/time.h>
 #include <assert.h>
 #include <stdio.h>
 #include <functional>
@@ -18,6 +19,13 @@
 #include <MNN/Tensor.hpp>
 #include <math.h>
 
+inline uint64_t NowMicros() {
+  struct timeval tv;
+  gettimeofday(&tv, nullptr);
+  return static_cast<int64_t>(tv.tv_sec) * 1000000 + tv.tv_usec;
+}
+
+
 /**
  * @brief create session with net and backend
  * @param net       given net
diff --git a/test/main.cpp b/test/main.cpp
index 1e84b76..5ae3b3a 100644
--- a/test/main.cpp
+++ b/test/main.cpp
@@ -9,11 +9,28 @@
 #include <stdio.h>
 #include <stdlib.h>
 #include <time.h>
+#include <pthread.h>
+
 #include "MNNTestSuite.h"
 
+void* runBench(void *arg){
+    char* name = (char*)arg;
+    MNNTestSuite::run(name);
+    return (void*)0;
+}
+
 int main(int argc, char* argv[]) {
     srand(time(NULL));
-    if (argc > 1) {
+    if (argc > 2){
+        auto cpuName = argv[1];
+        auto gpuName = argv[2];
+        pthread_t cpuThread, gpuThread;
+        pthread_create(&cpuThread, NULL, runBench, (void*)cpuName);
+        pthread_create(&gpuThread, NULL, runBench, (void*)gpuName);
+        pthread_join(cpuThread, NULL);
+        pthread_join(gpuThread, NULL);
+    }
+    else if (argc > 1) {
         auto name = argv[1];
         MNNTestSuite::run(name);
     } else {
diff --git a/test/op/ConvolutionTest.cpp b/test/op/ConvolutionTest.cpp
index 90b65e3..743b977 100644
--- a/test/op/ConvolutionTest.cpp
+++ b/test/op/ConvolutionTest.cpp
@@ -104,16 +104,30 @@ protected:
         auto output = _Conv(std::move(weightData), std::move(biasData), _Convert(input, NC4HW4), {ic, oc}, {kw, kh},
                             padMap[mode], {stride, stride}, {dilation, dilation}, group, {pad_w, pad_h});
         output = _Convert(output, NCHW);
-        if (type != MNN_FORWARD_CPU) {
+        
+        //if (type != MNN_FORWARD_CPU) {
             Optimizer::Config config;
             config.forwardType = type;
+            if(type == MNN_FORWARD_CPU){
+                config.numThread = 4;
+            }
             auto optimizer = Optimizer::create(config);
             if (optimizer == nullptr) {
                 MNN_ERROR("backend %s not support\n", device_name.c_str());
                 return false;
             }
-            optimizer->onExecute({output});
-        }
+            MNN_PRINT("type == MNN_FORWARD_CPUs? %d\n", (type == MNN_FORWARD_CPU));
+            int loop = 1000;
+            auto start = NowMicros();
+            for(int i=0; i<loop; ++i){
+                if(i % 100 == 0){
+                    MNN_PRINT("%s loop: %d\n",(type == MNN_FORWARD_CPU ? "CPU" : "GPU"), i);
+                }
+                optimizer->onExecute({output});
+            }
+            auto end = NowMicros();
+            MNN_PRINT("bench %d times used %lu us\n", loop, (end - start));
+        //}
 
         ::memcpy(input->writeMap<float>(), inputData.data(), inputData.size() * sizeof(float));
         // difference below 0.5% relative error is considered correct.
@@ -161,6 +175,28 @@ protected:
     }
 };
 
+class ConvolutionBenchmark : public ConvolutionCommonTest {
+public:
+    virtual ~ConvolutionBenchmark() = default;
+
+    static bool test(MNNForwardType type, const std::string& device_name) {
+        int b = 1, oc = 64, ic = 64, is = 64, kw = 3, kh = 3, d = 1, s = 1, p =0;
+        if (d > std::min(kw, kh) || d * (std::max(kw, kh) - 1) + 1 > is){
+            MNN_PRINT("Exceed!!!!\n");
+        }
+                                        
+        bool succ = ConvolutionCommonTest::test(type, device_name, "Conv2D",
+                                                b, ic, oc, is, is, PadMode_CAFFE,
+                                                p, p, kh, kw, s, d, 1);
+        MNN_PRINT("In ConvolutionTest.cpp ConvolutionBenchmark test\n");
+        if (!succ) {
+            return false;
+        }
+        return true;
+    }
+};
+
+
 class ConvolutionTestOnCPU : public ConvolutionTest {
 public:
     ~ConvolutionTestOnCPU() = default;
@@ -177,6 +213,26 @@ public:
     }
 };
 
+class ConvolutionBenchmarkOnCPU : public ConvolutionTest {
+public:
+    ~ConvolutionBenchmarkOnCPU() = default;
+    virtual bool run() {
+        return ConvolutionBenchmark::test(MNN_FORWARD_CPU, "CPU");
+    }
+};
+
+class ConvolutionBenchmarkOnOpencl : public ConvolutionTest {
+public:
+    ~ConvolutionBenchmarkOnOpencl() = default;
+    virtual bool run() {
+        return ConvolutionBenchmark::test(MNN_FORWARD_OPENCL, "OPENCL");
+    }
+};
+
+// Benchmark
+MNNTestSuiteRegister(ConvolutionBenchmarkOnCPU, "op/convolution/conv/cpu/benchmark");
+MNNTestSuiteRegister(ConvolutionBenchmarkOnOpencl, "op/convolution/conv/opencl/benchmark");
+
 class DepthwiseConvolutionTest : public ConvolutionCommonTest {
 public:
     virtual ~DepthwiseConvolutionTest() = default;
@@ -291,6 +347,9 @@ MNNTestSuiteRegister(DepthwiseConvolutionTestOnOpencl, "op/convolution/depthwise
 MNNTestSuiteRegister(GroupConvolutionTestOnCPU, "op/convolution/conv_group/cpu");
 MNNTestSuiteRegister(GroupConvolutionTestOnOpencl, "op/convolution/conv_group/opencl");
 
+
+
+
 static Interpreter *create(int oc, // output channel
                            int w,  // input width
                            int h,  // input height
